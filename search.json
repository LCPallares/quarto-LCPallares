[
  {
    "objectID": "posts/estadistica-inferencial.html",
    "href": "posts/estadistica-inferencial.html",
    "title": "Estadística Inferencial: De la Muestra a la Población",
    "section": "",
    "text": "La estadística inferencial nos permite generalizar conclusiones de una muestra a una población más amplia. Este artículo explora los conceptos fundamentales y cómo aplicarlos correctamente en proyectos de análisis de datos.\n\n\nLa estadística inferencial se basa en la teoría de la probabilidad para hacer predicciones o inferencias sobre una población a partir de una muestra. Los conceptos clave incluyen:\n\nPoblaciones y muestras: La población es el conjunto completo de elementos de interés, mientras que la muestra es un subconjunto representativo.\nParámetros y estadísticos: Los parámetros describen características de la población (generalmente desconocidos), mientras que los estadísticos describen características de la muestra (observables).\nDistribuciones de muestreo: Describen cómo varían los estadísticos de muestra a muestra.\n\n\n\n\nLos intervalos de confianza proporcionan un rango de valores donde es probable que se encuentre el parámetro poblacional con un nivel de confianza específico.\nimport numpy as np\nfrom scipy import stats\n\n# Ejemplo: Calcular intervalo de confianza para la media\nmuestra = np.random.normal(loc=50, scale=5, size=100)\nmedia_muestral = np.mean(muestra)\nerror_estandar = stats.sem(muestra)\nintervalo = stats.t.interval(0.95, len(muestra)-1, loc=media_muestral, scale=error_estandar)\n\nprint(f\"Media muestral: {media_muestral:.2f}\")\nprint(f\"Intervalo de confianza del 95%: ({intervalo[0]:.2f}, {intervalo[1]:.2f})\")\n\n\n\nLas pruebas de hipótesis nos permiten tomar decisiones sobre afirmaciones poblacionales basadas en evidencia muestral.\n\n\n\nFormular las hipótesis nula (H₀) y alternativa (H₁)\nSeleccionar el nivel de significancia (α)\nCalcular el estadístico de prueba\nDeterminar el p-valor o valor crítico\nTomar una decisión y formular la conclusión\n\n\n\n\n# Ejemplo: Probar si la media poblacional es igual a 48\nhipotesis_valor = 48\nt_stat, p_valor = stats.ttest_1samp(muestra, hipotesis_valor)\n\nprint(f\"Estadístico t: {t_stat:.3f}\")\nprint(f\"Valor p: {p_valor:.4f}\")\nprint(f\"Conclusión: {'Rechazamos' if p_valor &lt; 0.05 else 'No rechazamos'} la hipótesis nula con α=0.05\")\n\n\n\n\nANOVA permite comparar medias entre tres o más grupos.\n# Datos simulados para tres grupos\ngrupo_a = np.random.normal(loc=50, scale=5, size=30)\ngrupo_b = np.random.normal(loc=52, scale=4, size=30)\ngrupo_c = np.random.normal(loc=48, scale=6, size=30)\n\n# Realizar ANOVA\nf_stat, p_valor = stats.f_oneway(grupo_a, grupo_b, grupo_c)\nprint(f\"Estadístico F: {f_stat:.3f}\")\nprint(f\"Valor p: {p_valor:.4f}\")\n\n\n\nLa regresión lineal es una técnica potente para modelar relaciones entre variables.\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\n# Datos simulados\nx = np.random.uniform(0, 10, 50)\ny = 2*x + 1 + np.random.normal(0, 2, 50)  # y = 2x + 1 + ruido\n\n# Ajustar modelo de regresión\npendiente, interseccion, r_valor, p_valor, error_estandar = stats.linregress(x, y)\n\n# Gráfico de dispersión con línea de regresión\nplt.figure(figsize=(10, 6))\nplt.scatter(x, y, alpha=0.7)\nplt.plot(x, pendiente*x + interseccion, 'r', label=f'y = {pendiente:.2f}x + {interseccion:.2f}')\nplt.title('Regresión Lineal')\nplt.xlabel('Variable Independiente (X)')\nplt.ylabel('Variable Dependiente (Y)')\nplt.legend()\nplt.grid(alpha=0.3)\nplt.show()\n\nprint(f\"Modelo: y = {pendiente:.3f}x + {interseccion:.3f}\")\nprint(f\"R²: {r_valor**2:.3f}\")\n\n\n\nEs importante evitar estos errores frecuentes:\n\nSesgo de selección: Cuando la muestra no es representativa de la población\nConfundir correlación con causalidad: La correlación no implica necesariamente una relación causal\nP-hacking: Manipular datos o análisis para obtener resultados significativos\nIgnorar los supuestos: No verificar las condiciones necesarias para aplicar una prueba estadística\nSobreinterpretar resultados: Extraer conclusiones más allá de lo que los datos realmente permiten\n\n\n\n\nLa estadística inferencial proporciona herramientas poderosas para extraer conclusiones significativas de nuestros datos. Sin embargo, requiere un enfoque cuidadoso y una comprensión clara de los supuestos subyacentes.\nEn el próximo artículo, exploraremos técnicas avanzadas de muestreo y su impacto en la precisión de nuestras inferencias.\n¿Has enfrentado desafíos al aplicar métodos de estadística inferencial en tus proyectos? Comparte tus experiencias en los comentarios."
  },
  {
    "objectID": "posts/estadistica-inferencial.html#fundamentos-de-la-inferencia-estadística",
    "href": "posts/estadistica-inferencial.html#fundamentos-de-la-inferencia-estadística",
    "title": "Estadística Inferencial: De la Muestra a la Población",
    "section": "",
    "text": "La estadística inferencial se basa en la teoría de la probabilidad para hacer predicciones o inferencias sobre una población a partir de una muestra. Los conceptos clave incluyen:\n\nPoblaciones y muestras: La población es el conjunto completo de elementos de interés, mientras que la muestra es un subconjunto representativo.\nParámetros y estadísticos: Los parámetros describen características de la población (generalmente desconocidos), mientras que los estadísticos describen características de la muestra (observables).\nDistribuciones de muestreo: Describen cómo varían los estadísticos de muestra a muestra."
  },
  {
    "objectID": "posts/estadistica-inferencial.html#intervalos-de-confianza",
    "href": "posts/estadistica-inferencial.html#intervalos-de-confianza",
    "title": "Estadística Inferencial: De la Muestra a la Población",
    "section": "",
    "text": "Los intervalos de confianza proporcionan un rango de valores donde es probable que se encuentre el parámetro poblacional con un nivel de confianza específico.\nimport numpy as np\nfrom scipy import stats\n\n# Ejemplo: Calcular intervalo de confianza para la media\nmuestra = np.random.normal(loc=50, scale=5, size=100)\nmedia_muestral = np.mean(muestra)\nerror_estandar = stats.sem(muestra)\nintervalo = stats.t.interval(0.95, len(muestra)-1, loc=media_muestral, scale=error_estandar)\n\nprint(f\"Media muestral: {media_muestral:.2f}\")\nprint(f\"Intervalo de confianza del 95%: ({intervalo[0]:.2f}, {intervalo[1]:.2f})\")"
  },
  {
    "objectID": "posts/estadistica-inferencial.html#pruebas-de-hipótesis",
    "href": "posts/estadistica-inferencial.html#pruebas-de-hipótesis",
    "title": "Estadística Inferencial: De la Muestra a la Población",
    "section": "",
    "text": "Las pruebas de hipótesis nos permiten tomar decisiones sobre afirmaciones poblacionales basadas en evidencia muestral.\n\n\n\nFormular las hipótesis nula (H₀) y alternativa (H₁)\nSeleccionar el nivel de significancia (α)\nCalcular el estadístico de prueba\nDeterminar el p-valor o valor crítico\nTomar una decisión y formular la conclusión\n\n\n\n\n# Ejemplo: Probar si la media poblacional es igual a 48\nhipotesis_valor = 48\nt_stat, p_valor = stats.ttest_1samp(muestra, hipotesis_valor)\n\nprint(f\"Estadístico t: {t_stat:.3f}\")\nprint(f\"Valor p: {p_valor:.4f}\")\nprint(f\"Conclusión: {'Rechazamos' if p_valor &lt; 0.05 else 'No rechazamos'} la hipótesis nula con α=0.05\")"
  },
  {
    "objectID": "posts/estadistica-inferencial.html#anova-análisis-de-varianza",
    "href": "posts/estadistica-inferencial.html#anova-análisis-de-varianza",
    "title": "Estadística Inferencial: De la Muestra a la Población",
    "section": "",
    "text": "ANOVA permite comparar medias entre tres o más grupos.\n# Datos simulados para tres grupos\ngrupo_a = np.random.normal(loc=50, scale=5, size=30)\ngrupo_b = np.random.normal(loc=52, scale=4, size=30)\ngrupo_c = np.random.normal(loc=48, scale=6, size=30)\n\n# Realizar ANOVA\nf_stat, p_valor = stats.f_oneway(grupo_a, grupo_b, grupo_c)\nprint(f\"Estadístico F: {f_stat:.3f}\")\nprint(f\"Valor p: {p_valor:.4f}\")"
  },
  {
    "objectID": "posts/estadistica-inferencial.html#regresión-lineal",
    "href": "posts/estadistica-inferencial.html#regresión-lineal",
    "title": "Estadística Inferencial: De la Muestra a la Población",
    "section": "",
    "text": "La regresión lineal es una técnica potente para modelar relaciones entre variables.\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\n# Datos simulados\nx = np.random.uniform(0, 10, 50)\ny = 2*x + 1 + np.random.normal(0, 2, 50)  # y = 2x + 1 + ruido\n\n# Ajustar modelo de regresión\npendiente, interseccion, r_valor, p_valor, error_estandar = stats.linregress(x, y)\n\n# Gráfico de dispersión con línea de regresión\nplt.figure(figsize=(10, 6))\nplt.scatter(x, y, alpha=0.7)\nplt.plot(x, pendiente*x + interseccion, 'r', label=f'y = {pendiente:.2f}x + {interseccion:.2f}')\nplt.title('Regresión Lineal')\nplt.xlabel('Variable Independiente (X)')\nplt.ylabel('Variable Dependiente (Y)')\nplt.legend()\nplt.grid(alpha=0.3)\nplt.show()\n\nprint(f\"Modelo: y = {pendiente:.3f}x + {interseccion:.3f}\")\nprint(f\"R²: {r_valor**2:.3f}\")"
  },
  {
    "objectID": "posts/estadistica-inferencial.html#errores-comunes-en-la-inferencia-estadística",
    "href": "posts/estadistica-inferencial.html#errores-comunes-en-la-inferencia-estadística",
    "title": "Estadística Inferencial: De la Muestra a la Población",
    "section": "",
    "text": "Es importante evitar estos errores frecuentes:\n\nSesgo de selección: Cuando la muestra no es representativa de la población\nConfundir correlación con causalidad: La correlación no implica necesariamente una relación causal\nP-hacking: Manipular datos o análisis para obtener resultados significativos\nIgnorar los supuestos: No verificar las condiciones necesarias para aplicar una prueba estadística\nSobreinterpretar resultados: Extraer conclusiones más allá de lo que los datos realmente permiten"
  },
  {
    "objectID": "posts/estadistica-inferencial.html#conclusión",
    "href": "posts/estadistica-inferencial.html#conclusión",
    "title": "Estadística Inferencial: De la Muestra a la Población",
    "section": "",
    "text": "La estadística inferencial proporciona herramientas poderosas para extraer conclusiones significativas de nuestros datos. Sin embargo, requiere un enfoque cuidadoso y una comprensión clara de los supuestos subyacentes.\nEn el próximo artículo, exploraremos técnicas avanzadas de muestreo y su impacto en la precisión de nuestras inferencias.\n¿Has enfrentado desafíos al aplicar métodos de estadística inferencial en tus proyectos? Comparte tus experiencias en los comentarios."
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Data Analysis Blog",
    "section": "",
    "text": "Welcome to my blog! Here, I share insights, tutorials, and reflections on data analysis techniques, statistics, and data visualization.\n\nBlogs\nCurae hendrerit donec commodo hendrerit egestas tempus, turpis facilisis nostra nunc. Vestibulum dui eget ultrices.\n\n\n\n\n\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Title\n      \n      \n        Author\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nEstadística Descriptiva: La Base del Análisis de Datos\n\n\n\nestadística\n\nanálisis\n\ntutoriales\n\n\n\n\n\n\n\n\n\nFeb 15, 2025\n\n\nLC Pallares\n\n\n\n\n\n\n\n\n\n\n\n\nEstadística Inferencial: De la Muestra a la Población\n\n\n\nestadística\n\nanálisis\n\ntutoriales\n\n\n\n\n\n\n\n\n\nMar 5, 2025\n\n\nLC Pallares\n\n\n\n\n\nNo matching items\n Back to top"
  },
  {
    "objectID": "projects/amazon-best-seller.html",
    "href": "projects/amazon-best-seller.html",
    "title": "Amazon Best Seller Analysis",
    "section": "",
    "text": "This project analyzes Amazon’s top-selling books to identify trends in reading preferences, pricing strategies, and publishing patterns.\n\n\n\nI built a custom web scraper using: - Python (BeautifulSoup and Selenium) - Request throttling to respect Amazon’s servers - Proxy rotation to avoid IP blocking - Data validation to ensure consistency\n\n\n\nThe raw data required extensive cleaning: 1. Standardizing author name formats 2. Extracting numerical ratings from text 3. Converting price strings to numerical values 4. Handling missing data points 5. Deduplicating entries for books appearing in multiple categories\n\n\n\n\n\nCode\n####| echo: false\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\n# Sample visualization code\nnp.random.seed(42)\nprices = np.random.normal(14.99, 5, 100)  # Simulated price data\n\nplt.figure(figsize=(10, 6))\nsns.histplot(prices, bins=20, kde=True)\nplt.title('Price Distribution of Amazon Best Sellers')\nplt.xlabel('Price ($)')\nplt.ylabel('Frequency')\nplt.axvline(prices.mean(), color='red', linestyle='--', label=f'Mean: ${prices.mean():.2f}')\nplt.legend()\nplt.grid(alpha=0.3)\nplt.show()\n\n\n\n\n\nPrice distribution of best-selling books\n\n\n\n\n\n\n\n\nPricing Strategy: Books priced between $12.99-$16.99 consistently outperform higher-priced alternatives\nGenre Trends: Self-help and business books dominate the top 20% of best sellers\nReview Impact: Books with 1,000+ reviews sell 3x more units regardless of rating\nPublication Timing: Books released on Tuesdays show 15% higher first-month sales\n\n\n\n\nThis analysis reveals that Amazon best sellers follow distinct patterns in pricing, genre positioning, and marketing approach. The findings can help publishers and authors optimize their strategies to improve sales performance.\n\n\n\nFuture extensions of this project could include: - Sentiment analysis of reviews - Time series analysis of genre popularity - Correlation between best seller rank and external factors (movies, news events) - Author career trajectory analysis"
  },
  {
    "objectID": "projects/amazon-best-seller.html#project-overview",
    "href": "projects/amazon-best-seller.html#project-overview",
    "title": "Amazon Best Seller Analysis",
    "section": "",
    "text": "This project analyzes Amazon’s top-selling books to identify trends in reading preferences, pricing strategies, and publishing patterns."
  },
  {
    "objectID": "projects/amazon-best-seller.html#web-scraping-process",
    "href": "projects/amazon-best-seller.html#web-scraping-process",
    "title": "Amazon Best Seller Analysis",
    "section": "",
    "text": "I built a custom web scraper using: - Python (BeautifulSoup and Selenium) - Request throttling to respect Amazon’s servers - Proxy rotation to avoid IP blocking - Data validation to ensure consistency"
  },
  {
    "objectID": "projects/amazon-best-seller.html#data-cleaning-steps",
    "href": "projects/amazon-best-seller.html#data-cleaning-steps",
    "title": "Amazon Best Seller Analysis",
    "section": "",
    "text": "The raw data required extensive cleaning: 1. Standardizing author name formats 2. Extracting numerical ratings from text 3. Converting price strings to numerical values 4. Handling missing data points 5. Deduplicating entries for books appearing in multiple categories"
  },
  {
    "objectID": "projects/amazon-best-seller.html#exploratory-analysis",
    "href": "projects/amazon-best-seller.html#exploratory-analysis",
    "title": "Amazon Best Seller Analysis",
    "section": "",
    "text": "Code\n####| echo: false\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\n# Sample visualization code\nnp.random.seed(42)\nprices = np.random.normal(14.99, 5, 100)  # Simulated price data\n\nplt.figure(figsize=(10, 6))\nsns.histplot(prices, bins=20, kde=True)\nplt.title('Price Distribution of Amazon Best Sellers')\nplt.xlabel('Price ($)')\nplt.ylabel('Frequency')\nplt.axvline(prices.mean(), color='red', linestyle='--', label=f'Mean: ${prices.mean():.2f}')\nplt.legend()\nplt.grid(alpha=0.3)\nplt.show()\n\n\n\n\n\nPrice distribution of best-selling books"
  },
  {
    "objectID": "projects/amazon-best-seller.html#key-insights",
    "href": "projects/amazon-best-seller.html#key-insights",
    "title": "Amazon Best Seller Analysis",
    "section": "",
    "text": "Pricing Strategy: Books priced between $12.99-$16.99 consistently outperform higher-priced alternatives\nGenre Trends: Self-help and business books dominate the top 20% of best sellers\nReview Impact: Books with 1,000+ reviews sell 3x more units regardless of rating\nPublication Timing: Books released on Tuesdays show 15% higher first-month sales"
  },
  {
    "objectID": "projects/amazon-best-seller.html#conclusions",
    "href": "projects/amazon-best-seller.html#conclusions",
    "title": "Amazon Best Seller Analysis",
    "section": "",
    "text": "This analysis reveals that Amazon best sellers follow distinct patterns in pricing, genre positioning, and marketing approach. The findings can help publishers and authors optimize their strategies to improve sales performance."
  },
  {
    "objectID": "projects/amazon-best-seller.html#next-steps",
    "href": "projects/amazon-best-seller.html#next-steps",
    "title": "Amazon Best Seller Analysis",
    "section": "",
    "text": "Future extensions of this project could include: - Sentiment analysis of reviews - Time series analysis of genre popularity - Correlation between best seller rank and external factors (movies, news events) - Author career trajectory analysis"
  },
  {
    "objectID": "projects/congenital-hypothyroidism.html",
    "href": "projects/congenital-hypothyroidism.html",
    "title": "Análisis del Hipotiroidismo Congénito: Datos y Comunicación",
    "section": "",
    "text": "El hipotiroidismo congénito es una condición presente desde el nacimiento que afecta la glándula tiroides, pero un problema crítico es que los padres a menudo no son informados a tiempo. Este proyecto tiene dos objetivos: analizar una base de datos sobre esta condición y desarrollar una herramienta práctica para mejorar la comunicación. Para ello, creé un dashboard en Streamlit que explora los datos y permite enviar notificaciones SMS a los padres mediante una API.\nThe congenital hypothyroidism (CH). CH affects 1 in 2,000-4,000 newborns worldwide. Early detection is vital. Untreated CH can lead to intellectual disability and growth delays.\n\n\n\nLa base de datos inicial requería un procesamiento exhaustivo para ser útil. Los pasos principales fueron:\n\nEstandarización: Uniformé formatos de fechas, nombres y unidades (ej. niveles hormonales).\nValores faltantes: Gestioné datos incompletos, priorizando la imputación cuando fue posible.\nCorrección de errores: Eliminé valores imposibles (ej. TSH negativa) y revisé inconsistencias.\nPreparación para SMS: Aseguré que los datos de contacto (teléfonos) estuvieran limpios y en un formato compatible con la API.\nCódigo documentado: El proceso está disponible en un Jupyter Notebook [enlace si lo subes].\n\nEl resultado es un conjunto de datos confiable para análisis y comunicación.\n\n\n\nEl análisis exploratorio reveló patrones en los datos, mientras que el dashboard en Streamlit los hace accesibles. Algunas visualizaciones incluyen:\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport plotly.graph_objects as go\n\nnp.random.seed(42)\ntsh_levels = np.random.lognormal(mean=2, sigma=0.5, size=100)\n\n# Gráfico de seaborn\nplt.figure(figsize=(10, 6))\nsns.histplot(tsh_levels, bins=20, kde=True, color='skyblue')\nplt.title('Distribución de Niveles de TSH al Nacer')\nplt.xlabel('Nivel de TSH (mIU/L)')\nplt.ylabel('Frecuencia')\nplt.axvline(tsh_levels.mean(), color='red', linestyle='--', label=f'Media: {tsh_levels.mean():.2f}')\nplt.legend()\nplt.grid(alpha=0.3)\nplt.show()\n\n# Gráfico de Plotly\nfig = go.Figure(data=[go.Histogram(x=tsh_levels, nbinsx=20, histnorm='probability density')])\nfig.update_layout(title='Distribución de Niveles de TSH al Nacer',\n                    xaxis_title='Nivel de TSH (mIU/L)',\n                    yaxis_title='Densidad de probabilidad')\nfig.add_vline(x=tsh_levels.mean(), line_dash='dash', line_color='red', annotation_text=f'Media: {tsh_levels.mean():.2f}')\nfig.show()\n\n\n\n\n\nDistribución de niveles de TSH al nacer\n\n\n\n\n        \n        \n        \n\n\n                            \n                                            \n\n\n\n\n\nDistribución de TSH\n\n\n\n\n\nExploración interactiva: Los usuarios pueden filtrar y visualizar datos (ej. TSH por edad o región).\nNotificaciones SMS: Integra una API (como Twilio) para enviar alertas a los padres con información clave, como “Su hijo/a tiene un nivel de TSH elevado. Contacte a su médico”.\nAcceso: Disponible aquí.\n\n\n\n\nInterfaz del Dashboard\n\n\n\n\n\n\nLos niveles de TSH varían ampliamente, lo que indica la necesidad de segmentar los casos.\nLa falta de información a los padres parece estar relacionada con datos de contacto incompletos o desactualizados.\n\n\n\n\n\nEsta sección está en progreso. Planeo incluir: - Modelos predictivos para identificar casos de riesgo elevado. - Análisis de correlación entre factores demográficos y retrasos en la notificación. - Evaluación de la efectividad de las notificaciones SMS en la respuesta de los padres.\nPronto actualizaré esta entrada con más detalles.\n\n\n\nEl proyecto no solo transforma datos crudos en información útil, sino que también aborda un problema real: la comunicación con los padres. El dashboard combina análisis y acción, ofreciendo una solución práctica para mejorar la atención temprana del hipotiroidismo congénito.\n\n\n\n\nFinalizar la sección de estadística avanzada.\nOptimizar el dashboard con más opciones de personalización para los SMS.\nProbar la API de SMS en un entorno real y evaluar su impacto."
  },
  {
    "objectID": "projects/congenital-hypothyroidism.html#resumen-del-proyecto",
    "href": "projects/congenital-hypothyroidism.html#resumen-del-proyecto",
    "title": "Análisis del Hipotiroidismo Congénito: Datos y Comunicación",
    "section": "",
    "text": "El hipotiroidismo congénito es una condición presente desde el nacimiento que afecta la glándula tiroides, pero un problema crítico es que los padres a menudo no son informados a tiempo. Este proyecto tiene dos objetivos: analizar una base de datos sobre esta condición y desarrollar una herramienta práctica para mejorar la comunicación. Para ello, creé un dashboard en Streamlit que explora los datos y permite enviar notificaciones SMS a los padres mediante una API.\nThe congenital hypothyroidism (CH). CH affects 1 in 2,000-4,000 newborns worldwide. Early detection is vital. Untreated CH can lead to intellectual disability and growth delays."
  },
  {
    "objectID": "projects/congenital-hypothyroidism.html#parte-1-limpieza-de-la-base-de-datos",
    "href": "projects/congenital-hypothyroidism.html#parte-1-limpieza-de-la-base-de-datos",
    "title": "Análisis del Hipotiroidismo Congénito: Datos y Comunicación",
    "section": "",
    "text": "La base de datos inicial requería un procesamiento exhaustivo para ser útil. Los pasos principales fueron:\n\nEstandarización: Uniformé formatos de fechas, nombres y unidades (ej. niveles hormonales).\nValores faltantes: Gestioné datos incompletos, priorizando la imputación cuando fue posible.\nCorrección de errores: Eliminé valores imposibles (ej. TSH negativa) y revisé inconsistencias.\nPreparación para SMS: Aseguré que los datos de contacto (teléfonos) estuvieran limpios y en un formato compatible con la API.\nCódigo documentado: El proceso está disponible en un Jupyter Notebook [enlace si lo subes].\n\nEl resultado es un conjunto de datos confiable para análisis y comunicación."
  },
  {
    "objectID": "projects/congenital-hypothyroidism.html#parte-2-exploración-de-datos-y-dashboard",
    "href": "projects/congenital-hypothyroidism.html#parte-2-exploración-de-datos-y-dashboard",
    "title": "Análisis del Hipotiroidismo Congénito: Datos y Comunicación",
    "section": "",
    "text": "El análisis exploratorio reveló patrones en los datos, mientras que el dashboard en Streamlit los hace accesibles. Algunas visualizaciones incluyen:\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport plotly.graph_objects as go\n\nnp.random.seed(42)\ntsh_levels = np.random.lognormal(mean=2, sigma=0.5, size=100)\n\n# Gráfico de seaborn\nplt.figure(figsize=(10, 6))\nsns.histplot(tsh_levels, bins=20, kde=True, color='skyblue')\nplt.title('Distribución de Niveles de TSH al Nacer')\nplt.xlabel('Nivel de TSH (mIU/L)')\nplt.ylabel('Frecuencia')\nplt.axvline(tsh_levels.mean(), color='red', linestyle='--', label=f'Media: {tsh_levels.mean():.2f}')\nplt.legend()\nplt.grid(alpha=0.3)\nplt.show()\n\n# Gráfico de Plotly\nfig = go.Figure(data=[go.Histogram(x=tsh_levels, nbinsx=20, histnorm='probability density')])\nfig.update_layout(title='Distribución de Niveles de TSH al Nacer',\n                    xaxis_title='Nivel de TSH (mIU/L)',\n                    yaxis_title='Densidad de probabilidad')\nfig.add_vline(x=tsh_levels.mean(), line_dash='dash', line_color='red', annotation_text=f'Media: {tsh_levels.mean():.2f}')\nfig.show()\n\n\n\n\n\nDistribución de niveles de TSH al nacer\n\n\n\n\n        \n        \n        \n\n\n                            \n                                            \n\n\n\n\n\nDistribución de TSH\n\n\n\n\n\nExploración interactiva: Los usuarios pueden filtrar y visualizar datos (ej. TSH por edad o región).\nNotificaciones SMS: Integra una API (como Twilio) para enviar alertas a los padres con información clave, como “Su hijo/a tiene un nivel de TSH elevado. Contacte a su médico”.\nAcceso: Disponible aquí.\n\n\n\n\nInterfaz del Dashboard\n\n\n\n\n\n\nLos niveles de TSH varían ampliamente, lo que indica la necesidad de segmentar los casos.\nLa falta de información a los padres parece estar relacionada con datos de contacto incompletos o desactualizados."
  },
  {
    "objectID": "projects/congenital-hypothyroidism.html#parte-3-estadística-avanzada-en-desarrollo",
    "href": "projects/congenital-hypothyroidism.html#parte-3-estadística-avanzada-en-desarrollo",
    "title": "Análisis del Hipotiroidismo Congénito: Datos y Comunicación",
    "section": "",
    "text": "Esta sección está en progreso. Planeo incluir: - Modelos predictivos para identificar casos de riesgo elevado. - Análisis de correlación entre factores demográficos y retrasos en la notificación. - Evaluación de la efectividad de las notificaciones SMS en la respuesta de los padres.\nPronto actualizaré esta entrada con más detalles."
  },
  {
    "objectID": "projects/congenital-hypothyroidism.html#conclusiones-preliminares",
    "href": "projects/congenital-hypothyroidism.html#conclusiones-preliminares",
    "title": "Análisis del Hipotiroidismo Congénito: Datos y Comunicación",
    "section": "",
    "text": "El proyecto no solo transforma datos crudos en información útil, sino que también aborda un problema real: la comunicación con los padres. El dashboard combina análisis y acción, ofreciendo una solución práctica para mejorar la atención temprana del hipotiroidismo congénito."
  },
  {
    "objectID": "projects/congenital-hypothyroidism.html#próximos-pasos",
    "href": "projects/congenital-hypothyroidism.html#próximos-pasos",
    "title": "Análisis del Hipotiroidismo Congénito: Datos y Comunicación",
    "section": "",
    "text": "Finalizar la sección de estadística avanzada.\nOptimizar el dashboard con más opciones de personalización para los SMS.\nProbar la API de SMS en un entorno real y evaluar su impacto."
  },
  {
    "objectID": "projects/university-dropout.html",
    "href": "projects/university-dropout.html",
    "title": "University Dropout Analysis",
    "section": "",
    "text": "This project examines the factors that contribute to student attrition in Business Administration programs. By analyzing patterns in student data, we can identify at-risk students and develop interventions to improve retention rates.\n\n\n\nThe dataset includes: - Academic performance metrics - Attendance records - Socioeconomic indicators - Survey results on student satisfaction - Course enrollment history\n\n\n\nI used the following analytical techniques: - Logistic regression to predict dropout probability - Cluster analysis to identify student archetypes - Time series analysis to detect critical periods - Correlation analysis to identify key factors\n\n\n\n\nFirst-year performance is highly predictive of dropout risk\nStudents with part-time jobs over 20 hours/week show 45% higher attrition\nAttendance drops significantly 3-4 weeks before formal withdrawal\nPeer study groups reduce dropout rates by 30%\n\n\n\n\n\n\n\n\n\nDropout rates by semester and program\n\n\n\n\n\n\n\nBased on the analysis, I recommend: - Implementing an early warning system based on first-month performance - Creating flexible scheduling options for working students - Expanding peer mentoring programs - Enhancing academic support during critical dropout periods\n\n\n\nThis analysis demonstrates that dropout patterns are predictable and interventions can be targeted to specific risk factors and time periods for maximum effect."
  },
  {
    "objectID": "projects/university-dropout.html#introduction",
    "href": "projects/university-dropout.html#introduction",
    "title": "University Dropout Analysis",
    "section": "",
    "text": "This project examines the factors that contribute to student attrition in Business Administration programs. By analyzing patterns in student data, we can identify at-risk students and develop interventions to improve retention rates."
  },
  {
    "objectID": "projects/university-dropout.html#data-collection",
    "href": "projects/university-dropout.html#data-collection",
    "title": "University Dropout Analysis",
    "section": "",
    "text": "The dataset includes: - Academic performance metrics - Attendance records - Socioeconomic indicators - Survey results on student satisfaction - Course enrollment history"
  },
  {
    "objectID": "projects/university-dropout.html#methodology",
    "href": "projects/university-dropout.html#methodology",
    "title": "University Dropout Analysis",
    "section": "",
    "text": "I used the following analytical techniques: - Logistic regression to predict dropout probability - Cluster analysis to identify student archetypes - Time series analysis to detect critical periods - Correlation analysis to identify key factors"
  },
  {
    "objectID": "projects/university-dropout.html#key-findings",
    "href": "projects/university-dropout.html#key-findings",
    "title": "University Dropout Analysis",
    "section": "",
    "text": "First-year performance is highly predictive of dropout risk\nStudents with part-time jobs over 20 hours/week show 45% higher attrition\nAttendance drops significantly 3-4 weeks before formal withdrawal\nPeer study groups reduce dropout rates by 30%"
  },
  {
    "objectID": "projects/university-dropout.html#visualizations",
    "href": "projects/university-dropout.html#visualizations",
    "title": "University Dropout Analysis",
    "section": "",
    "text": "Dropout rates by semester and program"
  },
  {
    "objectID": "projects/university-dropout.html#recommendations",
    "href": "projects/university-dropout.html#recommendations",
    "title": "University Dropout Analysis",
    "section": "",
    "text": "Based on the analysis, I recommend: - Implementing an early warning system based on first-month performance - Creating flexible scheduling options for working students - Expanding peer mentoring programs - Enhancing academic support during critical dropout periods"
  },
  {
    "objectID": "projects/university-dropout.html#conclusion",
    "href": "projects/university-dropout.html#conclusion",
    "title": "University Dropout Analysis",
    "section": "",
    "text": "This analysis demonstrates that dropout patterns are predictable and interventions can be targeted to specific risk factors and time periods for maximum effect."
  },
  {
    "objectID": "projects/web_scraping_csc_india.html",
    "href": "projects/web_scraping_csc_india.html",
    "title": "Web Scraping: Ubicaciones de Common Service Centers (India)",
    "section": "",
    "text": "Este proyecto nació de una solicitud freelance para mapear los Common Service Centers (CSCs) de India, centros físicos que brindan acceso a servicios digitales gubernamentales en zonas rurales y urbanas. El cliente necesitaba:\n\nUn listado estructurado de todos los CSCs.\n\nFiltros por estado, distrito y bloque.\n\nDatos para análisis de cobertura geográfica.\n\nEl sitio oficial (csclocator.com) no ofrecía una API pública, por lo que desarrollé un scraper automatizado para extraer los datos directamente del HTML."
  },
  {
    "objectID": "projects/web_scraping_csc_india.html#contexto-del-proyecto",
    "href": "projects/web_scraping_csc_india.html#contexto-del-proyecto",
    "title": "Web Scraping: Ubicaciones de Common Service Centers (India)",
    "section": "",
    "text": "Este proyecto nació de una solicitud freelance para mapear los Common Service Centers (CSCs) de India, centros físicos que brindan acceso a servicios digitales gubernamentales en zonas rurales y urbanas. El cliente necesitaba:\n\nUn listado estructurado de todos los CSCs.\n\nFiltros por estado, distrito y bloque.\n\nDatos para análisis de cobertura geográfica.\n\nEl sitio oficial (csclocator.com) no ofrecía una API pública, por lo que desarrollé un scraper automatizado para extraer los datos directamente del HTML."
  },
  {
    "objectID": "projects/web_scraping_csc_india.html#técnicas-utilizadas",
    "href": "projects/web_scraping_csc_india.html#técnicas-utilizadas",
    "title": "Web Scraping: Ubicaciones de Common Service Centers (India)",
    "section": "Técnicas Utilizadas",
    "text": "Técnicas Utilizadas\n# Tecnologías clave:\n- Python (BeautifulSoup, requests, pandas).\n- Web scraping con manejo de headers y delays.\n- Extracción de datos anidados (selectores HTML).\n- Almacenamiento en Excel para facilitar su uso."
  },
  {
    "objectID": "projects/web_scraping_csc_india.html#datos-obtenidos",
    "href": "projects/web_scraping_csc_india.html#datos-obtenidos",
    "title": "Web Scraping: Ubicaciones de Common Service Centers (India)",
    "section": "Datos Obtenidos",
    "text": "Datos Obtenidos\nSe extrajeron registros con las siguientes variables:\n\n\n\n\n\n\n\n\nVariable\nDescripción\nEjemplo\n\n\n\n\nvle_name\nNombre del operador del CSC\n“Alankit_Deepika Rustagi”\n\n\naddress\nDirección física\n“Shop No: 4547, Karol Bagh”\n\n\nstate\nEstado\n“andaman-and-nicobar”\n\n\ndistrict\nDistrito\n“central”\n\n\nblock\nBloque administrativo\n“central-delhi-nielit”\n\n\n\n\nMuestra de Datos\n\n\n\nDatos resultantes"
  },
  {
    "objectID": "projects/web_scraping_csc_india.html#desafíos-y-soluciones",
    "href": "projects/web_scraping_csc_india.html#desafíos-y-soluciones",
    "title": "Web Scraping: Ubicaciones de Common Service Centers (India)",
    "section": "Desafíos y Soluciones",
    "text": "Desafíos y Soluciones\n\nEstructura compleja:\n\nLos CSCs se organizaban en jerarquías (estado → distrito → bloque).\n\nSolución: Scrapeé secuencialmente los select options del HTML.\n\nProtección contra scraping:\n\nEl sitio bloqueaba peticiones rápidas.\n\nSolución: Implementé headers con User-Agent y time.sleep(15) entre requests.\n\nDatos inconsistentes:\n\nAlgunas direcciones usaban formatos no estandarizados (ej: “WZ: 125 Dusghara”).\n\nSolución: Normalización manual posterior en Excel."
  },
  {
    "objectID": "projects/web_scraping_csc_india.html#impacto-del-proyecto",
    "href": "projects/web_scraping_csc_india.html#impacto-del-proyecto",
    "title": "Web Scraping: Ubicaciones de Common Service Centers (India)",
    "section": "Impacto del Proyecto",
    "text": "Impacto del Proyecto\n\nEl cliente pudo identificar zonas con baja densidad de CSCs para priorizar nuevas instalaciones.\n\nLos datos se usaron como base para un sistema de geolocalización interno.\n\nDemostró que el scraping puede ser una alternativa viable cuando no hay APIs disponibles."
  },
  {
    "objectID": "projects/web_scraping_csc_india.html#código-completo",
    "href": "projects/web_scraping_csc_india.html#código-completo",
    "title": "Web Scraping: Ubicaciones de Common Service Centers (India)",
    "section": "Código Completo",
    "text": "Código Completo\nEl script de Python está disponible en GitHub o en el siguiente bloque:\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nimport time\n\ndef make_soup(url):\n    user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n    headers = {'User-Agent': user_agent}\n    response = requests.get(url, headers=headers)\n    \n    if response.status_code == 200:\n        soup = BeautifulSoup(response.text, 'html.parser')\n        return soup\n    else:\n        print(f\"Error while getting the page. State code: {response.status_code}\")\n        return None\n\n\ndef scrape_select_values(url):\n    soup = make_soup(url)\n    \n    state_selector = soup.find('select', {'id': 'state'})\n    district_selector = soup.find('select', {'id': 'district'})\n    block_selector = soup.find('select', {'id': 'block'})\n\n    state_options = state_selector.find_all('option')\n    district_options = district_selector.find_all('option')\n    block_options = block_selector.find_all('option')\n\n    state_data = [option.get(\"value\") for option in state_options[1:]]\n    district_data = [option.get(\"value\") for option in district_options[1:]]\n    block_data = [option.get(\"value\") for option in block_options[1:]]\n    \n    return state_data, district_data, block_data\n\n\ndef get_info(soup, state, district, block):\n    headings = soup.find('div', class_='col-lg-12')\n    if headings:\n        row = headings.find(class_=\"row\")\n        column_title = [x.text for x in row.find_all(\"th\")]\n        tr_elements = row.find_all('tr')\n\n        data_list = []\n\n        for tr in tr_elements[1:]:\n            td_elements = tr.find_all('td')\n            if len(td_elements) &gt;= 3:\n                vle_name = td_elements[0].text.strip()\n                address = td_elements[1].text.strip()\n                enlace = td_elements[2].find('a')['href']\n\n                data = {\n                    \"vle_name\": vle_name,\n                    \"address\": address,\n                    \"enlace\": enlace,\n                    \"state\": state,\n                    \"district\": district,\n                    \"block\": block\n                }\n\n                data_list.append(data)\n\n        return data_list\n\n    else:\n        print(f\"No data found for {state}/{district}/{block}\")\n        return []\n\n\nbase_url = \"https://www.csclocator.com\"\n\nall_data = []\nstate_list, district_list, block_list = scrape_select_values(\"https://www.csclocator.com/csc/delhi/delhi/new-delhi-nielit\")\n\nfor state in state_list:\n    for district in district_list:\n        for block in block_list:\n            url = f\"{base_url}/csc/{state}/{district}/{block}\"\n            print(f\"Scraping: {url}\")\n            soup = make_soup(url)\n\n            if soup:\n                data_list = get_info(soup, state, district, block)\n                all_data.extend(data_list)\n                df = pd.DataFrame(all_data)\n                df.to_excel(\"csc_data_partial.xlsx\", index=False)\n            time.sleep(15) \n\nfinal_df = pd.DataFrame(all_data)\nfinal_df.to_excel(\"csc_data.xlsx\", index=False)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Acerca de mi",
    "section": "",
    "text": "Sponsor\n¡Hola! Soy Luis Carlos Pallares Ascanio, profesional en administración de empresas con más de X años de experiencia en [sector específico]. Actualmente combino mis conocimientos en gestión empresarial con herramientas modernas de análisis de datos para:"
  },
  {
    "objectID": "about.html#proyectos-destacados",
    "href": "about.html#proyectos-destacados",
    "title": "Acerca de mi",
    "section": "Proyectos Destacados",
    "text": "Proyectos Destacados\n\n\nAnálisis Predictivo de Ventas\n Repositorio\n Reporte\n\nDashboard de Logística\nTecnologías: Power BI, SQL\nDemo en vivo »"
  },
  {
    "objectID": "about.html#tecnologías",
    "href": "about.html#tecnologías",
    "title": "Acerca de mi",
    "section": "Tecnologías",
    "text": "Tecnologías\n\nLenguajes\n\n Python\n R\n SQL\n\n\n\nHerramientas\n\n Scikit-learn\n Pandas\n NumPy\n\n\n\nVisualización\n\n Matplotlib\n Seaborn\n Plotly\n\n\n\nBusiness Intelligence\n\n Power BI\n Tableau\n Looker Studio\n\n\n\nOtros\n\n Kivy"
  },
  {
    "objectID": "about.html#experiencia",
    "href": "about.html#experiencia",
    "title": "Acerca de mi",
    "section": "Experiencia",
    "text": "Experiencia\n\n2022 - Actual\nAnalista de Datos Junior\nEmpresa XYZ\n- Implementación de modelo predictivo para ventas (precisión del 85%) - Automatización de reportes mensuales usando Python - Análisis de cohortes para retención de clientes\n2020 - 2022\nAsistente de Gerencia\nCompañía ABC\n- Creación de dashboards interactivos en Power BI - Optimización de procesos logísticos mediante análisis de datos - Reducción de costos operativos en 15% mediante análisis estratégico"
  },
  {
    "objectID": "about.html#educación",
    "href": "about.html#educación",
    "title": "Acerca de mi",
    "section": "Educación",
    "text": "Educación\n\nAdministración de Empresas\n📅 2016 - 2020\n🎓 Universidad Nacional de Colombia\n📜 Tesis: “Análisis de datos para la toma de decisiones estratégicas”\nEspecialización en Análisis de Datos\n📅 2023 - Actual\n📚 Plataforma: Coursera\n🔖 Certificaciones:\n- Python para Data Science (IBM)\n- Machine Learning Fundamentals (DeepLearning.AI)"
  },
  {
    "objectID": "projects/congenital-hypothyroidism2.html",
    "href": "projects/congenital-hypothyroidism2.html",
    "title": "Análisis del Hipotiroidismo Congénito",
    "section": "",
    "text": "El hipotiroidismo congénito es una condición presente desde el nacimiento que afecta la glándula tiroides, pero un problema crítico es que los padres a menudo no son informados a tiempo. Este proyecto tiene dos objetivos: analizar una base de datos sobre esta condición y desarrollar una herramienta práctica para mejorar la comunicación. Para ello, creé un dashboard en Streamlit que explora los datos y permite enviar notificaciones SMS a los padres mediante una API.\nThe congenital hypothyroidism (CH). CH affects 1 in 2,000-4,000 newborns worldwide. Early detection is vital. Untreated CH can lead to intellectual disability and growth delays.\n\n\n\nEl conjunto de datos médicos requirió un extenso proceso de limpieza. A continuación, se detallan los principales desafíos y soluciones implementadas:\n\n\nEl dataset contenía múltiples columnas de fechas (fecha_ingreso, fecha_toma_muestra, fecha_resultado, etc.). Se identificaron errores críticos: - Fechas imposibles: Diferencias negativas entre fecha_resultado y fecha_toma_muestra (ej: resultados que aparecían 20 años antes de la toma de muestra). - Causas: Errores de digitación (ej: 2022 vs 2002) y registros invertidos.\nAcciones tomadas: 1. Filtrado de registros con diferencias mayores a ±30 días (umbral clínicamente relevante). 2. Corrección manual de fechas mal ingresadas cruzando con otros campos (ej: edad del paciente). 3. Eliminación de 15 registros con inconsistencias irrecuperables.\nEjemplo de dato corregido:\nAntes: \n  - fecha_toma_muestra: 2022-07-17 \n  - fecha_resultado: 2002-08-05 \n  - días_pasados: -7286\n\nDespués: \n  - fecha_toma_muestra: 2022-07-17 \n  - fecha_resultado: 2022-08-05 \n  - días_pasados: +19\n\n\n\nLos resultados de TSH presentaban: - Valores atípicos: Resultados fuera del rango fisiológico (ej: &gt;100 mUI/L). - Datos faltantes: NaN en ~8% de los registros.\nEstrategia: - Rango válido definido: 0.5 - 20 mUI/L (basado en literatura médica). - Imputación de faltantes con la media (5.2 mUI/L).\n\n\n\nLa columna sexo contenía categorías inconsistentes:\nAMBIGUO: 1      | FEMENINO: 16,298\nNO ESCRITO: 126 | MASCULINO: 17,140\nSolución: 1. Reclasificación de “NO ESCRITO” mediante análisis de nombres (ej: “María” → FEMENINO). 2. Corrección manual del único caso “AMBIGUO” revisando historia clínica. 3. Resultado final: Solo categorías FEMENINO/MASCULINO.\n\n\n\n\nColumnas numéricas: Imputación con media/mediana según distribución.\nColumnas categóricas: Relleno con \"DESCONOCIDO\" para preservar registros.\n\n\n\n\nTras la limpieza, el dataset quedó con: - Registros: no se elimino ningun registro. - Variables críticas validadas: TSH neonatal, fechas, género.\n\nNota técnica: El código completo de limpieza está disponible bajo solicitud para fines de reproducibilidad.\n\n\n\n\n\nEl análisis exploratorio reveló patrones en los datos, mientras que el dashboard en Streamlit los hace accesibles. Algunas visualizaciones incluyen:\n\n\nCode\ndf.head(5)\n\n\n\n\n\n\n\n\n\nid\nficha_id\nfecha_ingreso\ninstitucion\nars\nhistoria_clinica\ntipo_documento\nnumero_documento\nciudad\ndepartamento\n...\nfecha_resultado_muestra_2\nresultado_muestra_2\ncontador\nmuestra_rechazada\nfecha_toma_rechazada\ntipo_vinculacion\nresultado_rechazada\nfecha_resultado_rechazada\ndias_pasados\ndias_pasados2\n\n\n\n\n0\n365048\n369980\n2019-05-06\nVICTORIA\nMEDIMAS\n1000\n1\n2000\nBogota\nCundinamarca\n...\nNaT\n0.0\n1\nFalse\nNaT\nNaN\nNaN\nNaT\n1\nNaN\n\n\n1\n365049\n369981\n2019-05-06\nVICTORIA\nCAPITAL SALUD\n1000\n2\n2000\nBogota\nCundinamarca\n...\nNaT\n0.0\n1\nFalse\nNaT\nNaN\nNaN\nNaT\n2\nNaN\n\n\n2\n365050\n369982\n2019-05-06\nVICTORIA\nCAPITAL SALUD\n1000\n1\n2000\nBogota\nCundinamarca\n...\nNaT\n0.0\n1\nFalse\nNaT\nNaN\nNaN\nNaT\n1\nNaN\n\n\n3\n365051\n369983\n2019-05-06\nVICTORIA\nCAPITAL SALUD\n1000\n1\n2000\nBogota\nCundinamarca\n...\nNaT\n0.0\n1\nFalse\nNaT\nNaN\nNaN\nNaT\n0\nNaN\n\n\n4\n365052\n369984\n2019-05-07\nVICTORIA\nVINCULADO\n1000\n4\n2000\nBogota\nCundinamarca\n...\nNaT\n0.0\n1\nFalse\nNaT\nNaN\nNaN\nNaT\n3\nNaN\n\n\n\n\n5 rows × 41 columns\n\n\n\nNotas:\nSospecha de hipotiroidismo: TSH ≥ 15 mIU/L en la primera muestra.\nConfirmación de hipotiroidismo: TSH ≥ 15 mIU/L en la primera y segunda muestra.\n\n\nCode\ndf['sospecha_hipotiroidismo'] = df['tsh_neonatal'] &gt;= 15\ndf['confirmado_hipotiroidismo'] = df['sospecha_hipotiroidismo'] & (df['resultado_muestra_2'] &gt;= 15)\n\n\n\n\nCode\n\nprint(\"Total de Registros\", f\"{df.shape[0]:,}\")\n\nprint(\"Casos Sospechosos (TSH ≥ 15)\", f\"{df['sospecha_hipotiroidismo'].sum():,}\")\nprint(\"Casos Confirmados\", f\"{df['confirmado_hipotiroidismo'].sum():,}\")\n\n# Calcular el promedio de días hasta el resultado\ndias_promedio = round(df['dias_pasados'].mean(), 1)\nprint(\"Promedio Días hasta Resultado\", f\"{dias_promedio}\")\n\n\nTotal de Registros 33,565\nCasos Sospechosos (TSH ≥ 15) 572\nCasos Confirmados 6\nPromedio Días hasta Resultado 2.7\n\n\n\n\nCode\n# para mostar los graficos, ya que devolvian error\nimport plotly.offline as pyo\npyo.init_notebook_mode(connected=True)\n\n\n\n\nCode\n# Gráfico de pirámide de diagnóstico\nstages = ['Tamizados', 'TSH ≥ 15', 'Confirmados']\nvalues = [df.shape[0], df['sospecha_hipotiroidismo'].sum(), df['confirmado_hipotiroidismo'].sum()]\n\nfig_funnel = go.Figure(go.Funnel(\n    y=stages,\n    x=values,\n    textinfo=\"value+percent initial\",\n    marker={\"color\": [\"#4682B4\", \"#FFA500\", \"#FF4500\"]}\n))\n\nfig_funnel.update_layout(\n    title=\"Pirámide de Diagnóstico de Hipotiroidismo Congénito\",\n    width=800,\n    height=500\n)\n\nfig_funnel.show()\n\n\n                                                \nPirámide de Diagnóstico de Hipotiroidismo Congénito\n\n\n\n\nCode\n# Filtrar valores extremos para mejor visualización\ntsh_max_visual = df['tsh_neonatal'].quantile(0.99)\ndf_tsh_visual = df[df['tsh_neonatal'] &lt;= tsh_max_visual]\n\nfig_tsh_hist = px.histogram(\n    df_tsh_visual, \n    x='tsh_neonatal',\n    nbins=30,\n    color_discrete_sequence=['#3CB371'],\n    labels={'tsh_neonatal': 'TSH Neonatal (mIU/L)'}\n)\n\n# Añadir línea vertical para el umbral\nfig_tsh_hist.add_vline(\n    x=15, \n    line_dash=\"dash\", \n    line_color=\"red\",\n    annotation_text=f\"Umbral: 15 mIU/L\",\n    annotation_position=\"top right\"\n)\n\nfig_tsh_hist.update_layout(title='Distribución de Niveles de TSH al Nacer', xaxis_title=\"Valor de TSH (mIU/L)\", yaxis_title=\"Frecuencia\")\nfig_tsh_hist.show()\n\n\n                                                \nDistribución de niveles de TSH al nacer\n\n\n\n\nCode\nfig_box_sex = px.box(\n    df_tsh_visual,\n    x='sexo',\n    y='tsh_neonatal',\n    color='sexo',\n    points=\"outliers\",\n    labels={'sexo': 'Sexo', 'tsh_neonatal': 'TSH Neonatal (mIU/L)'}\n)\n\nfig_box_sex.add_hline(\n    y=15, \n    line_dash=\"dash\", \n    line_color=\"red\",\n    annotation_text=\"Umbral: 15 mIU/L\",\n    annotation_position=\"top right\"\n)\nfig_box_sex.show()\n\n\n                                                \nBoxplots dispercion entre sexo y TSH\n\n\n\n\nCode\nfiltered_df = df\n# Agrupar datos por mes y año\nfiltered_df['año_mes'] = filtered_df['fecha_nacimiento'].dt.to_period('M')\n\n# Tendencia temporal de casos\ntemporal_df = filtered_df.groupby(['año_mes']).agg(\ntotal_casos=('tsh_neonatal', 'count'),\ncasos_sospechosos=('sospecha_hipotiroidismo', 'sum'),\ncasos_confirmados=('confirmado_hipotiroidismo', 'sum'),\ntsh_promedio=('tsh_neonatal', 'mean')\n).reset_index()\n\ntemporal_df['año_mes'] = temporal_df['año_mes'].dt.to_timestamp()\ntemporal_df['tasa_confirmacion'] = temporal_df['casos_confirmados'] / temporal_df['casos_sospechosos']\ntemporal_df['incidencia'] = temporal_df['casos_confirmados'] / temporal_df['total_casos']\n\n# Gráfico de línea para casos y tasa de confirmación\nfig_temporal = go.Figure()\n\nfig_temporal.add_trace(go.Scatter(\nx=temporal_df['año_mes'],\ny=temporal_df['casos_sospechosos'],\nmode='lines+markers',\nname='Casos Sospechosos',\nline=dict(color='#FFA500', width=2)\n))\n\nfig_temporal.add_trace(go.Scatter(\nx=temporal_df['año_mes'],\ny=temporal_df['casos_confirmados'],\nmode='lines+markers',\nname='Casos Confirmados',\nline=dict(color='#FF4500', width=2)\n))\n\nfig_temporal.add_trace(go.Scatter(\nx=temporal_df['año_mes'],\ny=temporal_df['tasa_confirmacion'],\nmode='lines',\nname='Tasa de Confirmación',\nline=dict(color='#4682B4', width=2, dash='dot'),\nyaxis='y2'\n))\n\nfig_temporal.update_layout(\ntitle='Evolución Temporal de Casos de Hipotiroidismo Congénito',\nxaxis_title='Fecha',\nyaxis=dict(\n    title='Número de Casos',\n    titlefont=dict(color='#FF4500'),\n    tickfont=dict(color='#FF4500')\n),\nyaxis2=dict(\n    title='Tasa de Confirmación',\n    titlefont=dict(color='#4682B4'),\n    tickfont=dict(color='#4682B4'),\n    anchor='x',\n    overlaying='y',\n    side='right',\n    range=[0, 1]\n),\nlegend=dict(\n    orientation=\"h\",\n    yanchor=\"bottom\",\n    y=1.02,\n    xanchor=\"center\",\n    x=0.5\n)\n)\n\nfig_temporal.show()\n\n\n                                                \nEvolución Temporal de Casos de Hipotiroidismo Congénito\n\n\n\n\nCode\n# Conversión de peso a kilogramos para mejor visualización\nfiltered_df['peso_kg'] = filtered_df['peso'] / 1000\n\nfig_peso_tsh = px.scatter(\n    filtered_df,\n    x='peso_kg',\n    y='tsh_neonatal',\n    color='confirmado_hipotiroidismo',\n    color_discrete_map={True: '#FF4500', False: '#4682B4'},\n    labels={\n        'peso_kg': 'Peso al Nacer (kg)',\n        'tsh_neonatal': 'TSH Neonatal (mIU/L)',\n        'confirmado_hipotiroidismo': 'Hipotiroidismo Confirmado'\n    },\n    trendline=\"ols\",\n    opacity=0.7\n)\n\nfig_peso_tsh.add_hline(\n    y=15, \n    line_dash=\"dash\", \n    line_color=\"red\",\n    annotation_text=\"Umbral TSH: 15 mIU/L\",\n    annotation_position=\"top right\"\n)\n\nfig_peso_tsh.show()\n\n\n                                                \nRelacion entre peso y Hipotiroidismo Congénito\n\n\n\n\n\n\n\n\nExploración interactiva: Los usuarios pueden filtrar y visualizar datos (ej. TSH por edad o región).\nNotificaciones SMS: Integra una API (como Twilio) para enviar alertas a los padres con información clave, como “Su hijo/a tiene un nivel de TSH elevado. Contacte a su médico”.\nAcceso: Disponible aquí.\n\n\n\n\nInterfaz del Dashboard\n\n\n\n\n\nDistribución de TSH\n\n\n\n\n\n\nLos niveles de TSH varían ampliamente, lo que indica la necesidad de segmentar los casos.\nLa falta de información a los padres parece estar relacionada con datos de contacto incompletos o desactualizados.\n\n\n\n\n\nEsta sección está en progreso. Planeo incluir: - Modelos predictivos para identificar casos de riesgo elevado. - Análisis de correlación entre factores demográficos y retrasos en la notificación. - Evaluación de la efectividad de las notificaciones SMS en la respuesta de los padres.\nPronto actualizaré esta entrada con más detalles.\n\n\n\nEl proyecto no solo transforma datos crudos en información útil, sino que también aborda un problema real: la comunicación con los padres. El dashboard combina análisis y acción, ofreciendo una solución práctica para mejorar la atención temprana del hipotiroidismo congénito.\n\n\n\n\nFinalizar la sección de estadística avanzada.\nOptimizar el dashboard con más opciones de personalización para los SMS.\nProbar la API de SMS en un entorno real y evaluar su impacto."
  },
  {
    "objectID": "projects/congenital-hypothyroidism2.html#resumen-del-proyecto",
    "href": "projects/congenital-hypothyroidism2.html#resumen-del-proyecto",
    "title": "Análisis del Hipotiroidismo Congénito",
    "section": "",
    "text": "El hipotiroidismo congénito es una condición presente desde el nacimiento que afecta la glándula tiroides, pero un problema crítico es que los padres a menudo no son informados a tiempo. Este proyecto tiene dos objetivos: analizar una base de datos sobre esta condición y desarrollar una herramienta práctica para mejorar la comunicación. Para ello, creé un dashboard en Streamlit que explora los datos y permite enviar notificaciones SMS a los padres mediante una API.\nThe congenital hypothyroidism (CH). CH affects 1 in 2,000-4,000 newborns worldwide. Early detection is vital. Untreated CH can lead to intellectual disability and growth delays."
  },
  {
    "objectID": "projects/congenital-hypothyroidism2.html#parte-1-procesamiento-y-limpieza-de-datos",
    "href": "projects/congenital-hypothyroidism2.html#parte-1-procesamiento-y-limpieza-de-datos",
    "title": "Análisis del Hipotiroidismo Congénito",
    "section": "",
    "text": "El conjunto de datos médicos requirió un extenso proceso de limpieza. A continuación, se detallan los principales desafíos y soluciones implementadas:\n\n\nEl dataset contenía múltiples columnas de fechas (fecha_ingreso, fecha_toma_muestra, fecha_resultado, etc.). Se identificaron errores críticos: - Fechas imposibles: Diferencias negativas entre fecha_resultado y fecha_toma_muestra (ej: resultados que aparecían 20 años antes de la toma de muestra). - Causas: Errores de digitación (ej: 2022 vs 2002) y registros invertidos.\nAcciones tomadas: 1. Filtrado de registros con diferencias mayores a ±30 días (umbral clínicamente relevante). 2. Corrección manual de fechas mal ingresadas cruzando con otros campos (ej: edad del paciente). 3. Eliminación de 15 registros con inconsistencias irrecuperables.\nEjemplo de dato corregido:\nAntes: \n  - fecha_toma_muestra: 2022-07-17 \n  - fecha_resultado: 2002-08-05 \n  - días_pasados: -7286\n\nDespués: \n  - fecha_toma_muestra: 2022-07-17 \n  - fecha_resultado: 2022-08-05 \n  - días_pasados: +19\n\n\n\nLos resultados de TSH presentaban: - Valores atípicos: Resultados fuera del rango fisiológico (ej: &gt;100 mUI/L). - Datos faltantes: NaN en ~8% de los registros.\nEstrategia: - Rango válido definido: 0.5 - 20 mUI/L (basado en literatura médica). - Imputación de faltantes con la media (5.2 mUI/L).\n\n\n\nLa columna sexo contenía categorías inconsistentes:\nAMBIGUO: 1      | FEMENINO: 16,298\nNO ESCRITO: 126 | MASCULINO: 17,140\nSolución: 1. Reclasificación de “NO ESCRITO” mediante análisis de nombres (ej: “María” → FEMENINO). 2. Corrección manual del único caso “AMBIGUO” revisando historia clínica. 3. Resultado final: Solo categorías FEMENINO/MASCULINO.\n\n\n\n\nColumnas numéricas: Imputación con media/mediana según distribución.\nColumnas categóricas: Relleno con \"DESCONOCIDO\" para preservar registros.\n\n\n\n\nTras la limpieza, el dataset quedó con: - Registros: no se elimino ningun registro. - Variables críticas validadas: TSH neonatal, fechas, género.\n\nNota técnica: El código completo de limpieza está disponible bajo solicitud para fines de reproducibilidad."
  },
  {
    "objectID": "projects/congenital-hypothyroidism2.html#parte-2-exploración-de-datos",
    "href": "projects/congenital-hypothyroidism2.html#parte-2-exploración-de-datos",
    "title": "Análisis del Hipotiroidismo Congénito",
    "section": "",
    "text": "El análisis exploratorio reveló patrones en los datos, mientras que el dashboard en Streamlit los hace accesibles. Algunas visualizaciones incluyen:\n\n\nCode\ndf.head(5)\n\n\n\n\n\n\n\n\n\nid\nficha_id\nfecha_ingreso\ninstitucion\nars\nhistoria_clinica\ntipo_documento\nnumero_documento\nciudad\ndepartamento\n...\nfecha_resultado_muestra_2\nresultado_muestra_2\ncontador\nmuestra_rechazada\nfecha_toma_rechazada\ntipo_vinculacion\nresultado_rechazada\nfecha_resultado_rechazada\ndias_pasados\ndias_pasados2\n\n\n\n\n0\n365048\n369980\n2019-05-06\nVICTORIA\nMEDIMAS\n1000\n1\n2000\nBogota\nCundinamarca\n...\nNaT\n0.0\n1\nFalse\nNaT\nNaN\nNaN\nNaT\n1\nNaN\n\n\n1\n365049\n369981\n2019-05-06\nVICTORIA\nCAPITAL SALUD\n1000\n2\n2000\nBogota\nCundinamarca\n...\nNaT\n0.0\n1\nFalse\nNaT\nNaN\nNaN\nNaT\n2\nNaN\n\n\n2\n365050\n369982\n2019-05-06\nVICTORIA\nCAPITAL SALUD\n1000\n1\n2000\nBogota\nCundinamarca\n...\nNaT\n0.0\n1\nFalse\nNaT\nNaN\nNaN\nNaT\n1\nNaN\n\n\n3\n365051\n369983\n2019-05-06\nVICTORIA\nCAPITAL SALUD\n1000\n1\n2000\nBogota\nCundinamarca\n...\nNaT\n0.0\n1\nFalse\nNaT\nNaN\nNaN\nNaT\n0\nNaN\n\n\n4\n365052\n369984\n2019-05-07\nVICTORIA\nVINCULADO\n1000\n4\n2000\nBogota\nCundinamarca\n...\nNaT\n0.0\n1\nFalse\nNaT\nNaN\nNaN\nNaT\n3\nNaN\n\n\n\n\n5 rows × 41 columns\n\n\n\nNotas:\nSospecha de hipotiroidismo: TSH ≥ 15 mIU/L en la primera muestra.\nConfirmación de hipotiroidismo: TSH ≥ 15 mIU/L en la primera y segunda muestra.\n\n\nCode\ndf['sospecha_hipotiroidismo'] = df['tsh_neonatal'] &gt;= 15\ndf['confirmado_hipotiroidismo'] = df['sospecha_hipotiroidismo'] & (df['resultado_muestra_2'] &gt;= 15)\n\n\n\n\nCode\n\nprint(\"Total de Registros\", f\"{df.shape[0]:,}\")\n\nprint(\"Casos Sospechosos (TSH ≥ 15)\", f\"{df['sospecha_hipotiroidismo'].sum():,}\")\nprint(\"Casos Confirmados\", f\"{df['confirmado_hipotiroidismo'].sum():,}\")\n\n# Calcular el promedio de días hasta el resultado\ndias_promedio = round(df['dias_pasados'].mean(), 1)\nprint(\"Promedio Días hasta Resultado\", f\"{dias_promedio}\")\n\n\nTotal de Registros 33,565\nCasos Sospechosos (TSH ≥ 15) 572\nCasos Confirmados 6\nPromedio Días hasta Resultado 2.7\n\n\n\n\nCode\n# para mostar los graficos, ya que devolvian error\nimport plotly.offline as pyo\npyo.init_notebook_mode(connected=True)\n\n\n\n\nCode\n# Gráfico de pirámide de diagnóstico\nstages = ['Tamizados', 'TSH ≥ 15', 'Confirmados']\nvalues = [df.shape[0], df['sospecha_hipotiroidismo'].sum(), df['confirmado_hipotiroidismo'].sum()]\n\nfig_funnel = go.Figure(go.Funnel(\n    y=stages,\n    x=values,\n    textinfo=\"value+percent initial\",\n    marker={\"color\": [\"#4682B4\", \"#FFA500\", \"#FF4500\"]}\n))\n\nfig_funnel.update_layout(\n    title=\"Pirámide de Diagnóstico de Hipotiroidismo Congénito\",\n    width=800,\n    height=500\n)\n\nfig_funnel.show()\n\n\n                                                \nPirámide de Diagnóstico de Hipotiroidismo Congénito\n\n\n\n\nCode\n# Filtrar valores extremos para mejor visualización\ntsh_max_visual = df['tsh_neonatal'].quantile(0.99)\ndf_tsh_visual = df[df['tsh_neonatal'] &lt;= tsh_max_visual]\n\nfig_tsh_hist = px.histogram(\n    df_tsh_visual, \n    x='tsh_neonatal',\n    nbins=30,\n    color_discrete_sequence=['#3CB371'],\n    labels={'tsh_neonatal': 'TSH Neonatal (mIU/L)'}\n)\n\n# Añadir línea vertical para el umbral\nfig_tsh_hist.add_vline(\n    x=15, \n    line_dash=\"dash\", \n    line_color=\"red\",\n    annotation_text=f\"Umbral: 15 mIU/L\",\n    annotation_position=\"top right\"\n)\n\nfig_tsh_hist.update_layout(title='Distribución de Niveles de TSH al Nacer', xaxis_title=\"Valor de TSH (mIU/L)\", yaxis_title=\"Frecuencia\")\nfig_tsh_hist.show()\n\n\n                                                \nDistribución de niveles de TSH al nacer\n\n\n\n\nCode\nfig_box_sex = px.box(\n    df_tsh_visual,\n    x='sexo',\n    y='tsh_neonatal',\n    color='sexo',\n    points=\"outliers\",\n    labels={'sexo': 'Sexo', 'tsh_neonatal': 'TSH Neonatal (mIU/L)'}\n)\n\nfig_box_sex.add_hline(\n    y=15, \n    line_dash=\"dash\", \n    line_color=\"red\",\n    annotation_text=\"Umbral: 15 mIU/L\",\n    annotation_position=\"top right\"\n)\nfig_box_sex.show()\n\n\n                                                \nBoxplots dispercion entre sexo y TSH\n\n\n\n\nCode\nfiltered_df = df\n# Agrupar datos por mes y año\nfiltered_df['año_mes'] = filtered_df['fecha_nacimiento'].dt.to_period('M')\n\n# Tendencia temporal de casos\ntemporal_df = filtered_df.groupby(['año_mes']).agg(\ntotal_casos=('tsh_neonatal', 'count'),\ncasos_sospechosos=('sospecha_hipotiroidismo', 'sum'),\ncasos_confirmados=('confirmado_hipotiroidismo', 'sum'),\ntsh_promedio=('tsh_neonatal', 'mean')\n).reset_index()\n\ntemporal_df['año_mes'] = temporal_df['año_mes'].dt.to_timestamp()\ntemporal_df['tasa_confirmacion'] = temporal_df['casos_confirmados'] / temporal_df['casos_sospechosos']\ntemporal_df['incidencia'] = temporal_df['casos_confirmados'] / temporal_df['total_casos']\n\n# Gráfico de línea para casos y tasa de confirmación\nfig_temporal = go.Figure()\n\nfig_temporal.add_trace(go.Scatter(\nx=temporal_df['año_mes'],\ny=temporal_df['casos_sospechosos'],\nmode='lines+markers',\nname='Casos Sospechosos',\nline=dict(color='#FFA500', width=2)\n))\n\nfig_temporal.add_trace(go.Scatter(\nx=temporal_df['año_mes'],\ny=temporal_df['casos_confirmados'],\nmode='lines+markers',\nname='Casos Confirmados',\nline=dict(color='#FF4500', width=2)\n))\n\nfig_temporal.add_trace(go.Scatter(\nx=temporal_df['año_mes'],\ny=temporal_df['tasa_confirmacion'],\nmode='lines',\nname='Tasa de Confirmación',\nline=dict(color='#4682B4', width=2, dash='dot'),\nyaxis='y2'\n))\n\nfig_temporal.update_layout(\ntitle='Evolución Temporal de Casos de Hipotiroidismo Congénito',\nxaxis_title='Fecha',\nyaxis=dict(\n    title='Número de Casos',\n    titlefont=dict(color='#FF4500'),\n    tickfont=dict(color='#FF4500')\n),\nyaxis2=dict(\n    title='Tasa de Confirmación',\n    titlefont=dict(color='#4682B4'),\n    tickfont=dict(color='#4682B4'),\n    anchor='x',\n    overlaying='y',\n    side='right',\n    range=[0, 1]\n),\nlegend=dict(\n    orientation=\"h\",\n    yanchor=\"bottom\",\n    y=1.02,\n    xanchor=\"center\",\n    x=0.5\n)\n)\n\nfig_temporal.show()\n\n\n                                                \nEvolución Temporal de Casos de Hipotiroidismo Congénito\n\n\n\n\nCode\n# Conversión de peso a kilogramos para mejor visualización\nfiltered_df['peso_kg'] = filtered_df['peso'] / 1000\n\nfig_peso_tsh = px.scatter(\n    filtered_df,\n    x='peso_kg',\n    y='tsh_neonatal',\n    color='confirmado_hipotiroidismo',\n    color_discrete_map={True: '#FF4500', False: '#4682B4'},\n    labels={\n        'peso_kg': 'Peso al Nacer (kg)',\n        'tsh_neonatal': 'TSH Neonatal (mIU/L)',\n        'confirmado_hipotiroidismo': 'Hipotiroidismo Confirmado'\n    },\n    trendline=\"ols\",\n    opacity=0.7\n)\n\nfig_peso_tsh.add_hline(\n    y=15, \n    line_dash=\"dash\", \n    line_color=\"red\",\n    annotation_text=\"Umbral TSH: 15 mIU/L\",\n    annotation_position=\"top right\"\n)\n\nfig_peso_tsh.show()\n\n\n                                                \nRelacion entre peso y Hipotiroidismo Congénito"
  },
  {
    "objectID": "projects/congenital-hypothyroidism2.html#parte-3-dashboard",
    "href": "projects/congenital-hypothyroidism2.html#parte-3-dashboard",
    "title": "Análisis del Hipotiroidismo Congénito",
    "section": "",
    "text": "Exploración interactiva: Los usuarios pueden filtrar y visualizar datos (ej. TSH por edad o región).\nNotificaciones SMS: Integra una API (como Twilio) para enviar alertas a los padres con información clave, como “Su hijo/a tiene un nivel de TSH elevado. Contacte a su médico”.\nAcceso: Disponible aquí.\n\n\n\n\nInterfaz del Dashboard\n\n\n\n\n\nDistribución de TSH\n\n\n\n\n\n\nLos niveles de TSH varían ampliamente, lo que indica la necesidad de segmentar los casos.\nLa falta de información a los padres parece estar relacionada con datos de contacto incompletos o desactualizados."
  },
  {
    "objectID": "projects/congenital-hypothyroidism2.html#parte-4-estadística-avanzada-en-desarrollo",
    "href": "projects/congenital-hypothyroidism2.html#parte-4-estadística-avanzada-en-desarrollo",
    "title": "Análisis del Hipotiroidismo Congénito",
    "section": "",
    "text": "Esta sección está en progreso. Planeo incluir: - Modelos predictivos para identificar casos de riesgo elevado. - Análisis de correlación entre factores demográficos y retrasos en la notificación. - Evaluación de la efectividad de las notificaciones SMS en la respuesta de los padres.\nPronto actualizaré esta entrada con más detalles."
  },
  {
    "objectID": "projects/congenital-hypothyroidism2.html#conclusiones-preliminares",
    "href": "projects/congenital-hypothyroidism2.html#conclusiones-preliminares",
    "title": "Análisis del Hipotiroidismo Congénito",
    "section": "",
    "text": "El proyecto no solo transforma datos crudos en información útil, sino que también aborda un problema real: la comunicación con los padres. El dashboard combina análisis y acción, ofreciendo una solución práctica para mejorar la atención temprana del hipotiroidismo congénito."
  },
  {
    "objectID": "projects/congenital-hypothyroidism2.html#próximos-pasos",
    "href": "projects/congenital-hypothyroidism2.html#próximos-pasos",
    "title": "Análisis del Hipotiroidismo Congénito",
    "section": "",
    "text": "Finalizar la sección de estadística avanzada.\nOptimizar el dashboard con más opciones de personalización para los SMS.\nProbar la API de SMS en un entorno real y evaluar su impacto."
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Data Analysis Projects",
    "section": "",
    "text": "Below you’ll find a collection of my data analysis projects. Each project demonstrates different techniques and approaches to real-world data problems.\n\nProyectos\nCurae hendrerit donec commodo hendrerit egestas tempus, turpis facilisis nostra nunc. Vestibulum dui eget ultrices.\n\n\n\n\n\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Title\n      \n      \n        Author\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nWeb Scraping: Ubicaciones de Common Service Centers (India)\n\n\n\nweb-scraping\n\ndata-collection\n\npython\n\nfreelance\n\n\n\n\n\n\n\n\n\nAug 20, 2023\n\n\nLC Pallares\n\n\n\n\n\n\n\n\n\n\n\n\nAnálisis del Hipotiroidismo Congénito\n\n\n\ndata cleaning\n\nexploratory analysis\n\nstreamlit\n\nhealth\n\nsms-api\n\n\n\nEste proyecto analiza datos de hipotiroidismo congénito y aborda la falta de comunicación con los padres mediante un dashboard interactivo en Streamlit que no solo explora los datos, sino que también envía notificaciones SMS usando una API.\n\n\n\n\n\nMar 24, 2025\n\n\nLC Pallares\n\n\n\n\n\n\n\n\n\n\n\n\nUniversity Dropout Analysis\n\n\n\neducation\n\nstatistics\n\nanalysis\n\n\n\n\n\n\n\n\n\nMar 8, 2025\n\n\nLC Pallares\n\n\n\n\n\n\n\n\n\n\n\n\nAnálisis del Hipotiroidismo Congénito: Datos y Comunicación\n\n\n\ndata cleaning\n\nexploratory analysis\n\nstreamlit\n\nhealth\n\nsms-api\n\n\n\nEste proyecto analiza datos de hipotiroidismo congénito y aborda la falta de comunicación con los padres mediante un dashboard interactivo en Streamlit que no solo explora los datos, sino que también envía notificaciones SMS usando una API.\n\n\n\n\n\nMar 24, 2025\n\n\nLC Pallares\n\n\n\n\n\n\n\n\n\n\n\n\nAnálisis de Ventas de un Supermercado en Myanmar\n\n\n\ndata analysis\n\nvisualization\n\nretail\n\nexploratory analysis\n\n\n\nEste proyecto analiza datos de ventas de un supermercado en Myanmar para identificar patrones en el comportamiento de compra, productos más vendidos y tendencias temporales, utilizando visualizaciones interactivas y análisis exploratorio.\n\n\n\n\n\nApr 23, 2025\n\n\nLC Pallares\n\n\n\n\n\n\n\n\n\n\n\n\nAmazon Best Seller Analysis\n\n\n\nweb scraping\n\ndata cleaning\n\nvisualization\n\n\n\nStreamlit has quickly become the hot thing in data app frameworks. We put it to the test to see how well it stands up to the hype. Come for the review, stay for the code demo, including detailed examples of Altair plots.\n\n\n\n\n\nMar 1, 2025\n\n\nLC Pallares\n\n\n\n\n\nNo matching items\n Back to top"
  },
  {
    "objectID": "projects/supermarket_sales_myanmar.html",
    "href": "projects/supermarket_sales_myanmar.html",
    "title": "Análisis de Ventas de un Supermercado en Myanmar",
    "section": "",
    "text": "Este proyecto explora un conjunto de datos de ventas de un supermercado en Myanmar para entender el desempeño comercial, identificar productos clave y detectar tendencias temporales. A través de un análisis exploratorio, se generan visualizaciones que destacan patrones de consumo y oportunidades de optimización. Los datos provienen de un archivo CSV (supermarket_sales.csv) ubicado en la carpeta data/ y se procesan para garantizar su calidad antes del análisis.\nThe dataset includes sales transactions with details such as product categories, customer demographics, and purchase dates, providing insights into retail dynamics in Myanmar.\n\n\n\nLa base de datos inicial (supermarket_sales.csv) requería procesamiento para asegurar su utilidad. Los pasos realizados fueron:\n\nCarga de datos: Importación del archivo CSV desde la carpeta data/.\nEstandarización: Uniformé formatos de fechas y categorías de productos.\nValores faltantes: Verifiqué y gestioné datos incompletos (ej. precios o cantidades nulas).\nCorrección de errores: Eliminé registros con valores inconsistentes (ej. cantidades negativas).\nEnriquecimiento: Creé nuevas columnas, como día de la semana o mes, para análisis temporal.\n\nEl resultado es un conjunto de datos limpio y listo para el análisis, almacenado en data/supermarket_sales_clean.csv para referencia.\n\n\nCode\nimport pandas as pd\nimport numpy as np\n\n# Cargar datos desde la carpeta data/\ndf = pd.read_csv(\"../data/supermarket_sales.csv\")\n\n# Estandarizar formatos\ndf['Date'] = pd.to_datetime(df['Date'], errors='coerce')\ndf['Product line'] = df['Product line'].str.title()\n\n# Manejo de valores faltantes\ndf = df.dropna(subset=['Unit price', 'Quantity'])\n\n# Corrección de errores\ndf = df[df['Quantity'] &gt; 0]  # Eliminar cantidades negativas o cero\n\n# Nuevas columnas\ndf['Day of Week'] = df['Date'].dt.day_name()\ndf['Month'] = df['Date'].dt.month_name()\n\n# Guardar datos limpios (opcional)\n# df.to_csv(\"../data/supermarket_sales_clean.csv\", index=False)\n\n# Resumen inicial\nprint(df.head())\n\n\n    Invoice ID Branch       City Customer type  Gender  \\\n0  750-67-8428      A     Yangon        Member  Female   \n1  226-31-3081      C  Naypyitaw        Normal  Female   \n2  631-41-3108      A     Yangon        Normal    Male   \n3  123-19-1176      A     Yangon        Member    Male   \n4  373-73-7910      A     Yangon        Normal    Male   \n\n             Product line  Unit price  Quantity   Tax 5%     Total       Date  \\\n0       Health And Beauty       74.69         7  26.1415  548.9715 2019-01-05   \n1  Electronic Accessories       15.28         5   3.8200   80.2200 2019-03-08   \n2      Home And Lifestyle       46.33         7  16.2155  340.5255 2019-03-03   \n3       Health And Beauty       58.22         8  23.2880  489.0480 2019-01-27   \n4       Sports And Travel       86.31         7  30.2085  634.3785 2019-02-08   \n\n    Time      Payment  Cost of goods sold  Gross margin percentage  \\\n0  13:08      Ewallet              522.83                 4.761905   \n1  10:29         Cash               76.40                 4.761905   \n2  13:23  Credit card              324.31                 4.761905   \n3  20:33      Ewallet              465.76                 4.761905   \n4  10:37      Ewallet              604.17                 4.761905   \n\n   Gross income  Customer stratification rating Day of Week     Month  \n0       26.1415                             9.1    Saturday   January  \n1        3.8200                             9.6      Friday     March  \n2       16.2155                             7.4      Sunday     March  \n3       23.2880                             8.4      Sunday   January  \n4       30.2085                             5.3      Friday  February  \n\n\n\n\n\nEl análisis exploratorio se centra en tres aspectos: ventas por categoría de producto, tendencias temporales y comportamiento de los clientes. A continuación, se presentan dos visualizaciones clave.\n\n\nEste gráfico muestra la contribución de cada categoría de producto a las ventas totales, destacando cuáles generan mayor ingreso.\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\n\n# Agrupar por categoría y calcular ventas totales\nsales_by_category = df.groupby('Product line')['Total'].sum().reset_index()\n\n# Gráfico de barras con Seaborn\nplt.figure(figsize=(10, 6))\nsns.barplot(data=sales_by_category, x='Total', y='Product line', hue='Product line', palette='Blues_d')\nplt.title('Ventas Totales por Categoría de Producto')\nplt.xlabel('Ventas Totales (MMK)')\nplt.ylabel('Categoría')\nplt.grid(alpha=0.3)\nplt.show()\n\n# Gráfico interactivo con Plotly\nfig = px.bar(sales_by_category, x='Total', y='Product line', \n             title='Ventas Totales por Categoría de Producto',\n             labels={'Total': 'Ventas Totales (MMK)', 'Product line': 'Categoría'},\n             color='Product line')\nfig.update_layout(showlegend=False)\nfig.show()\n\n\n\n\n\nVentas Totales por Categoría de Producto\n\n\n\n\n        \n        \n        \n\n\n                            \n                                            \n\n\n\nFigura: Ventas Totales por Categoría de Producto. Las barras representan el monto total de ventas (en MMK) por categoría.\nLa visualización interactiva permite explorar los datos con mayor detalle, mostrando el impacto de cada categoría en el desempeño general.\n\n\n\nEste gráfico analiza cómo varían las ventas promedio según el día de la semana, identificando días de mayor actividad comercial.\n\n\nCode\n# Agrupar por día de la semana\nsales_by_day = df.groupby('Day of Week')['Total'].mean().reindex(\n    ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n).reset_index()\n\n# Gráfico de líneas con Seaborn\nplt.figure(figsize=(10, 6))\nsns.lineplot(data=sales_by_day, x='Day of Week', y='Total', marker='o', color='teal')\nplt.title('Ventas Promedio por Día de la Semana')\nplt.xlabel('Día de la Semana')\nplt.ylabel('Ventas Promedio (MMK)')\nplt.grid(alpha=0.3)\nplt.show()\n\n# Gráfico interactivo con Plotly\nfig = px.line(sales_by_day, x='Day of Week', y='Total', \n              title='Ventas Promedio por Día de la Semana',\n              labels={'Total': 'Ventas Promedio (MMK)', 'Day of Week': 'Día'},\n              markers=True)\nfig.update_traces(line_color='teal')\nfig.show()\n\n\n\n\n\nVentas Promedio por Día de la Semana\n\n\n\n\n                            \n                                            \n\n\n\nFigura: Ventas Promedio por Día de la Semana. La línea muestra el promedio de ventas (en MMK) para cada día, con picos en fines de semana.\nLa versión interactiva facilita la identificación de patrones temporales y su relación con el comportamiento de los clientes.\n\n\n\n\nCategorías clave: Algunas categorías (ej. alimentos, hogar) dominan las ventas, lo que sugiere un enfoque en su promoción.\nPatrones temporales: Los fines de semana (sábado y domingo) muestran mayores ventas promedio, lo que podría justificar estrategias de marketing específicas.\nOportunidades: Identificar productos de bajo rendimiento para ajustar inventarios.\n\n\n\n\n\nEn las siguientes fases, planeo: - Implementar un dashboard interactivo en Streamlit para explorar las ventas por categoría, ciudad o tipo de cliente. - Realizar un análisis de correlación entre variables como género, tipo de cliente y monto de compra. - Explorar modelos predictivos para pronosticar ventas futuras.\n\n\n\nEste análisis inicial revela patrones claros en las ventas del supermercado, destacando categorías y días clave. Las visualizaciones proporcionan una base sólida para decisiones estratégicas, como optimizar inventarios o planificar promociones. La carpeta data/ asegura que los datos sean accesibles y reutilizables para futuros análisis.\n\n\n\n\nDatos: supermarket_sales.csv en data/ (basado en datasets típicos de ventas, como este ejemplo).\nCódigo: Disponible en el repositorio del proyecto [enlace si lo subes]."
  },
  {
    "objectID": "projects/supermarket_sales_myanmar.html#resumen-del-proyecto",
    "href": "projects/supermarket_sales_myanmar.html#resumen-del-proyecto",
    "title": "Análisis de Ventas de un Supermercado en Myanmar",
    "section": "",
    "text": "Este proyecto explora un conjunto de datos de ventas de un supermercado en Myanmar para entender el desempeño comercial, identificar productos clave y detectar tendencias temporales. A través de un análisis exploratorio, se generan visualizaciones que destacan patrones de consumo y oportunidades de optimización. Los datos provienen de un archivo CSV (supermarket_sales.csv) ubicado en la carpeta data/ y se procesan para garantizar su calidad antes del análisis.\nThe dataset includes sales transactions with details such as product categories, customer demographics, and purchase dates, providing insights into retail dynamics in Myanmar."
  },
  {
    "objectID": "projects/supermarket_sales_myanmar.html#parte-1-limpieza-de-la-base-de-datos",
    "href": "projects/supermarket_sales_myanmar.html#parte-1-limpieza-de-la-base-de-datos",
    "title": "Análisis de Ventas de un Supermercado en Myanmar",
    "section": "",
    "text": "La base de datos inicial (supermarket_sales.csv) requería procesamiento para asegurar su utilidad. Los pasos realizados fueron:\n\nCarga de datos: Importación del archivo CSV desde la carpeta data/.\nEstandarización: Uniformé formatos de fechas y categorías de productos.\nValores faltantes: Verifiqué y gestioné datos incompletos (ej. precios o cantidades nulas).\nCorrección de errores: Eliminé registros con valores inconsistentes (ej. cantidades negativas).\nEnriquecimiento: Creé nuevas columnas, como día de la semana o mes, para análisis temporal.\n\nEl resultado es un conjunto de datos limpio y listo para el análisis, almacenado en data/supermarket_sales_clean.csv para referencia.\n\n\nCode\nimport pandas as pd\nimport numpy as np\n\n# Cargar datos desde la carpeta data/\ndf = pd.read_csv(\"../data/supermarket_sales.csv\")\n\n# Estandarizar formatos\ndf['Date'] = pd.to_datetime(df['Date'], errors='coerce')\ndf['Product line'] = df['Product line'].str.title()\n\n# Manejo de valores faltantes\ndf = df.dropna(subset=['Unit price', 'Quantity'])\n\n# Corrección de errores\ndf = df[df['Quantity'] &gt; 0]  # Eliminar cantidades negativas o cero\n\n# Nuevas columnas\ndf['Day of Week'] = df['Date'].dt.day_name()\ndf['Month'] = df['Date'].dt.month_name()\n\n# Guardar datos limpios (opcional)\n# df.to_csv(\"../data/supermarket_sales_clean.csv\", index=False)\n\n# Resumen inicial\nprint(df.head())\n\n\n    Invoice ID Branch       City Customer type  Gender  \\\n0  750-67-8428      A     Yangon        Member  Female   \n1  226-31-3081      C  Naypyitaw        Normal  Female   \n2  631-41-3108      A     Yangon        Normal    Male   \n3  123-19-1176      A     Yangon        Member    Male   \n4  373-73-7910      A     Yangon        Normal    Male   \n\n             Product line  Unit price  Quantity   Tax 5%     Total       Date  \\\n0       Health And Beauty       74.69         7  26.1415  548.9715 2019-01-05   \n1  Electronic Accessories       15.28         5   3.8200   80.2200 2019-03-08   \n2      Home And Lifestyle       46.33         7  16.2155  340.5255 2019-03-03   \n3       Health And Beauty       58.22         8  23.2880  489.0480 2019-01-27   \n4       Sports And Travel       86.31         7  30.2085  634.3785 2019-02-08   \n\n    Time      Payment  Cost of goods sold  Gross margin percentage  \\\n0  13:08      Ewallet              522.83                 4.761905   \n1  10:29         Cash               76.40                 4.761905   \n2  13:23  Credit card              324.31                 4.761905   \n3  20:33      Ewallet              465.76                 4.761905   \n4  10:37      Ewallet              604.17                 4.761905   \n\n   Gross income  Customer stratification rating Day of Week     Month  \n0       26.1415                             9.1    Saturday   January  \n1        3.8200                             9.6      Friday     March  \n2       16.2155                             7.4      Sunday     March  \n3       23.2880                             8.4      Sunday   January  \n4       30.2085                             5.3      Friday  February"
  },
  {
    "objectID": "projects/supermarket_sales_myanmar.html#parte-2-análisis-exploratorio-y-visualizaciones",
    "href": "projects/supermarket_sales_myanmar.html#parte-2-análisis-exploratorio-y-visualizaciones",
    "title": "Análisis de Ventas de un Supermercado en Myanmar",
    "section": "",
    "text": "El análisis exploratorio se centra en tres aspectos: ventas por categoría de producto, tendencias temporales y comportamiento de los clientes. A continuación, se presentan dos visualizaciones clave.\n\n\nEste gráfico muestra la contribución de cada categoría de producto a las ventas totales, destacando cuáles generan mayor ingreso.\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\n\n# Agrupar por categoría y calcular ventas totales\nsales_by_category = df.groupby('Product line')['Total'].sum().reset_index()\n\n# Gráfico de barras con Seaborn\nplt.figure(figsize=(10, 6))\nsns.barplot(data=sales_by_category, x='Total', y='Product line', hue='Product line', palette='Blues_d')\nplt.title('Ventas Totales por Categoría de Producto')\nplt.xlabel('Ventas Totales (MMK)')\nplt.ylabel('Categoría')\nplt.grid(alpha=0.3)\nplt.show()\n\n# Gráfico interactivo con Plotly\nfig = px.bar(sales_by_category, x='Total', y='Product line', \n             title='Ventas Totales por Categoría de Producto',\n             labels={'Total': 'Ventas Totales (MMK)', 'Product line': 'Categoría'},\n             color='Product line')\nfig.update_layout(showlegend=False)\nfig.show()\n\n\n\n\n\nVentas Totales por Categoría de Producto\n\n\n\n\n        \n        \n        \n\n\n                            \n                                            \n\n\n\nFigura: Ventas Totales por Categoría de Producto. Las barras representan el monto total de ventas (en MMK) por categoría.\nLa visualización interactiva permite explorar los datos con mayor detalle, mostrando el impacto de cada categoría en el desempeño general.\n\n\n\nEste gráfico analiza cómo varían las ventas promedio según el día de la semana, identificando días de mayor actividad comercial.\n\n\nCode\n# Agrupar por día de la semana\nsales_by_day = df.groupby('Day of Week')['Total'].mean().reindex(\n    ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n).reset_index()\n\n# Gráfico de líneas con Seaborn\nplt.figure(figsize=(10, 6))\nsns.lineplot(data=sales_by_day, x='Day of Week', y='Total', marker='o', color='teal')\nplt.title('Ventas Promedio por Día de la Semana')\nplt.xlabel('Día de la Semana')\nplt.ylabel('Ventas Promedio (MMK)')\nplt.grid(alpha=0.3)\nplt.show()\n\n# Gráfico interactivo con Plotly\nfig = px.line(sales_by_day, x='Day of Week', y='Total', \n              title='Ventas Promedio por Día de la Semana',\n              labels={'Total': 'Ventas Promedio (MMK)', 'Day of Week': 'Día'},\n              markers=True)\nfig.update_traces(line_color='teal')\nfig.show()\n\n\n\n\n\nVentas Promedio por Día de la Semana\n\n\n\n\n                            \n                                            \n\n\n\nFigura: Ventas Promedio por Día de la Semana. La línea muestra el promedio de ventas (en MMK) para cada día, con picos en fines de semana.\nLa versión interactiva facilita la identificación de patrones temporales y su relación con el comportamiento de los clientes.\n\n\n\n\nCategorías clave: Algunas categorías (ej. alimentos, hogar) dominan las ventas, lo que sugiere un enfoque en su promoción.\nPatrones temporales: Los fines de semana (sábado y domingo) muestran mayores ventas promedio, lo que podría justificar estrategias de marketing específicas.\nOportunidades: Identificar productos de bajo rendimiento para ajustar inventarios."
  },
  {
    "objectID": "projects/supermarket_sales_myanmar.html#parte-3-próximos-pasos-en-desarrollo",
    "href": "projects/supermarket_sales_myanmar.html#parte-3-próximos-pasos-en-desarrollo",
    "title": "Análisis de Ventas de un Supermercado en Myanmar",
    "section": "",
    "text": "En las siguientes fases, planeo: - Implementar un dashboard interactivo en Streamlit para explorar las ventas por categoría, ciudad o tipo de cliente. - Realizar un análisis de correlación entre variables como género, tipo de cliente y monto de compra. - Explorar modelos predictivos para pronosticar ventas futuras."
  },
  {
    "objectID": "projects/supermarket_sales_myanmar.html#conclusiones-preliminares",
    "href": "projects/supermarket_sales_myanmar.html#conclusiones-preliminares",
    "title": "Análisis de Ventas de un Supermercado en Myanmar",
    "section": "",
    "text": "Este análisis inicial revela patrones claros en las ventas del supermercado, destacando categorías y días clave. Las visualizaciones proporcionan una base sólida para decisiones estratégicas, como optimizar inventarios o planificar promociones. La carpeta data/ asegura que los datos sean accesibles y reutilizables para futuros análisis."
  },
  {
    "objectID": "projects/supermarket_sales_myanmar.html#recursos",
    "href": "projects/supermarket_sales_myanmar.html#recursos",
    "title": "Análisis de Ventas de un Supermercado en Myanmar",
    "section": "",
    "text": "Datos: supermarket_sales.csv en data/ (basado en datasets típicos de ventas, como este ejemplo).\nCódigo: Disponible en el repositorio del proyecto [enlace si lo subes]."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "LCPallares - Análisis de Datos y Más",
    "section": "",
    "text": "Sponsor\n¡Hola! Soy Luis Carlos Pallares Ascanio, profesional en administración de empresas con una sólida experiencia en gestión y toma de decisiones estratégicas. Actualmente estoy estudiando análisis de datos para potenciar mis habilidades y aplicarlo en el mundo empresarial. Estoy creando mi sitio web con Quarto para compartir mis proyectos, reflexiones y avances en este campo. Me apasiona descubrir cómo los datos pueden transformar los negocios y estoy comprometido con seguir aprendiendo y creciendo en esta área."
  },
  {
    "objectID": "index.html#hola-soy-lcpallares-estudiante-de-análisis-de-datos.",
    "href": "index.html#hola-soy-lcpallares-estudiante-de-análisis-de-datos.",
    "title": "LCPallares - Análisis de Datos y Más",
    "section": "¡Hola! Soy LCPallares, estudiante de análisis de datos.",
    "text": "¡Hola! Soy LCPallares, estudiante de análisis de datos.\nBienvenido a mi sitio web, donde comparto mis proyectos y escribo sobre temas relacionados con la estadística, visualización de datos y más.\n\n\nProyectos\nCurae hendrerit donec commodo hendrerit egestas tempus, turpis facilisis nostra nunc. Vestibulum dui eget ultrices.\n\n\n\n\n\n\nBlogs\nCurae hendrerit donec commodo hendrerit egestas tempus, turpis facilisis nostra nunc. Vestibulum dui eget ultrices."
  },
  {
    "objectID": "posts/estadistica-descriptiva.html",
    "href": "posts/estadistica-descriptiva.html",
    "title": "Estadística Descriptiva: La Base del Análisis de Datos",
    "section": "",
    "text": "La estadística descriptiva constituye el primer paso en cualquier proceso de análisis de datos. En este artículo, exploraremos los conceptos fundamentales y cómo aplicarlos efectivamente en proyectos reales.\n\n\nLa estadística descriptiva comprende métodos para organizar, resumir y presentar datos de manera informativa. A diferencia de la estadística inferencial, que busca hacer predicciones basadas en muestras, la estadística descriptiva se centra en describir lo que ya existe en nuestros datos.\n\n\n\nLas medidas de tendencia central nos ayudan a identificar los valores “típicos” de un conjunto de datos:\n\nMedia: El promedio aritmético de todos los valores.\nMediana: El valor central cuando los datos están ordenados.\nModa: El valor que aparece con mayor frecuencia.\n\nimport numpy as np\nimport pandas as pd\n\n# Ejemplo con un conjunto de datos\ndatos = [23, 45, 12, 67, 34, 23, 56, 23, 78, 45]\n\nmedia = np.mean(datos)\nmediana = np.median(datos)\n# Calculando la moda manualmente\nmoda = max(set(datos), key = datos.count)\n\nprint(f\"Media: {media}\")\nprint(f\"Mediana: {mediana}\")\nprint(f\"Moda: {moda}\")\n\n\n\nLas medidas de dispersión describen cuán extendidos o concentrados están los datos:\n\nRango: La diferencia entre el valor máximo y mínimo.\nDesviación estándar: Medida de cuánto se alejan típicamente los valores de la media.\nVarianza: El cuadrado de la desviación estándar.\nRango intercuartílico (IQR): La diferencia entre el tercer y primer cuartil.\n\n\n\n\nLa representación visual es crucial para entender patrones en los datos:\n\n\nLos histogramas muestran la distribución de frecuencias de un conjunto de datos continuos:\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 6))\nplt.hist(datos, bins=5, color='skyblue', edgecolor='black')\nplt.title('Histograma de Datos')\nplt.xlabel('Valor')\nplt.ylabel('Frecuencia')\nplt.grid(axis='y', alpha=0.75)\nplt.show()\n\n\n\nLos boxplots son excelentes para visualizar la distribución y detectar valores atípicos:\nplt.figure(figsize=(8, 6))\nplt.boxplot(datos, vert=False, patch_artist=True)\nplt.title('Diagrama de Caja')\nplt.grid(axis='x', linestyle='--')\nplt.show()\n\n\n\n\nVeamos cómo aplicar estos conceptos a un escenario real de análisis de ventas mensuales:\n# Datos de ventas mensuales (en miles de $)\nventas = {\n    'Enero': 120, 'Febrero': 135, 'Marzo': 142, \n    'Abril': 130, 'Mayo': 125, 'Junio': 145\n}\n\ndf_ventas = pd.DataFrame(list(ventas.items()), columns=['Mes', 'Ventas'])\n\n# Estadísticas descriptivas\nestadisticas = df_ventas['Ventas'].describe()\nprint(estadisticas)\n\n# Visualización\nplt.figure(figsize=(12, 6))\nplt.bar(df_ventas['Mes'], df_ventas['Ventas'], color='green')\nplt.title('Ventas Mensuales')\nplt.xlabel('Mes')\nplt.ylabel('Ventas (miles $)')\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\n\n\nLa estadística descriptiva proporciona las herramientas fundamentales para explorar y entender cualquier conjunto de datos. Dominar estos conceptos básicos es esencial antes de avanzar a técnicas más complejas de análisis e inferencia.\nEn próximos artículos, exploraremos cómo pasar de la descripción a la inferencia estadística, permitiendo hacer predicciones y tomar decisiones basadas en datos.\n¿Qué técnicas de estadística descriptiva utilizas más frecuentemente en tus análisis? ¡Comparte tu experiencia en los comentarios!"
  },
  {
    "objectID": "posts/estadistica-descriptiva.html#qué-es-la-estadística-descriptiva",
    "href": "posts/estadistica-descriptiva.html#qué-es-la-estadística-descriptiva",
    "title": "Estadística Descriptiva: La Base del Análisis de Datos",
    "section": "",
    "text": "La estadística descriptiva comprende métodos para organizar, resumir y presentar datos de manera informativa. A diferencia de la estadística inferencial, que busca hacer predicciones basadas en muestras, la estadística descriptiva se centra en describir lo que ya existe en nuestros datos."
  },
  {
    "objectID": "posts/estadistica-descriptiva.html#medidas-de-tendencia-central",
    "href": "posts/estadistica-descriptiva.html#medidas-de-tendencia-central",
    "title": "Estadística Descriptiva: La Base del Análisis de Datos",
    "section": "",
    "text": "Las medidas de tendencia central nos ayudan a identificar los valores “típicos” de un conjunto de datos:\n\nMedia: El promedio aritmético de todos los valores.\nMediana: El valor central cuando los datos están ordenados.\nModa: El valor que aparece con mayor frecuencia.\n\nimport numpy as np\nimport pandas as pd\n\n# Ejemplo con un conjunto de datos\ndatos = [23, 45, 12, 67, 34, 23, 56, 23, 78, 45]\n\nmedia = np.mean(datos)\nmediana = np.median(datos)\n# Calculando la moda manualmente\nmoda = max(set(datos), key = datos.count)\n\nprint(f\"Media: {media}\")\nprint(f\"Mediana: {mediana}\")\nprint(f\"Moda: {moda}\")"
  },
  {
    "objectID": "posts/estadistica-descriptiva.html#medidas-de-dispersión",
    "href": "posts/estadistica-descriptiva.html#medidas-de-dispersión",
    "title": "Estadística Descriptiva: La Base del Análisis de Datos",
    "section": "",
    "text": "Las medidas de dispersión describen cuán extendidos o concentrados están los datos:\n\nRango: La diferencia entre el valor máximo y mínimo.\nDesviación estándar: Medida de cuánto se alejan típicamente los valores de la media.\nVarianza: El cuadrado de la desviación estándar.\nRango intercuartílico (IQR): La diferencia entre el tercer y primer cuartil."
  },
  {
    "objectID": "posts/estadistica-descriptiva.html#visualización-en-la-estadística-descriptiva",
    "href": "posts/estadistica-descriptiva.html#visualización-en-la-estadística-descriptiva",
    "title": "Estadística Descriptiva: La Base del Análisis de Datos",
    "section": "",
    "text": "La representación visual es crucial para entender patrones en los datos:\n\n\nLos histogramas muestran la distribución de frecuencias de un conjunto de datos continuos:\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 6))\nplt.hist(datos, bins=5, color='skyblue', edgecolor='black')\nplt.title('Histograma de Datos')\nplt.xlabel('Valor')\nplt.ylabel('Frecuencia')\nplt.grid(axis='y', alpha=0.75)\nplt.show()\n\n\n\nLos boxplots son excelentes para visualizar la distribución y detectar valores atípicos:\nplt.figure(figsize=(8, 6))\nplt.boxplot(datos, vert=False, patch_artist=True)\nplt.title('Diagrama de Caja')\nplt.grid(axis='x', linestyle='--')\nplt.show()"
  },
  {
    "objectID": "posts/estadistica-descriptiva.html#aplicación-práctica-análisis-de-ventas",
    "href": "posts/estadistica-descriptiva.html#aplicación-práctica-análisis-de-ventas",
    "title": "Estadística Descriptiva: La Base del Análisis de Datos",
    "section": "",
    "text": "Veamos cómo aplicar estos conceptos a un escenario real de análisis de ventas mensuales:\n# Datos de ventas mensuales (en miles de $)\nventas = {\n    'Enero': 120, 'Febrero': 135, 'Marzo': 142, \n    'Abril': 130, 'Mayo': 125, 'Junio': 145\n}\n\ndf_ventas = pd.DataFrame(list(ventas.items()), columns=['Mes', 'Ventas'])\n\n# Estadísticas descriptivas\nestadisticas = df_ventas['Ventas'].describe()\nprint(estadisticas)\n\n# Visualización\nplt.figure(figsize=(12, 6))\nplt.bar(df_ventas['Mes'], df_ventas['Ventas'], color='green')\nplt.title('Ventas Mensuales')\nplt.xlabel('Mes')\nplt.ylabel('Ventas (miles $)')\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/estadistica-descriptiva.html#conclusión",
    "href": "posts/estadistica-descriptiva.html#conclusión",
    "title": "Estadística Descriptiva: La Base del Análisis de Datos",
    "section": "",
    "text": "La estadística descriptiva proporciona las herramientas fundamentales para explorar y entender cualquier conjunto de datos. Dominar estos conceptos básicos es esencial antes de avanzar a técnicas más complejas de análisis e inferencia.\nEn próximos artículos, exploraremos cómo pasar de la descripción a la inferencia estadística, permitiendo hacer predicciones y tomar decisiones basadas en datos.\n¿Qué técnicas de estadística descriptiva utilizas más frecuentemente en tus análisis? ¡Comparte tu experiencia en los comentarios!"
  }
]
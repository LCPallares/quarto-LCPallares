[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Acerca de mi",
    "section": "",
    "text": "Sponsor\n¬°Hola! Soy Luis Carlos Pallares Ascanio, profesional en administraci√≥n de empresas con m√°s de X a√±os de experiencia en [sector espec√≠fico]. Actualmente combino mis conocimientos en gesti√≥n empresarial con herramientas modernas de an√°lisis de datos para:"
  },
  {
    "objectID": "about.html#proyectos-destacados",
    "href": "about.html#proyectos-destacados",
    "title": "Acerca de mi",
    "section": "Proyectos Destacados",
    "text": "Proyectos Destacados\n\n\nAn√°lisis Predictivo de Ventas\n Repositorio\n Reporte\n\nDashboard de Log√≠stica\nTecnolog√≠as: Power BI, SQL\nDemo en vivo ¬ª"
  },
  {
    "objectID": "about.html#tecnolog√≠as",
    "href": "about.html#tecnolog√≠as",
    "title": "Acerca de mi",
    "section": "Tecnolog√≠as",
    "text": "Tecnolog√≠as\n\nLenguajes\n\n Python\n R\n SQL\n\n\n\nHerramientas\n\n Scikit-learn\n Pandas\n NumPy\n\n\n\nVisualizaci√≥n\n\n Matplotlib\n Seaborn\n Plotly\n\n\n\nBusiness Intelligence\n\n Power BI\n Tableau\n Looker Studio\n\n\n\nOtros\n\n Kivy"
  },
  {
    "objectID": "about.html#experiencia",
    "href": "about.html#experiencia",
    "title": "Acerca de mi",
    "section": "Experiencia",
    "text": "Experiencia\n\n2022 - Actual\nAnalista de Datos Junior\nEmpresa XYZ\n- Implementaci√≥n de modelo predictivo para ventas (precisi√≥n del 85%) - Automatizaci√≥n de reportes mensuales usando Python - An√°lisis de cohortes para retenci√≥n de clientes\n2020 - 2022\nAsistente de Gerencia\nCompa√±√≠a ABC\n- Creaci√≥n de dashboards interactivos en Power BI - Optimizaci√≥n de procesos log√≠sticos mediante an√°lisis de datos - Reducci√≥n de costos operativos en 15% mediante an√°lisis estrat√©gico"
  },
  {
    "objectID": "about.html#educaci√≥n",
    "href": "about.html#educaci√≥n",
    "title": "Acerca de mi",
    "section": "Educaci√≥n",
    "text": "Educaci√≥n\n\nAdministraci√≥n de Empresas\nüìÖ 2016 - 2020\nüéì Universidad Nacional de Colombia\nüìú Tesis: ‚ÄúAn√°lisis de datos para la toma de decisiones estrat√©gicas‚Äù\nEspecializaci√≥n en An√°lisis de Datos\nüìÖ 2023 - Actual\nüìö Plataforma: Coursera\nüîñ Certificaciones:\n- Python para Data Science (IBM)\n- Machine Learning Fundamentals (DeepLearning.AI)"
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Data Analysis Blog",
    "section": "",
    "text": "Welcome to my blog! Here, I share insights, tutorials, and reflections on data analysis techniques, statistics, and data visualization.\n\nBlogs\nCurae hendrerit donec commodo hendrerit egestas tempus, turpis facilisis nostra nunc. Vestibulum dui eget ultrices.\n\n\n\n\n\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Title\n      \n      \n        Author\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nEstad√≠stica Descriptiva: La Base del An√°lisis de Datos\n\n\n\nestad√≠stica\n\nan√°lisis\n\ntutoriales\n\n\n\n\n\n\n\n\n\nFeb 15, 2025\n\n\nLC Pallares\n\n\n\n\n\n\n\n\n\n\n\n\nEstad√≠stica Inferencial: De la Muestra a la Poblaci√≥n\n\n\n\nestad√≠stica\n\nan√°lisis\n\ntutoriales\n\n\n\n\n\n\n\n\n\nMar 5, 2025\n\n\nLC Pallares\n\n\n\n\n\nNo matching items\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "LCPallares - An√°lisis de Datos y M√°s",
    "section": "",
    "text": "Sponsor\n¬°Hola! Soy Luis Carlos Pallares Ascanio, profesional en administraci√≥n de empresas con una s√≥lida experiencia en gesti√≥n y toma de decisiones estrat√©gicas. Actualmente estoy estudiando an√°lisis de datos para potenciar mis habilidades y aplicarlo en el mundo empresarial. Estoy creando mi sitio web con Quarto para compartir mis proyectos, reflexiones y avances en este campo. Me apasiona descubrir c√≥mo los datos pueden transformar los negocios y estoy comprometido con seguir aprendiendo y creciendo en esta √°rea."
  },
  {
    "objectID": "index.html#hola-soy-lcpallares-estudiante-de-an√°lisis-de-datos.",
    "href": "index.html#hola-soy-lcpallares-estudiante-de-an√°lisis-de-datos.",
    "title": "LCPallares - An√°lisis de Datos y M√°s",
    "section": "¬°Hola! Soy LCPallares, estudiante de an√°lisis de datos.",
    "text": "¬°Hola! Soy LCPallares, estudiante de an√°lisis de datos.\nBienvenido a mi sitio web, donde comparto mis proyectos y escribo sobre temas relacionados con la estad√≠stica, visualizaci√≥n de datos y m√°s.\n\n\nProyectos\nCurae hendrerit donec commodo hendrerit egestas tempus, turpis facilisis nostra nunc. Vestibulum dui eget ultrices.\n\n\n\n\n\n\nBlogs\nCurae hendrerit donec commodo hendrerit egestas tempus, turpis facilisis nostra nunc. Vestibulum dui eget ultrices."
  },
  {
    "objectID": "projects/congenital-hypothyroidism2.html",
    "href": "projects/congenital-hypothyroidism2.html",
    "title": "An√°lisis del Hipotiroidismo Cong√©nito",
    "section": "",
    "text": "El hipotiroidismo cong√©nito es una condici√≥n presente desde el nacimiento que afecta la gl√°ndula tiroides, pero un problema cr√≠tico es que los padres a menudo no son informados a tiempo. Este proyecto tiene dos objetivos: analizar una base de datos sobre esta condici√≥n y desarrollar una herramienta pr√°ctica para mejorar la comunicaci√≥n. Para ello, cre√© un dashboard en Streamlit que explora los datos y permite enviar notificaciones SMS a los padres mediante una API.\nThe congenital hypothyroidism (CH). CH affects 1 in 2,000-4,000 newborns worldwide. Early detection is vital. Untreated CH can lead to intellectual disability and growth delays.\n\n\n\nEl conjunto de datos m√©dicos requiri√≥ un extenso proceso de limpieza. A continuaci√≥n, se detallan los principales desaf√≠os y soluciones implementadas:\n\n\nEl dataset conten√≠a m√∫ltiples columnas de fechas (fecha_ingreso, fecha_toma_muestra, fecha_resultado, etc.). Se identificaron errores cr√≠ticos: - Fechas imposibles: Diferencias negativas entre fecha_resultado y fecha_toma_muestra (ej: resultados que aparec√≠an 20 a√±os antes de la toma de muestra). - Causas: Errores de digitaci√≥n (ej: 2022 vs 2002) y registros invertidos.\nAcciones tomadas: 1. Filtrado de registros con diferencias mayores a ¬±30 d√≠as (umbral cl√≠nicamente relevante). 2. Correcci√≥n manual de fechas mal ingresadas cruzando con otros campos (ej: edad del paciente). 3. Eliminaci√≥n de 15 registros con inconsistencias irrecuperables.\nEjemplo de dato corregido:\nAntes: \n  - fecha_toma_muestra: 2022-07-17 \n  - fecha_resultado: 2002-08-05 \n  - d√≠as_pasados: -7286\n\nDespu√©s: \n  - fecha_toma_muestra: 2022-07-17 \n  - fecha_resultado: 2022-08-05 \n  - d√≠as_pasados: +19\n\n\n\nLos resultados de TSH presentaban: - Valores at√≠picos: Resultados fuera del rango fisiol√≥gico (ej: &gt;100 mUI/L). - Datos faltantes: NaN en ~8% de los registros.\nEstrategia: - Rango v√°lido definido: 0.5 - 20 mUI/L (basado en literatura m√©dica). - Imputaci√≥n de faltantes con la media (5.2 mUI/L).\n\n\n\nLa columna sexo conten√≠a categor√≠as inconsistentes:\nAMBIGUO: 1      | FEMENINO: 16,298\nNO ESCRITO: 126 | MASCULINO: 17,140\nSoluci√≥n: 1. Reclasificaci√≥n de ‚ÄúNO ESCRITO‚Äù mediante an√°lisis de nombres (ej: ‚ÄúMar√≠a‚Äù ‚Üí FEMENINO). 2. Correcci√≥n manual del √∫nico caso ‚ÄúAMBIGUO‚Äù revisando historia cl√≠nica. 3. Resultado final: Solo categor√≠as FEMENINO/MASCULINO.\n\n\n\n\nColumnas num√©ricas: Imputaci√≥n con media/mediana seg√∫n distribuci√≥n.\nColumnas categ√≥ricas: Relleno con \"DESCONOCIDO\" para preservar registros.\n\n\n\n\nTras la limpieza, el dataset qued√≥ con: - Registros: no se elimino ningun registro. - Variables cr√≠ticas validadas: TSH neonatal, fechas, g√©nero.\n\nNota t√©cnica: El c√≥digo completo de limpieza est√° disponible bajo solicitud para fines de reproducibilidad.\n\n\n\n\n\nEl an√°lisis exploratorio revel√≥ patrones en los datos, mientras que el dashboard en Streamlit los hace accesibles. Algunas visualizaciones incluyen:\n\n\nCode\ndf.head(5)\n\n\n\n\n\n\n\n\n\nid\nficha_id\nfecha_ingreso\ninstitucion\nars\nhistoria_clinica\ntipo_documento\nnumero_documento\nciudad\ndepartamento\n...\nfecha_resultado_muestra_2\nresultado_muestra_2\ncontador\nmuestra_rechazada\nfecha_toma_rechazada\ntipo_vinculacion\nresultado_rechazada\nfecha_resultado_rechazada\ndias_pasados\ndias_pasados2\n\n\n\n\n0\n365048\n369980\n2019-05-06\nVICTORIA\nMEDIMAS\n1000\n1\n2000\nBogota\nCundinamarca\n...\nNaT\n0.0\n1\nFalse\nNaT\nNaN\nNaN\nNaT\n1\nNaN\n\n\n1\n365049\n369981\n2019-05-06\nVICTORIA\nCAPITAL SALUD\n1000\n2\n2000\nBogota\nCundinamarca\n...\nNaT\n0.0\n1\nFalse\nNaT\nNaN\nNaN\nNaT\n2\nNaN\n\n\n2\n365050\n369982\n2019-05-06\nVICTORIA\nCAPITAL SALUD\n1000\n1\n2000\nBogota\nCundinamarca\n...\nNaT\n0.0\n1\nFalse\nNaT\nNaN\nNaN\nNaT\n1\nNaN\n\n\n3\n365051\n369983\n2019-05-06\nVICTORIA\nCAPITAL SALUD\n1000\n1\n2000\nBogota\nCundinamarca\n...\nNaT\n0.0\n1\nFalse\nNaT\nNaN\nNaN\nNaT\n0\nNaN\n\n\n4\n365052\n369984\n2019-05-07\nVICTORIA\nVINCULADO\n1000\n4\n2000\nBogota\nCundinamarca\n...\nNaT\n0.0\n1\nFalse\nNaT\nNaN\nNaN\nNaT\n3\nNaN\n\n\n\n\n5 rows √ó 41 columns\n\n\n\nNotas:\nSospecha de hipotiroidismo: TSH ‚â• 15 mIU/L en la primera muestra.\nConfirmaci√≥n de hipotiroidismo: TSH ‚â• 15 mIU/L en la primera y segunda muestra.\n\n\nCode\ndf['sospecha_hipotiroidismo'] = df['tsh_neonatal'] &gt;= 15\ndf['confirmado_hipotiroidismo'] = df['sospecha_hipotiroidismo'] & (df['resultado_muestra_2'] &gt;= 15)\n\n\n\n\nCode\n\nprint(\"Total de Registros\", f\"{df.shape[0]:,}\")\n\nprint(\"Casos Sospechosos (TSH ‚â• 15)\", f\"{df['sospecha_hipotiroidismo'].sum():,}\")\nprint(\"Casos Confirmados\", f\"{df['confirmado_hipotiroidismo'].sum():,}\")\n\n# Calcular el promedio de d√≠as hasta el resultado\ndias_promedio = round(df['dias_pasados'].mean(), 1)\nprint(\"Promedio D√≠as hasta Resultado\", f\"{dias_promedio}\")\n\n\nTotal de Registros 33,565\nCasos Sospechosos (TSH ‚â• 15) 572\nCasos Confirmados 6\nPromedio D√≠as hasta Resultado 2.7\n\n\n\n\nCode\n# para mostar los graficos, ya que devolvian error\nimport plotly.offline as pyo\npyo.init_notebook_mode(connected=True)\n\n\n\n\nCode\n# Gr√°fico de pir√°mide de diagn√≥stico\nstages = ['Tamizados', 'TSH ‚â• 15', 'Confirmados']\nvalues = [df.shape[0], df['sospecha_hipotiroidismo'].sum(), df['confirmado_hipotiroidismo'].sum()]\n\nfig_funnel = go.Figure(go.Funnel(\n    y=stages,\n    x=values,\n    textinfo=\"value+percent initial\",\n    marker={\"color\": [\"#4682B4\", \"#FFA500\", \"#FF4500\"]}\n))\n\nfig_funnel.update_layout(\n    title=\"Pir√°mide de Diagn√≥stico de Hipotiroidismo Cong√©nito\",\n    width=800,\n    height=500\n)\n\nfig_funnel.show()\n\n\n                                                \nPir√°mide de Diagn√≥stico de Hipotiroidismo Cong√©nito\n\n\n\n\nCode\n# Filtrar valores extremos para mejor visualizaci√≥n\ntsh_max_visual = df['tsh_neonatal'].quantile(0.99)\ndf_tsh_visual = df[df['tsh_neonatal'] &lt;= tsh_max_visual]\n\nfig_tsh_hist = px.histogram(\n    df_tsh_visual, \n    x='tsh_neonatal',\n    nbins=30,\n    color_discrete_sequence=['#3CB371'],\n    labels={'tsh_neonatal': 'TSH Neonatal (mIU/L)'}\n)\n\n# A√±adir l√≠nea vertical para el umbral\nfig_tsh_hist.add_vline(\n    x=15, \n    line_dash=\"dash\", \n    line_color=\"red\",\n    annotation_text=f\"Umbral: 15 mIU/L\",\n    annotation_position=\"top right\"\n)\n\nfig_tsh_hist.update_layout(title='Distribuci√≥n de Niveles de TSH al Nacer', xaxis_title=\"Valor de TSH (mIU/L)\", yaxis_title=\"Frecuencia\")\nfig_tsh_hist.show()\n\n\n                                                \nDistribuci√≥n de niveles de TSH al nacer\n\n\n\n\nCode\nfig_box_sex = px.box(\n    df_tsh_visual,\n    x='sexo',\n    y='tsh_neonatal',\n    color='sexo',\n    points=\"outliers\",\n    labels={'sexo': 'Sexo', 'tsh_neonatal': 'TSH Neonatal (mIU/L)'}\n)\n\nfig_box_sex.add_hline(\n    y=15, \n    line_dash=\"dash\", \n    line_color=\"red\",\n    annotation_text=\"Umbral: 15 mIU/L\",\n    annotation_position=\"top right\"\n)\nfig_box_sex.show()\n\n\n                                                \nBoxplots dispercion entre sexo y TSH\n\n\n\n\nCode\nfiltered_df = df\n# Agrupar datos por mes y a√±o\nfiltered_df['a√±o_mes'] = filtered_df['fecha_nacimiento'].dt.to_period('M')\n\n# Tendencia temporal de casos\ntemporal_df = filtered_df.groupby(['a√±o_mes']).agg(\ntotal_casos=('tsh_neonatal', 'count'),\ncasos_sospechosos=('sospecha_hipotiroidismo', 'sum'),\ncasos_confirmados=('confirmado_hipotiroidismo', 'sum'),\ntsh_promedio=('tsh_neonatal', 'mean')\n).reset_index()\n\ntemporal_df['a√±o_mes'] = temporal_df['a√±o_mes'].dt.to_timestamp()\ntemporal_df['tasa_confirmacion'] = temporal_df['casos_confirmados'] / temporal_df['casos_sospechosos']\ntemporal_df['incidencia'] = temporal_df['casos_confirmados'] / temporal_df['total_casos']\n\n# Gr√°fico de l√≠nea para casos y tasa de confirmaci√≥n\nfig_temporal = go.Figure()\n\nfig_temporal.add_trace(go.Scatter(\nx=temporal_df['a√±o_mes'],\ny=temporal_df['casos_sospechosos'],\nmode='lines+markers',\nname='Casos Sospechosos',\nline=dict(color='#FFA500', width=2)\n))\n\nfig_temporal.add_trace(go.Scatter(\nx=temporal_df['a√±o_mes'],\ny=temporal_df['casos_confirmados'],\nmode='lines+markers',\nname='Casos Confirmados',\nline=dict(color='#FF4500', width=2)\n))\n\nfig_temporal.add_trace(go.Scatter(\nx=temporal_df['a√±o_mes'],\ny=temporal_df['tasa_confirmacion'],\nmode='lines',\nname='Tasa de Confirmaci√≥n',\nline=dict(color='#4682B4', width=2, dash='dot'),\nyaxis='y2'\n))\n\nfig_temporal.update_layout(\ntitle='Evoluci√≥n Temporal de Casos de Hipotiroidismo Cong√©nito',\nxaxis_title='Fecha',\nyaxis=dict(\n    title='N√∫mero de Casos',\n    titlefont=dict(color='#FF4500'),\n    tickfont=dict(color='#FF4500')\n),\nyaxis2=dict(\n    title='Tasa de Confirmaci√≥n',\n    titlefont=dict(color='#4682B4'),\n    tickfont=dict(color='#4682B4'),\n    anchor='x',\n    overlaying='y',\n    side='right',\n    range=[0, 1]\n),\nlegend=dict(\n    orientation=\"h\",\n    yanchor=\"bottom\",\n    y=1.02,\n    xanchor=\"center\",\n    x=0.5\n)\n)\n\nfig_temporal.show()\n\n\n                                                \nEvoluci√≥n Temporal de Casos de Hipotiroidismo Cong√©nito\n\n\n\n\nCode\n# Conversi√≥n de peso a kilogramos para mejor visualizaci√≥n\nfiltered_df['peso_kg'] = filtered_df['peso'] / 1000\n\nfig_peso_tsh = px.scatter(\n    filtered_df,\n    x='peso_kg',\n    y='tsh_neonatal',\n    color='confirmado_hipotiroidismo',\n    color_discrete_map={True: '#FF4500', False: '#4682B4'},\n    labels={\n        'peso_kg': 'Peso al Nacer (kg)',\n        'tsh_neonatal': 'TSH Neonatal (mIU/L)',\n        'confirmado_hipotiroidismo': 'Hipotiroidismo Confirmado'\n    },\n    trendline=\"ols\",\n    opacity=0.7\n)\n\nfig_peso_tsh.add_hline(\n    y=15, \n    line_dash=\"dash\", \n    line_color=\"red\",\n    annotation_text=\"Umbral TSH: 15 mIU/L\",\n    annotation_position=\"top right\"\n)\n\nfig_peso_tsh.show()\n\n\n                                                \nRelacion entre peso y Hipotiroidismo Cong√©nito\n\n\n\n\n\n\n\n\nExploraci√≥n interactiva: Los usuarios pueden filtrar y visualizar datos (ej. TSH por edad o regi√≥n).\nNotificaciones SMS: Integra una API (como Twilio) para enviar alertas a los padres con informaci√≥n clave, como ‚ÄúSu hijo/a tiene un nivel de TSH elevado. Contacte a su m√©dico‚Äù.\nAcceso: Disponible aqu√≠.\n\n\n\n\nInterfaz del Dashboard\n\n\n\n\n\nDistribuci√≥n de TSH\n\n\n\n\n\n\nLos niveles de TSH var√≠an ampliamente, lo que indica la necesidad de segmentar los casos.\nLa falta de informaci√≥n a los padres parece estar relacionada con datos de contacto incompletos o desactualizados.\n\n\n\n\n\nEsta secci√≥n est√° en progreso. Planeo incluir: - Modelos predictivos para identificar casos de riesgo elevado. - An√°lisis de correlaci√≥n entre factores demogr√°ficos y retrasos en la notificaci√≥n. - Evaluaci√≥n de la efectividad de las notificaciones SMS en la respuesta de los padres.\nPronto actualizar√© esta entrada con m√°s detalles.\n\n\n\nEl proyecto no solo transforma datos crudos en informaci√≥n √∫til, sino que tambi√©n aborda un problema real: la comunicaci√≥n con los padres. El dashboard combina an√°lisis y acci√≥n, ofreciendo una soluci√≥n pr√°ctica para mejorar la atenci√≥n temprana del hipotiroidismo cong√©nito.\n\n\n\n\nFinalizar la secci√≥n de estad√≠stica avanzada.\nOptimizar el dashboard con m√°s opciones de personalizaci√≥n para los SMS.\nProbar la API de SMS en un entorno real y evaluar su impacto."
  },
  {
    "objectID": "projects/congenital-hypothyroidism2.html#resumen-del-proyecto",
    "href": "projects/congenital-hypothyroidism2.html#resumen-del-proyecto",
    "title": "An√°lisis del Hipotiroidismo Cong√©nito",
    "section": "",
    "text": "El hipotiroidismo cong√©nito es una condici√≥n presente desde el nacimiento que afecta la gl√°ndula tiroides, pero un problema cr√≠tico es que los padres a menudo no son informados a tiempo. Este proyecto tiene dos objetivos: analizar una base de datos sobre esta condici√≥n y desarrollar una herramienta pr√°ctica para mejorar la comunicaci√≥n. Para ello, cre√© un dashboard en Streamlit que explora los datos y permite enviar notificaciones SMS a los padres mediante una API.\nThe congenital hypothyroidism (CH). CH affects 1 in 2,000-4,000 newborns worldwide. Early detection is vital. Untreated CH can lead to intellectual disability and growth delays."
  },
  {
    "objectID": "projects/congenital-hypothyroidism2.html#parte-1-procesamiento-y-limpieza-de-datos",
    "href": "projects/congenital-hypothyroidism2.html#parte-1-procesamiento-y-limpieza-de-datos",
    "title": "An√°lisis del Hipotiroidismo Cong√©nito",
    "section": "",
    "text": "El conjunto de datos m√©dicos requiri√≥ un extenso proceso de limpieza. A continuaci√≥n, se detallan los principales desaf√≠os y soluciones implementadas:\n\n\nEl dataset conten√≠a m√∫ltiples columnas de fechas (fecha_ingreso, fecha_toma_muestra, fecha_resultado, etc.). Se identificaron errores cr√≠ticos: - Fechas imposibles: Diferencias negativas entre fecha_resultado y fecha_toma_muestra (ej: resultados que aparec√≠an 20 a√±os antes de la toma de muestra). - Causas: Errores de digitaci√≥n (ej: 2022 vs 2002) y registros invertidos.\nAcciones tomadas: 1. Filtrado de registros con diferencias mayores a ¬±30 d√≠as (umbral cl√≠nicamente relevante). 2. Correcci√≥n manual de fechas mal ingresadas cruzando con otros campos (ej: edad del paciente). 3. Eliminaci√≥n de 15 registros con inconsistencias irrecuperables.\nEjemplo de dato corregido:\nAntes: \n  - fecha_toma_muestra: 2022-07-17 \n  - fecha_resultado: 2002-08-05 \n  - d√≠as_pasados: -7286\n\nDespu√©s: \n  - fecha_toma_muestra: 2022-07-17 \n  - fecha_resultado: 2022-08-05 \n  - d√≠as_pasados: +19\n\n\n\nLos resultados de TSH presentaban: - Valores at√≠picos: Resultados fuera del rango fisiol√≥gico (ej: &gt;100 mUI/L). - Datos faltantes: NaN en ~8% de los registros.\nEstrategia: - Rango v√°lido definido: 0.5 - 20 mUI/L (basado en literatura m√©dica). - Imputaci√≥n de faltantes con la media (5.2 mUI/L).\n\n\n\nLa columna sexo conten√≠a categor√≠as inconsistentes:\nAMBIGUO: 1      | FEMENINO: 16,298\nNO ESCRITO: 126 | MASCULINO: 17,140\nSoluci√≥n: 1. Reclasificaci√≥n de ‚ÄúNO ESCRITO‚Äù mediante an√°lisis de nombres (ej: ‚ÄúMar√≠a‚Äù ‚Üí FEMENINO). 2. Correcci√≥n manual del √∫nico caso ‚ÄúAMBIGUO‚Äù revisando historia cl√≠nica. 3. Resultado final: Solo categor√≠as FEMENINO/MASCULINO.\n\n\n\n\nColumnas num√©ricas: Imputaci√≥n con media/mediana seg√∫n distribuci√≥n.\nColumnas categ√≥ricas: Relleno con \"DESCONOCIDO\" para preservar registros.\n\n\n\n\nTras la limpieza, el dataset qued√≥ con: - Registros: no se elimino ningun registro. - Variables cr√≠ticas validadas: TSH neonatal, fechas, g√©nero.\n\nNota t√©cnica: El c√≥digo completo de limpieza est√° disponible bajo solicitud para fines de reproducibilidad."
  },
  {
    "objectID": "projects/congenital-hypothyroidism2.html#parte-2-exploraci√≥n-de-datos",
    "href": "projects/congenital-hypothyroidism2.html#parte-2-exploraci√≥n-de-datos",
    "title": "An√°lisis del Hipotiroidismo Cong√©nito",
    "section": "",
    "text": "El an√°lisis exploratorio revel√≥ patrones en los datos, mientras que el dashboard en Streamlit los hace accesibles. Algunas visualizaciones incluyen:\n\n\nCode\ndf.head(5)\n\n\n\n\n\n\n\n\n\nid\nficha_id\nfecha_ingreso\ninstitucion\nars\nhistoria_clinica\ntipo_documento\nnumero_documento\nciudad\ndepartamento\n...\nfecha_resultado_muestra_2\nresultado_muestra_2\ncontador\nmuestra_rechazada\nfecha_toma_rechazada\ntipo_vinculacion\nresultado_rechazada\nfecha_resultado_rechazada\ndias_pasados\ndias_pasados2\n\n\n\n\n0\n365048\n369980\n2019-05-06\nVICTORIA\nMEDIMAS\n1000\n1\n2000\nBogota\nCundinamarca\n...\nNaT\n0.0\n1\nFalse\nNaT\nNaN\nNaN\nNaT\n1\nNaN\n\n\n1\n365049\n369981\n2019-05-06\nVICTORIA\nCAPITAL SALUD\n1000\n2\n2000\nBogota\nCundinamarca\n...\nNaT\n0.0\n1\nFalse\nNaT\nNaN\nNaN\nNaT\n2\nNaN\n\n\n2\n365050\n369982\n2019-05-06\nVICTORIA\nCAPITAL SALUD\n1000\n1\n2000\nBogota\nCundinamarca\n...\nNaT\n0.0\n1\nFalse\nNaT\nNaN\nNaN\nNaT\n1\nNaN\n\n\n3\n365051\n369983\n2019-05-06\nVICTORIA\nCAPITAL SALUD\n1000\n1\n2000\nBogota\nCundinamarca\n...\nNaT\n0.0\n1\nFalse\nNaT\nNaN\nNaN\nNaT\n0\nNaN\n\n\n4\n365052\n369984\n2019-05-07\nVICTORIA\nVINCULADO\n1000\n4\n2000\nBogota\nCundinamarca\n...\nNaT\n0.0\n1\nFalse\nNaT\nNaN\nNaN\nNaT\n3\nNaN\n\n\n\n\n5 rows √ó 41 columns\n\n\n\nNotas:\nSospecha de hipotiroidismo: TSH ‚â• 15 mIU/L en la primera muestra.\nConfirmaci√≥n de hipotiroidismo: TSH ‚â• 15 mIU/L en la primera y segunda muestra.\n\n\nCode\ndf['sospecha_hipotiroidismo'] = df['tsh_neonatal'] &gt;= 15\ndf['confirmado_hipotiroidismo'] = df['sospecha_hipotiroidismo'] & (df['resultado_muestra_2'] &gt;= 15)\n\n\n\n\nCode\n\nprint(\"Total de Registros\", f\"{df.shape[0]:,}\")\n\nprint(\"Casos Sospechosos (TSH ‚â• 15)\", f\"{df['sospecha_hipotiroidismo'].sum():,}\")\nprint(\"Casos Confirmados\", f\"{df['confirmado_hipotiroidismo'].sum():,}\")\n\n# Calcular el promedio de d√≠as hasta el resultado\ndias_promedio = round(df['dias_pasados'].mean(), 1)\nprint(\"Promedio D√≠as hasta Resultado\", f\"{dias_promedio}\")\n\n\nTotal de Registros 33,565\nCasos Sospechosos (TSH ‚â• 15) 572\nCasos Confirmados 6\nPromedio D√≠as hasta Resultado 2.7\n\n\n\n\nCode\n# para mostar los graficos, ya que devolvian error\nimport plotly.offline as pyo\npyo.init_notebook_mode(connected=True)\n\n\n\n\nCode\n# Gr√°fico de pir√°mide de diagn√≥stico\nstages = ['Tamizados', 'TSH ‚â• 15', 'Confirmados']\nvalues = [df.shape[0], df['sospecha_hipotiroidismo'].sum(), df['confirmado_hipotiroidismo'].sum()]\n\nfig_funnel = go.Figure(go.Funnel(\n    y=stages,\n    x=values,\n    textinfo=\"value+percent initial\",\n    marker={\"color\": [\"#4682B4\", \"#FFA500\", \"#FF4500\"]}\n))\n\nfig_funnel.update_layout(\n    title=\"Pir√°mide de Diagn√≥stico de Hipotiroidismo Cong√©nito\",\n    width=800,\n    height=500\n)\n\nfig_funnel.show()\n\n\n                                                \nPir√°mide de Diagn√≥stico de Hipotiroidismo Cong√©nito\n\n\n\n\nCode\n# Filtrar valores extremos para mejor visualizaci√≥n\ntsh_max_visual = df['tsh_neonatal'].quantile(0.99)\ndf_tsh_visual = df[df['tsh_neonatal'] &lt;= tsh_max_visual]\n\nfig_tsh_hist = px.histogram(\n    df_tsh_visual, \n    x='tsh_neonatal',\n    nbins=30,\n    color_discrete_sequence=['#3CB371'],\n    labels={'tsh_neonatal': 'TSH Neonatal (mIU/L)'}\n)\n\n# A√±adir l√≠nea vertical para el umbral\nfig_tsh_hist.add_vline(\n    x=15, \n    line_dash=\"dash\", \n    line_color=\"red\",\n    annotation_text=f\"Umbral: 15 mIU/L\",\n    annotation_position=\"top right\"\n)\n\nfig_tsh_hist.update_layout(title='Distribuci√≥n de Niveles de TSH al Nacer', xaxis_title=\"Valor de TSH (mIU/L)\", yaxis_title=\"Frecuencia\")\nfig_tsh_hist.show()\n\n\n                                                \nDistribuci√≥n de niveles de TSH al nacer\n\n\n\n\nCode\nfig_box_sex = px.box(\n    df_tsh_visual,\n    x='sexo',\n    y='tsh_neonatal',\n    color='sexo',\n    points=\"outliers\",\n    labels={'sexo': 'Sexo', 'tsh_neonatal': 'TSH Neonatal (mIU/L)'}\n)\n\nfig_box_sex.add_hline(\n    y=15, \n    line_dash=\"dash\", \n    line_color=\"red\",\n    annotation_text=\"Umbral: 15 mIU/L\",\n    annotation_position=\"top right\"\n)\nfig_box_sex.show()\n\n\n                                                \nBoxplots dispercion entre sexo y TSH\n\n\n\n\nCode\nfiltered_df = df\n# Agrupar datos por mes y a√±o\nfiltered_df['a√±o_mes'] = filtered_df['fecha_nacimiento'].dt.to_period('M')\n\n# Tendencia temporal de casos\ntemporal_df = filtered_df.groupby(['a√±o_mes']).agg(\ntotal_casos=('tsh_neonatal', 'count'),\ncasos_sospechosos=('sospecha_hipotiroidismo', 'sum'),\ncasos_confirmados=('confirmado_hipotiroidismo', 'sum'),\ntsh_promedio=('tsh_neonatal', 'mean')\n).reset_index()\n\ntemporal_df['a√±o_mes'] = temporal_df['a√±o_mes'].dt.to_timestamp()\ntemporal_df['tasa_confirmacion'] = temporal_df['casos_confirmados'] / temporal_df['casos_sospechosos']\ntemporal_df['incidencia'] = temporal_df['casos_confirmados'] / temporal_df['total_casos']\n\n# Gr√°fico de l√≠nea para casos y tasa de confirmaci√≥n\nfig_temporal = go.Figure()\n\nfig_temporal.add_trace(go.Scatter(\nx=temporal_df['a√±o_mes'],\ny=temporal_df['casos_sospechosos'],\nmode='lines+markers',\nname='Casos Sospechosos',\nline=dict(color='#FFA500', width=2)\n))\n\nfig_temporal.add_trace(go.Scatter(\nx=temporal_df['a√±o_mes'],\ny=temporal_df['casos_confirmados'],\nmode='lines+markers',\nname='Casos Confirmados',\nline=dict(color='#FF4500', width=2)\n))\n\nfig_temporal.add_trace(go.Scatter(\nx=temporal_df['a√±o_mes'],\ny=temporal_df['tasa_confirmacion'],\nmode='lines',\nname='Tasa de Confirmaci√≥n',\nline=dict(color='#4682B4', width=2, dash='dot'),\nyaxis='y2'\n))\n\nfig_temporal.update_layout(\ntitle='Evoluci√≥n Temporal de Casos de Hipotiroidismo Cong√©nito',\nxaxis_title='Fecha',\nyaxis=dict(\n    title='N√∫mero de Casos',\n    titlefont=dict(color='#FF4500'),\n    tickfont=dict(color='#FF4500')\n),\nyaxis2=dict(\n    title='Tasa de Confirmaci√≥n',\n    titlefont=dict(color='#4682B4'),\n    tickfont=dict(color='#4682B4'),\n    anchor='x',\n    overlaying='y',\n    side='right',\n    range=[0, 1]\n),\nlegend=dict(\n    orientation=\"h\",\n    yanchor=\"bottom\",\n    y=1.02,\n    xanchor=\"center\",\n    x=0.5\n)\n)\n\nfig_temporal.show()\n\n\n                                                \nEvoluci√≥n Temporal de Casos de Hipotiroidismo Cong√©nito\n\n\n\n\nCode\n# Conversi√≥n de peso a kilogramos para mejor visualizaci√≥n\nfiltered_df['peso_kg'] = filtered_df['peso'] / 1000\n\nfig_peso_tsh = px.scatter(\n    filtered_df,\n    x='peso_kg',\n    y='tsh_neonatal',\n    color='confirmado_hipotiroidismo',\n    color_discrete_map={True: '#FF4500', False: '#4682B4'},\n    labels={\n        'peso_kg': 'Peso al Nacer (kg)',\n        'tsh_neonatal': 'TSH Neonatal (mIU/L)',\n        'confirmado_hipotiroidismo': 'Hipotiroidismo Confirmado'\n    },\n    trendline=\"ols\",\n    opacity=0.7\n)\n\nfig_peso_tsh.add_hline(\n    y=15, \n    line_dash=\"dash\", \n    line_color=\"red\",\n    annotation_text=\"Umbral TSH: 15 mIU/L\",\n    annotation_position=\"top right\"\n)\n\nfig_peso_tsh.show()\n\n\n                                                \nRelacion entre peso y Hipotiroidismo Cong√©nito"
  },
  {
    "objectID": "projects/congenital-hypothyroidism2.html#parte-3-dashboard",
    "href": "projects/congenital-hypothyroidism2.html#parte-3-dashboard",
    "title": "An√°lisis del Hipotiroidismo Cong√©nito",
    "section": "",
    "text": "Exploraci√≥n interactiva: Los usuarios pueden filtrar y visualizar datos (ej. TSH por edad o regi√≥n).\nNotificaciones SMS: Integra una API (como Twilio) para enviar alertas a los padres con informaci√≥n clave, como ‚ÄúSu hijo/a tiene un nivel de TSH elevado. Contacte a su m√©dico‚Äù.\nAcceso: Disponible aqu√≠.\n\n\n\n\nInterfaz del Dashboard\n\n\n\n\n\nDistribuci√≥n de TSH\n\n\n\n\n\n\nLos niveles de TSH var√≠an ampliamente, lo que indica la necesidad de segmentar los casos.\nLa falta de informaci√≥n a los padres parece estar relacionada con datos de contacto incompletos o desactualizados."
  },
  {
    "objectID": "projects/congenital-hypothyroidism2.html#parte-4-estad√≠stica-avanzada-en-desarrollo",
    "href": "projects/congenital-hypothyroidism2.html#parte-4-estad√≠stica-avanzada-en-desarrollo",
    "title": "An√°lisis del Hipotiroidismo Cong√©nito",
    "section": "",
    "text": "Esta secci√≥n est√° en progreso. Planeo incluir: - Modelos predictivos para identificar casos de riesgo elevado. - An√°lisis de correlaci√≥n entre factores demogr√°ficos y retrasos en la notificaci√≥n. - Evaluaci√≥n de la efectividad de las notificaciones SMS en la respuesta de los padres.\nPronto actualizar√© esta entrada con m√°s detalles."
  },
  {
    "objectID": "projects/congenital-hypothyroidism2.html#conclusiones-preliminares",
    "href": "projects/congenital-hypothyroidism2.html#conclusiones-preliminares",
    "title": "An√°lisis del Hipotiroidismo Cong√©nito",
    "section": "",
    "text": "El proyecto no solo transforma datos crudos en informaci√≥n √∫til, sino que tambi√©n aborda un problema real: la comunicaci√≥n con los padres. El dashboard combina an√°lisis y acci√≥n, ofreciendo una soluci√≥n pr√°ctica para mejorar la atenci√≥n temprana del hipotiroidismo cong√©nito."
  },
  {
    "objectID": "projects/congenital-hypothyroidism2.html#pr√≥ximos-pasos",
    "href": "projects/congenital-hypothyroidism2.html#pr√≥ximos-pasos",
    "title": "An√°lisis del Hipotiroidismo Cong√©nito",
    "section": "",
    "text": "Finalizar la secci√≥n de estad√≠stica avanzada.\nOptimizar el dashboard con m√°s opciones de personalizaci√≥n para los SMS.\nProbar la API de SMS en un entorno real y evaluar su impacto."
  },
  {
    "objectID": "projects/amazon-best-seller.html",
    "href": "projects/amazon-best-seller.html",
    "title": "Amazon Best Seller Analysis",
    "section": "",
    "text": "This project analyzes Amazon‚Äôs top-selling books to identify trends in reading preferences, pricing strategies, and publishing patterns.\n\n\n\nI built a custom web scraper using: - Python (BeautifulSoup and Selenium) - Request throttling to respect Amazon‚Äôs servers - Proxy rotation to avoid IP blocking - Data validation to ensure consistency\n\n\n\nThe raw data required extensive cleaning: 1. Standardizing author name formats 2. Extracting numerical ratings from text 3. Converting price strings to numerical values 4. Handling missing data points 5. Deduplicating entries for books appearing in multiple categories\n\n\n\n\n\nCode\n####| echo: false\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\n# Sample visualization code\nnp.random.seed(42)\nprices = np.random.normal(14.99, 5, 100)  # Simulated price data\n\nplt.figure(figsize=(10, 6))\nsns.histplot(prices, bins=20, kde=True)\nplt.title('Price Distribution of Amazon Best Sellers')\nplt.xlabel('Price ($)')\nplt.ylabel('Frequency')\nplt.axvline(prices.mean(), color='red', linestyle='--', label=f'Mean: ${prices.mean():.2f}')\nplt.legend()\nplt.grid(alpha=0.3)\nplt.show()\n\n\n\n\n\nPrice distribution of best-selling books\n\n\n\n\n\n\n\n\nPricing Strategy: Books priced between $12.99-$16.99 consistently outperform higher-priced alternatives\nGenre Trends: Self-help and business books dominate the top 20% of best sellers\nReview Impact: Books with 1,000+ reviews sell 3x more units regardless of rating\nPublication Timing: Books released on Tuesdays show 15% higher first-month sales\n\n\n\n\nThis analysis reveals that Amazon best sellers follow distinct patterns in pricing, genre positioning, and marketing approach. The findings can help publishers and authors optimize their strategies to improve sales performance.\n\n\n\nFuture extensions of this project could include: - Sentiment analysis of reviews - Time series analysis of genre popularity - Correlation between best seller rank and external factors (movies, news events) - Author career trajectory analysis"
  },
  {
    "objectID": "projects/amazon-best-seller.html#project-overview",
    "href": "projects/amazon-best-seller.html#project-overview",
    "title": "Amazon Best Seller Analysis",
    "section": "",
    "text": "This project analyzes Amazon‚Äôs top-selling books to identify trends in reading preferences, pricing strategies, and publishing patterns."
  },
  {
    "objectID": "projects/amazon-best-seller.html#web-scraping-process",
    "href": "projects/amazon-best-seller.html#web-scraping-process",
    "title": "Amazon Best Seller Analysis",
    "section": "",
    "text": "I built a custom web scraper using: - Python (BeautifulSoup and Selenium) - Request throttling to respect Amazon‚Äôs servers - Proxy rotation to avoid IP blocking - Data validation to ensure consistency"
  },
  {
    "objectID": "projects/amazon-best-seller.html#data-cleaning-steps",
    "href": "projects/amazon-best-seller.html#data-cleaning-steps",
    "title": "Amazon Best Seller Analysis",
    "section": "",
    "text": "The raw data required extensive cleaning: 1. Standardizing author name formats 2. Extracting numerical ratings from text 3. Converting price strings to numerical values 4. Handling missing data points 5. Deduplicating entries for books appearing in multiple categories"
  },
  {
    "objectID": "projects/amazon-best-seller.html#exploratory-analysis",
    "href": "projects/amazon-best-seller.html#exploratory-analysis",
    "title": "Amazon Best Seller Analysis",
    "section": "",
    "text": "Code\n####| echo: false\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\n# Sample visualization code\nnp.random.seed(42)\nprices = np.random.normal(14.99, 5, 100)  # Simulated price data\n\nplt.figure(figsize=(10, 6))\nsns.histplot(prices, bins=20, kde=True)\nplt.title('Price Distribution of Amazon Best Sellers')\nplt.xlabel('Price ($)')\nplt.ylabel('Frequency')\nplt.axvline(prices.mean(), color='red', linestyle='--', label=f'Mean: ${prices.mean():.2f}')\nplt.legend()\nplt.grid(alpha=0.3)\nplt.show()\n\n\n\n\n\nPrice distribution of best-selling books"
  },
  {
    "objectID": "projects/amazon-best-seller.html#key-insights",
    "href": "projects/amazon-best-seller.html#key-insights",
    "title": "Amazon Best Seller Analysis",
    "section": "",
    "text": "Pricing Strategy: Books priced between $12.99-$16.99 consistently outperform higher-priced alternatives\nGenre Trends: Self-help and business books dominate the top 20% of best sellers\nReview Impact: Books with 1,000+ reviews sell 3x more units regardless of rating\nPublication Timing: Books released on Tuesdays show 15% higher first-month sales"
  },
  {
    "objectID": "projects/amazon-best-seller.html#conclusions",
    "href": "projects/amazon-best-seller.html#conclusions",
    "title": "Amazon Best Seller Analysis",
    "section": "",
    "text": "This analysis reveals that Amazon best sellers follow distinct patterns in pricing, genre positioning, and marketing approach. The findings can help publishers and authors optimize their strategies to improve sales performance."
  },
  {
    "objectID": "projects/amazon-best-seller.html#next-steps",
    "href": "projects/amazon-best-seller.html#next-steps",
    "title": "Amazon Best Seller Analysis",
    "section": "",
    "text": "Future extensions of this project could include: - Sentiment analysis of reviews - Time series analysis of genre popularity - Correlation between best seller rank and external factors (movies, news events) - Author career trajectory analysis"
  },
  {
    "objectID": "projects/congenital-hypothyroidism.html",
    "href": "projects/congenital-hypothyroidism.html",
    "title": "An√°lisis del Hipotiroidismo Cong√©nito: Datos y Comunicaci√≥n",
    "section": "",
    "text": "El hipotiroidismo cong√©nito es una condici√≥n presente desde el nacimiento que afecta la gl√°ndula tiroides, pero un problema cr√≠tico es que los padres a menudo no son informados a tiempo. Este proyecto tiene dos objetivos: analizar una base de datos sobre esta condici√≥n y desarrollar una herramienta pr√°ctica para mejorar la comunicaci√≥n. Para ello, cre√© un dashboard en Streamlit que explora los datos y permite enviar notificaciones SMS a los padres mediante una API.\nThe congenital hypothyroidism (CH). CH affects 1 in 2,000-4,000 newborns worldwide. Early detection is vital. Untreated CH can lead to intellectual disability and growth delays.\n\n\n\nLa base de datos inicial requer√≠a un procesamiento exhaustivo para ser √∫til. Los pasos principales fueron:\n\nEstandarizaci√≥n: Uniform√© formatos de fechas, nombres y unidades (ej. niveles hormonales).\nValores faltantes: Gestion√© datos incompletos, priorizando la imputaci√≥n cuando fue posible.\nCorrecci√≥n de errores: Elimin√© valores imposibles (ej. TSH negativa) y revis√© inconsistencias.\nPreparaci√≥n para SMS: Asegur√© que los datos de contacto (tel√©fonos) estuvieran limpios y en un formato compatible con la API.\nC√≥digo documentado: El proceso est√° disponible en un Jupyter Notebook [enlace si lo subes].\n\nEl resultado es un conjunto de datos confiable para an√°lisis y comunicaci√≥n.\n\n\n\nEl an√°lisis exploratorio revel√≥ patrones en los datos, mientras que el dashboard en Streamlit los hace accesibles. Algunas visualizaciones incluyen:\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport plotly.graph_objects as go\n\nnp.random.seed(42)\ntsh_levels = np.random.lognormal(mean=2, sigma=0.5, size=100)\n\n# Gr√°fico de seaborn\nplt.figure(figsize=(10, 6))\nsns.histplot(tsh_levels, bins=20, kde=True, color='skyblue')\nplt.title('Distribuci√≥n de Niveles de TSH al Nacer')\nplt.xlabel('Nivel de TSH (mIU/L)')\nplt.ylabel('Frecuencia')\nplt.axvline(tsh_levels.mean(), color='red', linestyle='--', label=f'Media: {tsh_levels.mean():.2f}')\nplt.legend()\nplt.grid(alpha=0.3)\nplt.show()\n\n# Gr√°fico de Plotly\nfig = go.Figure(data=[go.Histogram(x=tsh_levels, nbinsx=20, histnorm='probability density')])\nfig.update_layout(title='Distribuci√≥n de Niveles de TSH al Nacer',\n                    xaxis_title='Nivel de TSH (mIU/L)',\n                    yaxis_title='Densidad de probabilidad')\nfig.add_vline(x=tsh_levels.mean(), line_dash='dash', line_color='red', annotation_text=f'Media: {tsh_levels.mean():.2f}')\nfig.show()\n\n\n\n\n\nDistribuci√≥n de niveles de TSH al nacer\n\n\n\n\n                            \n                                            \n\n\n\n\n\nDistribuci√≥n de TSH\n\n\n\n\n\nExploraci√≥n interactiva: Los usuarios pueden filtrar y visualizar datos (ej. TSH por edad o regi√≥n).\nNotificaciones SMS: Integra una API (como Twilio) para enviar alertas a los padres con informaci√≥n clave, como ‚ÄúSu hijo/a tiene un nivel de TSH elevado. Contacte a su m√©dico‚Äù.\nAcceso: Disponible aqu√≠.\n\n\n\n\nInterfaz del Dashboard\n\n\n\n\n\n\nLos niveles de TSH var√≠an ampliamente, lo que indica la necesidad de segmentar los casos.\nLa falta de informaci√≥n a los padres parece estar relacionada con datos de contacto incompletos o desactualizados.\n\n\n\n\n\nEsta secci√≥n est√° en progreso. Planeo incluir: - Modelos predictivos para identificar casos de riesgo elevado. - An√°lisis de correlaci√≥n entre factores demogr√°ficos y retrasos en la notificaci√≥n. - Evaluaci√≥n de la efectividad de las notificaciones SMS en la respuesta de los padres.\nPronto actualizar√© esta entrada con m√°s detalles.\n\n\n\nEl proyecto no solo transforma datos crudos en informaci√≥n √∫til, sino que tambi√©n aborda un problema real: la comunicaci√≥n con los padres. El dashboard combina an√°lisis y acci√≥n, ofreciendo una soluci√≥n pr√°ctica para mejorar la atenci√≥n temprana del hipotiroidismo cong√©nito.\n\n\n\n\nFinalizar la secci√≥n de estad√≠stica avanzada.\nOptimizar el dashboard con m√°s opciones de personalizaci√≥n para los SMS.\nProbar la API de SMS en un entorno real y evaluar su impacto."
  },
  {
    "objectID": "projects/congenital-hypothyroidism.html#resumen-del-proyecto",
    "href": "projects/congenital-hypothyroidism.html#resumen-del-proyecto",
    "title": "An√°lisis del Hipotiroidismo Cong√©nito: Datos y Comunicaci√≥n",
    "section": "",
    "text": "El hipotiroidismo cong√©nito es una condici√≥n presente desde el nacimiento que afecta la gl√°ndula tiroides, pero un problema cr√≠tico es que los padres a menudo no son informados a tiempo. Este proyecto tiene dos objetivos: analizar una base de datos sobre esta condici√≥n y desarrollar una herramienta pr√°ctica para mejorar la comunicaci√≥n. Para ello, cre√© un dashboard en Streamlit que explora los datos y permite enviar notificaciones SMS a los padres mediante una API.\nThe congenital hypothyroidism (CH). CH affects 1 in 2,000-4,000 newborns worldwide. Early detection is vital. Untreated CH can lead to intellectual disability and growth delays."
  },
  {
    "objectID": "projects/congenital-hypothyroidism.html#parte-1-limpieza-de-la-base-de-datos",
    "href": "projects/congenital-hypothyroidism.html#parte-1-limpieza-de-la-base-de-datos",
    "title": "An√°lisis del Hipotiroidismo Cong√©nito: Datos y Comunicaci√≥n",
    "section": "",
    "text": "La base de datos inicial requer√≠a un procesamiento exhaustivo para ser √∫til. Los pasos principales fueron:\n\nEstandarizaci√≥n: Uniform√© formatos de fechas, nombres y unidades (ej. niveles hormonales).\nValores faltantes: Gestion√© datos incompletos, priorizando la imputaci√≥n cuando fue posible.\nCorrecci√≥n de errores: Elimin√© valores imposibles (ej. TSH negativa) y revis√© inconsistencias.\nPreparaci√≥n para SMS: Asegur√© que los datos de contacto (tel√©fonos) estuvieran limpios y en un formato compatible con la API.\nC√≥digo documentado: El proceso est√° disponible en un Jupyter Notebook [enlace si lo subes].\n\nEl resultado es un conjunto de datos confiable para an√°lisis y comunicaci√≥n."
  },
  {
    "objectID": "projects/congenital-hypothyroidism.html#parte-2-exploraci√≥n-de-datos-y-dashboard",
    "href": "projects/congenital-hypothyroidism.html#parte-2-exploraci√≥n-de-datos-y-dashboard",
    "title": "An√°lisis del Hipotiroidismo Cong√©nito: Datos y Comunicaci√≥n",
    "section": "",
    "text": "El an√°lisis exploratorio revel√≥ patrones en los datos, mientras que el dashboard en Streamlit los hace accesibles. Algunas visualizaciones incluyen:\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport plotly.graph_objects as go\n\nnp.random.seed(42)\ntsh_levels = np.random.lognormal(mean=2, sigma=0.5, size=100)\n\n# Gr√°fico de seaborn\nplt.figure(figsize=(10, 6))\nsns.histplot(tsh_levels, bins=20, kde=True, color='skyblue')\nplt.title('Distribuci√≥n de Niveles de TSH al Nacer')\nplt.xlabel('Nivel de TSH (mIU/L)')\nplt.ylabel('Frecuencia')\nplt.axvline(tsh_levels.mean(), color='red', linestyle='--', label=f'Media: {tsh_levels.mean():.2f}')\nplt.legend()\nplt.grid(alpha=0.3)\nplt.show()\n\n# Gr√°fico de Plotly\nfig = go.Figure(data=[go.Histogram(x=tsh_levels, nbinsx=20, histnorm='probability density')])\nfig.update_layout(title='Distribuci√≥n de Niveles de TSH al Nacer',\n                    xaxis_title='Nivel de TSH (mIU/L)',\n                    yaxis_title='Densidad de probabilidad')\nfig.add_vline(x=tsh_levels.mean(), line_dash='dash', line_color='red', annotation_text=f'Media: {tsh_levels.mean():.2f}')\nfig.show()\n\n\n\n\n\nDistribuci√≥n de niveles de TSH al nacer\n\n\n\n\n                            \n                                            \n\n\n\n\n\nDistribuci√≥n de TSH\n\n\n\n\n\nExploraci√≥n interactiva: Los usuarios pueden filtrar y visualizar datos (ej. TSH por edad o regi√≥n).\nNotificaciones SMS: Integra una API (como Twilio) para enviar alertas a los padres con informaci√≥n clave, como ‚ÄúSu hijo/a tiene un nivel de TSH elevado. Contacte a su m√©dico‚Äù.\nAcceso: Disponible aqu√≠.\n\n\n\n\nInterfaz del Dashboard\n\n\n\n\n\n\nLos niveles de TSH var√≠an ampliamente, lo que indica la necesidad de segmentar los casos.\nLa falta de informaci√≥n a los padres parece estar relacionada con datos de contacto incompletos o desactualizados."
  },
  {
    "objectID": "projects/congenital-hypothyroidism.html#parte-3-estad√≠stica-avanzada-en-desarrollo",
    "href": "projects/congenital-hypothyroidism.html#parte-3-estad√≠stica-avanzada-en-desarrollo",
    "title": "An√°lisis del Hipotiroidismo Cong√©nito: Datos y Comunicaci√≥n",
    "section": "",
    "text": "Esta secci√≥n est√° en progreso. Planeo incluir: - Modelos predictivos para identificar casos de riesgo elevado. - An√°lisis de correlaci√≥n entre factores demogr√°ficos y retrasos en la notificaci√≥n. - Evaluaci√≥n de la efectividad de las notificaciones SMS en la respuesta de los padres.\nPronto actualizar√© esta entrada con m√°s detalles."
  },
  {
    "objectID": "projects/congenital-hypothyroidism.html#conclusiones-preliminares",
    "href": "projects/congenital-hypothyroidism.html#conclusiones-preliminares",
    "title": "An√°lisis del Hipotiroidismo Cong√©nito: Datos y Comunicaci√≥n",
    "section": "",
    "text": "El proyecto no solo transforma datos crudos en informaci√≥n √∫til, sino que tambi√©n aborda un problema real: la comunicaci√≥n con los padres. El dashboard combina an√°lisis y acci√≥n, ofreciendo una soluci√≥n pr√°ctica para mejorar la atenci√≥n temprana del hipotiroidismo cong√©nito."
  },
  {
    "objectID": "projects/congenital-hypothyroidism.html#pr√≥ximos-pasos",
    "href": "projects/congenital-hypothyroidism.html#pr√≥ximos-pasos",
    "title": "An√°lisis del Hipotiroidismo Cong√©nito: Datos y Comunicaci√≥n",
    "section": "",
    "text": "Finalizar la secci√≥n de estad√≠stica avanzada.\nOptimizar el dashboard con m√°s opciones de personalizaci√≥n para los SMS.\nProbar la API de SMS en un entorno real y evaluar su impacto."
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Data Analysis Projects",
    "section": "",
    "text": "Below you‚Äôll find a collection of my data analysis projects. Each project demonstrates different techniques and approaches to real-world data problems.\n\nProyectos\nCurae hendrerit donec commodo hendrerit egestas tempus, turpis facilisis nostra nunc. Vestibulum dui eget ultrices.\n\n\n\n\n\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Title\n      \n      \n        Author\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nMapa de Comunas de Barrancabermeja: Un Recurso para el An√°lisis de Datos Geogr√°ficos\n\n\n\n\n\n\nSIG\n\nQGis\n\nGeoJson\n\nMap\n\n\n\n\n\n\n\n\n\nFeb 9, 2026\n\n\nLCPallares\n\n\n\n\n\n\n\n\n\n\n\n\nWeb Scraping: Ubicaciones de Common Service Centers (India)\n\n\n\n\n\n\nweb-scraping\n\ndata-collection\n\npython\n\nfreelance\n\n\n\n\n\n\n\n\n\nAug 20, 2023\n\n\nLC Pallares\n\n\n\n\n\n\n\n\n\n\n\n\nAn√°lisis del Hipotiroidismo Cong√©nito: Datos y Comunicaci√≥n\n\n\n\n\n\n\ndata cleaning\n\nexploratory analysis\n\nstreamlit\n\nhealth\n\nsms-api\n\n\n\nEste proyecto analiza datos de hipotiroidismo cong√©nito y aborda la falta de comunicaci√≥n con los padres mediante un dashboard interactivo en Streamlit que no solo explora los datos, sino que tambi√©n env√≠a notificaciones SMS usando una API.\n\n\n\n\n\nMar 24, 2025\n\n\nLC Pallares\n\n\n\n\n\n\n\n\n\n\n\n\nRick and Morty Mobile App with KivyMD\n\n\n\n\n\n\nMobile Development\n\nKivyMD\n\nPython\n\nAPI\n\n\n\nA cross-platform mobile app built with KivyMD and the Rick and Morty API, featuring character browsing, favorites, and authentication.\n\n\n\n\n\nJul 10, 2025\n\n\nLuis Carlos Pallares Ascanio\n\n\n\n\n\n\n\n\n\n\n\n\nAmazon Best Seller Analysis\n\n\n\n\n\n\nweb scraping\n\ndata cleaning\n\nvisualization\n\n\n\nStreamlit has quickly become the hot thing in data app frameworks. We put it to the test to see how well it stands up to the hype. Come for the review, stay for the code demo, including detailed examples of Altair plots.\n\n\n\n\n\nMar 1, 2025\n\n\nLC Pallares\n\n\n\n\n\n\n\n\n\n\n\n\nInteligencia de Retail: An√°lisis de Ventas en Myanmar\n\n\nExplorando patrones de consumo y rendimiento comercial mediante Python\n\n\n\nPython\n\nEDA\n\nPlotly\n\nRetail\n\n\n\nUn an√°lisis profundo sobre el comportamiento del consumidor en tres sucursales de Myanmar, enfocado en la optimizaci√≥n de inventarios y marketing.\n\n\n\n\n\nApr 23, 2025\n\n\nLC Pallares\n\n\n\n\n\n\n\n\n\n\n\n\nAn√°lisis del Hipotiroidismo Cong√©nito\n\n\n\n\n\n\ndata cleaning\n\nexploratory analysis\n\nstreamlit\n\nhealth\n\nsms-api\n\n\n\nEste proyecto analiza datos de hipotiroidismo cong√©nito y aborda la falta de comunicaci√≥n con los padres mediante un dashboard interactivo en Streamlit que no solo explora los datos, sino que tambi√©n env√≠a notificaciones SMS usando una API.\n\n\n\n\n\nMar 24, 2025\n\n\nLC Pallares\n\n\n\n\n\n\n\n\n\n\n\n\nUniversity Dropout Analysis\n\n\n\n\n\n\neducation\n\nstatistics\n\nanalysis\n\n\n\n\n\n\n\n\n\nMar 8, 2025\n\n\nLC Pallares\n\n\n\n\n\nNo matching items\n Back to top"
  },
  {
    "objectID": "projects/barrancabermeja-comunas-geojson.html",
    "href": "projects/barrancabermeja-comunas-geojson.html",
    "title": "Mapa de Comunas de Barrancabermeja: Un Recurso para el An√°lisis de Datos Geogr√°ficos",
    "section": "",
    "text": "Introducci√≥n\nEn este proyecto, se presentar√° el proceso de creaci√≥n de un mapa de comunas de Barrancabermeja utilizando QGIS. El objetivo es proporcionar un recurso valioso para el an√°lisis de datos geogr√°ficos en la ciudad. En este sentido, se describir√° la metodolog√≠a utilizada para crear el mapa, los resultados obtenidos y las posibles aplicaciones del mismo.\n\n\nMetodolog√≠a\nEl proceso de creaci√≥n del mapa se llev√≥ a cabo en los siguientes pasos:\n\nObtenci√≥n de datos: Se obtuvo el mapa de la ciudad de Barrancabermeja en formato shapefile a trav√©s de Catastro Barrancabermeja.\nCreaci√≥n de un nuevo shapefile: Se cre√≥ un nuevo shapefile para las comunas utilizando QGIS.\nDigitalizaci√≥n de comunas: Se digitalizaron las comunas utilizando herramientas de edici√≥n de QGIS.\nAgregaci√≥n de atributos: Se agregaron atributos a cada comuna, como nombre y c√≥digo.\n\n\n\nResultados\nEl resultado es un mapa de comunas de Barrancabermeja en formato GeoJSON, que se puede utilizar para an√°lisis de datos geogr√°ficos.\n\n\nMapa de Comunas\nAqu√≠ se muestra el mapa de comunas de Barrancabermeja:\n\n\n\nMapa de Comunas de Barrancabermeja\n\n\n\n\nAplicaciones\nEl mapa de comunas de Barrancabermeja puede ser utilizado para:\n\nAn√°lisis de datos demogr√°ficos\nPlanificaci√≥n urbana\nGesti√≥n de recursos\nInvestigaci√≥n acad√©mica\n\nC√≥digo y Datos El c√≥digo y los datos utilizados en este proyecto se encuentran disponibles en https://github.com/LCPallares/barrancabermeja-comunas-geojson.\nAgradecimientos Agradezco a Gestor Catastral Distrito de Barrancabermeja por proporcionar los datos geogr√°ficos de la zona urbana de Barrancabermeja.\nLicencia Este trabajo est√° bajo licencia https://creativecommons.org/licenses/by/4.0/.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "projects/web_scraping_csc_india.html",
    "href": "projects/web_scraping_csc_india.html",
    "title": "Web Scraping: Ubicaciones de Common Service Centers (India)",
    "section": "",
    "text": "Este proyecto naci√≥ de una solicitud freelance para mapear los Common Service Centers (CSCs) de India, centros f√≠sicos que brindan acceso a servicios digitales gubernamentales en zonas rurales y urbanas. El cliente necesitaba:\n\nUn listado estructurado de todos los CSCs.\n\nFiltros por estado, distrito y bloque.\n\nDatos para an√°lisis de cobertura geogr√°fica.\n\nEl sitio oficial (csclocator.com) no ofrec√≠a una API p√∫blica, por lo que desarroll√© un scraper automatizado para extraer los datos directamente del HTML."
  },
  {
    "objectID": "projects/web_scraping_csc_india.html#contexto-del-proyecto",
    "href": "projects/web_scraping_csc_india.html#contexto-del-proyecto",
    "title": "Web Scraping: Ubicaciones de Common Service Centers (India)",
    "section": "",
    "text": "Este proyecto naci√≥ de una solicitud freelance para mapear los Common Service Centers (CSCs) de India, centros f√≠sicos que brindan acceso a servicios digitales gubernamentales en zonas rurales y urbanas. El cliente necesitaba:\n\nUn listado estructurado de todos los CSCs.\n\nFiltros por estado, distrito y bloque.\n\nDatos para an√°lisis de cobertura geogr√°fica.\n\nEl sitio oficial (csclocator.com) no ofrec√≠a una API p√∫blica, por lo que desarroll√© un scraper automatizado para extraer los datos directamente del HTML."
  },
  {
    "objectID": "projects/web_scraping_csc_india.html#t√©cnicas-utilizadas",
    "href": "projects/web_scraping_csc_india.html#t√©cnicas-utilizadas",
    "title": "Web Scraping: Ubicaciones de Common Service Centers (India)",
    "section": "T√©cnicas Utilizadas",
    "text": "T√©cnicas Utilizadas\n# Tecnolog√≠as clave:\n- Python (BeautifulSoup, requests, pandas).\n- Web scraping con manejo de headers y delays.\n- Extracci√≥n de datos anidados (selectores HTML).\n- Almacenamiento en Excel para facilitar su uso."
  },
  {
    "objectID": "projects/web_scraping_csc_india.html#datos-obtenidos",
    "href": "projects/web_scraping_csc_india.html#datos-obtenidos",
    "title": "Web Scraping: Ubicaciones de Common Service Centers (India)",
    "section": "Datos Obtenidos",
    "text": "Datos Obtenidos\nSe extrajeron registros con las siguientes variables:\n\n\n\n\n\n\n\n\nVariable\nDescripci√≥n\nEjemplo\n\n\n\n\nvle_name\nNombre del operador del CSC\n‚ÄúAlankit_Deepika Rustagi‚Äù\n\n\naddress\nDirecci√≥n f√≠sica\n‚ÄúShop No: 4547, Karol Bagh‚Äù\n\n\nstate\nEstado\n‚Äúandaman-and-nicobar‚Äù\n\n\ndistrict\nDistrito\n‚Äúcentral‚Äù\n\n\nblock\nBloque administrativo\n‚Äúcentral-delhi-nielit‚Äù\n\n\n\n\nMuestra de Datos\n\n\n\nDatos resultantes"
  },
  {
    "objectID": "projects/web_scraping_csc_india.html#desaf√≠os-y-soluciones",
    "href": "projects/web_scraping_csc_india.html#desaf√≠os-y-soluciones",
    "title": "Web Scraping: Ubicaciones de Common Service Centers (India)",
    "section": "Desaf√≠os y Soluciones",
    "text": "Desaf√≠os y Soluciones\n\nEstructura compleja:\n\nLos CSCs se organizaban en jerarqu√≠as (estado ‚Üí distrito ‚Üí bloque).\n\nSoluci√≥n: Scrape√© secuencialmente los select options del HTML.\n\nProtecci√≥n contra scraping:\n\nEl sitio bloqueaba peticiones r√°pidas.\n\nSoluci√≥n: Implement√© headers con User-Agent y time.sleep(15) entre requests.\n\nDatos inconsistentes:\n\nAlgunas direcciones usaban formatos no estandarizados (ej: ‚ÄúWZ: 125 Dusghara‚Äù).\n\nSoluci√≥n: Normalizaci√≥n manual posterior en Excel."
  },
  {
    "objectID": "projects/web_scraping_csc_india.html#impacto-del-proyecto",
    "href": "projects/web_scraping_csc_india.html#impacto-del-proyecto",
    "title": "Web Scraping: Ubicaciones de Common Service Centers (India)",
    "section": "Impacto del Proyecto",
    "text": "Impacto del Proyecto\n\nEl cliente pudo identificar zonas con baja densidad de CSCs para priorizar nuevas instalaciones.\n\nLos datos se usaron como base para un sistema de geolocalizaci√≥n interno.\n\nDemostr√≥ que el scraping puede ser una alternativa viable cuando no hay APIs disponibles."
  },
  {
    "objectID": "projects/web_scraping_csc_india.html#c√≥digo-completo",
    "href": "projects/web_scraping_csc_india.html#c√≥digo-completo",
    "title": "Web Scraping: Ubicaciones de Common Service Centers (India)",
    "section": "C√≥digo Completo",
    "text": "C√≥digo Completo\nEl script de Python est√° disponible en GitHub o en el siguiente bloque:\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nimport time\n\ndef make_soup(url):\n    user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n    headers = {'User-Agent': user_agent}\n    response = requests.get(url, headers=headers)\n    \n    if response.status_code == 200:\n        soup = BeautifulSoup(response.text, 'html.parser')\n        return soup\n    else:\n        print(f\"Error while getting the page. State code: {response.status_code}\")\n        return None\n\n\ndef scrape_select_values(url):\n    soup = make_soup(url)\n    \n    state_selector = soup.find('select', {'id': 'state'})\n    district_selector = soup.find('select', {'id': 'district'})\n    block_selector = soup.find('select', {'id': 'block'})\n\n    state_options = state_selector.find_all('option')\n    district_options = district_selector.find_all('option')\n    block_options = block_selector.find_all('option')\n\n    state_data = [option.get(\"value\") for option in state_options[1:]]\n    district_data = [option.get(\"value\") for option in district_options[1:]]\n    block_data = [option.get(\"value\") for option in block_options[1:]]\n    \n    return state_data, district_data, block_data\n\n\ndef get_info(soup, state, district, block):\n    headings = soup.find('div', class_='col-lg-12')\n    if headings:\n        row = headings.find(class_=\"row\")\n        column_title = [x.text for x in row.find_all(\"th\")]\n        tr_elements = row.find_all('tr')\n\n        data_list = []\n\n        for tr in tr_elements[1:]:\n            td_elements = tr.find_all('td')\n            if len(td_elements) &gt;= 3:\n                vle_name = td_elements[0].text.strip()\n                address = td_elements[1].text.strip()\n                enlace = td_elements[2].find('a')['href']\n\n                data = {\n                    \"vle_name\": vle_name,\n                    \"address\": address,\n                    \"enlace\": enlace,\n                    \"state\": state,\n                    \"district\": district,\n                    \"block\": block\n                }\n\n                data_list.append(data)\n\n        return data_list\n\n    else:\n        print(f\"No data found for {state}/{district}/{block}\")\n        return []\n\n\nbase_url = \"https://www.csclocator.com\"\n\nall_data = []\nstate_list, district_list, block_list = scrape_select_values(\"https://www.csclocator.com/csc/delhi/delhi/new-delhi-nielit\")\n\nfor state in state_list:\n    for district in district_list:\n        for block in block_list:\n            url = f\"{base_url}/csc/{state}/{district}/{block}\"\n            print(f\"Scraping: {url}\")\n            soup = make_soup(url)\n\n            if soup:\n                data_list = get_info(soup, state, district, block)\n                all_data.extend(data_list)\n                df = pd.DataFrame(all_data)\n                df.to_excel(\"csc_data_partial.xlsx\", index=False)\n            time.sleep(15) \n\nfinal_df = pd.DataFrame(all_data)\nfinal_df.to_excel(\"csc_data.xlsx\", index=False)"
  },
  {
    "objectID": "projects/rick-and-morty-app.html",
    "href": "projects/rick-and-morty-app.html",
    "title": "Rick and Morty Mobile App with KivyMD",
    "section": "",
    "text": "This project is a mobile application built with Python, Kivy, and KivyMD, leveraging the Rick and Morty API to create an engaging experience for fans. The app allows users to browse characters, view detailed information, save favorites, and manage accounts through a sleek, Material Design-inspired interface. It√±s designed to be responsive across Android, iOS, and desktop platforms.\n\n\n\n\nFigura: Current development version of kivymd (2.0.1.dev0).\n\nFigura: Stable version of kivymd (1.1.1), no use of themes.\n\n\n\n\nLogin and Registration: Secure user authentication for personalized access.\nCharacter Browser: Displays characters from the Rick and Morty API, including name, species, and status.\nFavorites Screen: Save and view favorite characters.\nNavigation Bar: Intuitive navigation across app sections.\nResponsive UI: Built with KivyMD for a modern, cross-platform interface.\n\n\n\n\n\nPython: Core language for development.\nKivy: Framework for cross-platform UI.\nKivyMD: Material Design components for a polished look.\nRick and Morty API: Provides character data.\nRequests: Handles API calls.\n\n\n\n\nrickmorty_app/\n‚îú‚îÄ‚îÄ main.py                   # Entry point\n‚îú‚îÄ‚îÄ assets/                   # Static resources\n‚îÇ   ‚îú‚îÄ‚îÄ fonts/                # Custom fonts\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ font1\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ font2\n‚îÇ   ‚îú‚îÄ‚îÄ images/               # App images (logo, icons)\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ logo.svg\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ icon.png\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ placeholder.png\n‚îÇ   ‚îî‚îÄ‚îÄ screenshots/          # Screenshots for documentation\n‚îú‚îÄ‚îÄ components/               # Reusable UI components\n‚îÇ   ‚îú‚îÄ‚îÄ character_tile.py     # Logic for character tile\n‚îÇ   ‚îî‚îÄ‚îÄ character_tile.kv     # Design for character tile\n‚îú‚îÄ‚îÄ controllers/              # Business logic\n‚îÇ   ‚îú‚îÄ‚îÄ auth_controller.py    # Authentication logic\n‚îÇ   ‚îú‚îÄ‚îÄ character_controller.py # Character data handling\n‚îÇ   ‚îî‚îÄ‚îÄ favorites_controller.py # Favorites management\n‚îú‚îÄ‚îÄ models/                   # Data models\n‚îÇ   ‚îú‚îÄ‚îÄ user_model.py         # User data\n‚îÇ   ‚îú‚îÄ‚îÄ favorite_model.py     # Favorite characters\n‚îÇ   ‚îî‚îÄ‚îÄ database.py           # Database operations\n‚îú‚îÄ‚îÄ views/                    # UI screens\n‚îÇ   ‚îú‚îÄ‚îÄ auth/\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ login.kv          # Login screen design\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ login.py          # Login screen logic\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ register.kv       # Registration screen design\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ register.py       # Registration screen logic\n‚îÇ   ‚îú‚îÄ‚îÄ characters/\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ characters_list.kv  # Character list design\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ characters_list.py  # Character list logic\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ character_detail.kv # Character detail design\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ character_detail.py # Character detail logic\n‚îÇ   ‚îî‚îÄ‚îÄ favorites/\n‚îÇ       ‚îú‚îÄ‚îÄ favorites_list.kv # Favorites list design\n‚îÇ       ‚îî‚îÄ‚îÄ favorites_list.py # Favorites list logic\n‚îî‚îÄ‚îÄ utils/                    # Utilities\n    ‚îú‚îÄ‚îÄ api_client.py         # API requests\n    ‚îî‚îÄ‚îÄ helpers.py            # Helper functions\n\n\n\n\nEnvironment Setup:\n\nInstalled Python 3.8+, Kivy, KivyMD, and Requests.\nConfigured Buildozer for Android packaging, excluding screenshots/ in buildozer.spec to keep the APK lightweight:\nsource.exclude_dirs = tests, bin, venv, drive, .git, .vscode, __pycache__, .kivy, screenshots\n\nUI Development:\n\nUsed KivyMD to create responsive screens with .kv files for design and .py files for logic.\nImplemented a navigation bar for seamless transitions.\n\nAPI Integration:\n\nDeveloped api_client.py to fetch character data from the Rick and Morty API.\nHandled pagination to load multiple pages of characters.\n\nData Management:\n\nCreated a local database (database.py) for user credentials and favorites.\nManaged favorites through favorites_controller.py.\n\nDocumentation:\n\nAdded a README.md with setup instructions and placeholders for screenshots in assets/screenshots/.\n\n\n\n\n\n\nKivyMD Syntax: Learning to separate design (.kv) and logic (.py) was initially tricky but improved code organization.\nAPI Pagination: Required extra logic in api_client.py to handle multiple API pages.\nBuildozer: Ensuring screenshots/ was excluded in buildozer.spec avoided bloating the APK.\n\n\n\n\n\nAdd search functionality for characters.\nImplement offline caching for API data.\nEnhance UI with animations for smoother transitions.\nUpdate screenshots in assets/screenshots/ and refine the Quarto project page.\n\n\n\n\nCheck out the project on GitHub.\n\n\n\nThis project was a fun way to combine my passion for Rick and Morty with mobile development. KivyMD made it easy to create a modern UI, and the Rick and Morty API provided rich data. Documenting it in my Quarto website√±s projects section helps showcase my work and track progress. Stay tuned for updates!"
  },
  {
    "objectID": "projects/rick-and-morty-app.html#project-overview",
    "href": "projects/rick-and-morty-app.html#project-overview",
    "title": "Rick and Morty Mobile App with KivyMD",
    "section": "",
    "text": "This project is a mobile application built with Python, Kivy, and KivyMD, leveraging the Rick and Morty API to create an engaging experience for fans. The app allows users to browse characters, view detailed information, save favorites, and manage accounts through a sleek, Material Design-inspired interface. It√±s designed to be responsive across Android, iOS, and desktop platforms."
  },
  {
    "objectID": "projects/rick-and-morty-app.html#screenshots",
    "href": "projects/rick-and-morty-app.html#screenshots",
    "title": "Rick and Morty Mobile App with KivyMD",
    "section": "",
    "text": "Figura: Current development version of kivymd (2.0.1.dev0).\n\nFigura: Stable version of kivymd (1.1.1), no use of themes."
  },
  {
    "objectID": "projects/rick-and-morty-app.html#features",
    "href": "projects/rick-and-morty-app.html#features",
    "title": "Rick and Morty Mobile App with KivyMD",
    "section": "",
    "text": "Login and Registration: Secure user authentication for personalized access.\nCharacter Browser: Displays characters from the Rick and Morty API, including name, species, and status.\nFavorites Screen: Save and view favorite characters.\nNavigation Bar: Intuitive navigation across app sections.\nResponsive UI: Built with KivyMD for a modern, cross-platform interface."
  },
  {
    "objectID": "projects/rick-and-morty-app.html#tech-stack",
    "href": "projects/rick-and-morty-app.html#tech-stack",
    "title": "Rick and Morty Mobile App with KivyMD",
    "section": "",
    "text": "Python: Core language for development.\nKivy: Framework for cross-platform UI.\nKivyMD: Material Design components for a polished look.\nRick and Morty API: Provides character data.\nRequests: Handles API calls."
  },
  {
    "objectID": "projects/rick-and-morty-app.html#project-structure",
    "href": "projects/rick-and-morty-app.html#project-structure",
    "title": "Rick and Morty Mobile App with KivyMD",
    "section": "",
    "text": "rickmorty_app/\n‚îú‚îÄ‚îÄ main.py                   # Entry point\n‚îú‚îÄ‚îÄ assets/                   # Static resources\n‚îÇ   ‚îú‚îÄ‚îÄ fonts/                # Custom fonts\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ font1\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ font2\n‚îÇ   ‚îú‚îÄ‚îÄ images/               # App images (logo, icons)\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ logo.svg\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ icon.png\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ placeholder.png\n‚îÇ   ‚îî‚îÄ‚îÄ screenshots/          # Screenshots for documentation\n‚îú‚îÄ‚îÄ components/               # Reusable UI components\n‚îÇ   ‚îú‚îÄ‚îÄ character_tile.py     # Logic for character tile\n‚îÇ   ‚îî‚îÄ‚îÄ character_tile.kv     # Design for character tile\n‚îú‚îÄ‚îÄ controllers/              # Business logic\n‚îÇ   ‚îú‚îÄ‚îÄ auth_controller.py    # Authentication logic\n‚îÇ   ‚îú‚îÄ‚îÄ character_controller.py # Character data handling\n‚îÇ   ‚îî‚îÄ‚îÄ favorites_controller.py # Favorites management\n‚îú‚îÄ‚îÄ models/                   # Data models\n‚îÇ   ‚îú‚îÄ‚îÄ user_model.py         # User data\n‚îÇ   ‚îú‚îÄ‚îÄ favorite_model.py     # Favorite characters\n‚îÇ   ‚îî‚îÄ‚îÄ database.py           # Database operations\n‚îú‚îÄ‚îÄ views/                    # UI screens\n‚îÇ   ‚îú‚îÄ‚îÄ auth/\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ login.kv          # Login screen design\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ login.py          # Login screen logic\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ register.kv       # Registration screen design\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ register.py       # Registration screen logic\n‚îÇ   ‚îú‚îÄ‚îÄ characters/\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ characters_list.kv  # Character list design\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ characters_list.py  # Character list logic\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ character_detail.kv # Character detail design\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ character_detail.py # Character detail logic\n‚îÇ   ‚îî‚îÄ‚îÄ favorites/\n‚îÇ       ‚îú‚îÄ‚îÄ favorites_list.kv # Favorites list design\n‚îÇ       ‚îî‚îÄ‚îÄ favorites_list.py # Favorites list logic\n‚îî‚îÄ‚îÄ utils/                    # Utilities\n    ‚îú‚îÄ‚îÄ api_client.py         # API requests\n    ‚îî‚îÄ‚îÄ helpers.py            # Helper functions"
  },
  {
    "objectID": "projects/rick-and-morty-app.html#development-process",
    "href": "projects/rick-and-morty-app.html#development-process",
    "title": "Rick and Morty Mobile App with KivyMD",
    "section": "",
    "text": "Environment Setup:\n\nInstalled Python 3.8+, Kivy, KivyMD, and Requests.\nConfigured Buildozer for Android packaging, excluding screenshots/ in buildozer.spec to keep the APK lightweight:\nsource.exclude_dirs = tests, bin, venv, drive, .git, .vscode, __pycache__, .kivy, screenshots\n\nUI Development:\n\nUsed KivyMD to create responsive screens with .kv files for design and .py files for logic.\nImplemented a navigation bar for seamless transitions.\n\nAPI Integration:\n\nDeveloped api_client.py to fetch character data from the Rick and Morty API.\nHandled pagination to load multiple pages of characters.\n\nData Management:\n\nCreated a local database (database.py) for user credentials and favorites.\nManaged favorites through favorites_controller.py.\n\nDocumentation:\n\nAdded a README.md with setup instructions and placeholders for screenshots in assets/screenshots/."
  },
  {
    "objectID": "projects/rick-and-morty-app.html#challenges",
    "href": "projects/rick-and-morty-app.html#challenges",
    "title": "Rick and Morty Mobile App with KivyMD",
    "section": "",
    "text": "KivyMD Syntax: Learning to separate design (.kv) and logic (.py) was initially tricky but improved code organization.\nAPI Pagination: Required extra logic in api_client.py to handle multiple API pages.\nBuildozer: Ensuring screenshots/ was excluded in buildozer.spec avoided bloating the APK."
  },
  {
    "objectID": "projects/rick-and-morty-app.html#future-improvements",
    "href": "projects/rick-and-morty-app.html#future-improvements",
    "title": "Rick and Morty Mobile App with KivyMD",
    "section": "",
    "text": "Add search functionality for characters.\nImplement offline caching for API data.\nEnhance UI with animations for smoother transitions.\nUpdate screenshots in assets/screenshots/ and refine the Quarto project page."
  },
  {
    "objectID": "projects/rick-and-morty-app.html#source-code",
    "href": "projects/rick-and-morty-app.html#source-code",
    "title": "Rick and Morty Mobile App with KivyMD",
    "section": "",
    "text": "Check out the project on GitHub."
  },
  {
    "objectID": "projects/rick-and-morty-app.html#conclusion",
    "href": "projects/rick-and-morty-app.html#conclusion",
    "title": "Rick and Morty Mobile App with KivyMD",
    "section": "",
    "text": "This project was a fun way to combine my passion for Rick and Morty with mobile development. KivyMD made it easy to create a modern UI, and the Rick and Morty API provided rich data. Documenting it in my Quarto website√±s projects section helps showcase my work and track progress. Stay tuned for updates!"
  },
  {
    "objectID": "projects/supermarket_sales_myanmar.html",
    "href": "projects/supermarket_sales_myanmar.html",
    "title": "Inteligencia de Retail: An√°lisis de Ventas en Myanmar",
    "section": "",
    "text": "NoteObjetivo del An√°lisis\n\n\n\nIdentificar los motores de ingresos y el comportamiento de compra para proponer estrategias basadas en datos que mejoren la rentabilidad de la cadena de supermercados."
  },
  {
    "objectID": "projects/supermarket_sales_myanmar.html#resumen-del-proyecto",
    "href": "projects/supermarket_sales_myanmar.html#resumen-del-proyecto",
    "title": "Inteligencia de Retail: An√°lisis de Ventas en Myanmar",
    "section": "üìù Resumen del Proyecto",
    "text": "üìù Resumen del Proyecto\nEste an√°lisis profundiza en las operaciones de una cadena de supermercados en Myanmar, utilizando un conjunto de datos que detalla transacciones hist√≥ricas en tres ciudades principales. El objetivo es transformar datos crudos en inteligencia de negocios para identificar motores de ingresos y entender las din√°micas del consumidor local.\n\n\n\n\n\n\nNoteEl Dataset\n\n\n\nEl conjunto de datos abarca variables clave como categor√≠as de productos, demograf√≠a del cliente, m√©todos de pago y m√©tricas financieras (margen bruto e impuestos), permitiendo una visi√≥n 360¬∞ de la din√°mica del retail.\n\n\n\nüéØ Objetivos Principales\n\nOptimizaci√≥n Comercial: Identificar las l√≠neas de productos con mayor rendimiento.\nInteligencia del Cliente: Analizar patrones de compra seg√∫n demograf√≠a y tipo de membres√≠a.\nAn√°lisis Temporal: Detectar picos de demanda para mejorar la planificaci√≥n operativa."
  },
  {
    "objectID": "projects/supermarket_sales_myanmar.html#metodolog√≠a",
    "href": "projects/supermarket_sales_myanmar.html#metodolog√≠a",
    "title": "Inteligencia de Retail: An√°lisis de Ventas en Myanmar",
    "section": "üõ†Ô∏è Metodolog√≠a",
    "text": "üõ†Ô∏è Metodolog√≠a\nPara este an√°lisis, implementamos un pipeline de datos moderno que prioriza la velocidad y el rigor estad√≠stico, estructurado en tres niveles:\n\n1. Ingesta y Limpieza (Pandas)\n\nNormalizaci√≥n: Renombramos el esquema a snake_case y ajustamos tipos de datos (fechas y categor√≠as) para asegurar la integridad de los c√°lculos.\nCuraci√≥n: Tratamiento de nulos y validaci√≥n de rangos en precios y cantidades.\n\n\n\n2. Motor de Consultas (DuckDB)\n\nEficiencia: Utilizamos DuckDB como motor anal√≠tico in-process. Esto nos permite ejecutar consultas SQL complejas (como el Market Basket Analysis) directamente sobre los DataFrames de Pandas, logrando una ejecuci√≥n mucho m√°s r√°pida que los m√©todos tradicionales.\n\n\n\n3. Validaci√≥n y Visualizaci√≥n (Scipy & Plotly)\n\nRigor: Aplicamos pruebas de hip√≥tesis (T-Tests) para confirmar que los hallazgos no son producto del azar.\nInteractividad: Traducimos los resultados en gr√°ficos din√°micos que permiten explorar dimensiones de tiempo, ciudad y producto.\n\n\n\n\n\n\n\nTipEl Diferenciador T√©cnico\n\n\n\nLa combinaci√≥n de Pandas + DuckDB permite que este flujo de trabajo sea escalable: la flexibilidad de Python con la potencia de un motor SQL dise√±ado para anal√≠tica masiva.\n\n\n\nPandasDuckDB (SQL)\n\n\n\n\nVer c√≥digo\nimport pandas as pd\n\n# Carga y normalizaci√≥n de columnas a snake_case\ndf = pd.read_csv(\"../data/supermarket_sales.csv\")\n# df.columns = [c.lower().replace(' ', '_') for c in df.columns]\n\n# Conversi√≥n de tipos\n# df['date'] = pd.to_datetime(df['date'])\ndf.head(3)\n\n\n\n\n\n\n\n\n\nInvoice ID\nBranch\nCity\nCustomer type\nGender\nProduct line\nUnit price\nQuantity\nTax 5%\nTotal\nDate\nTime\nPayment\nCost of goods sold\nGross margin percentage\nGross income\nCustomer stratification rating\n\n\n\n\n0\n750-67-8428\nA\nYangon\nMember\nFemale\nHealth and beauty\n74.69\n7\n26.1415\n548.9715\n1/5/2019\n13:08\nEwallet\n522.83\n4.761905\n26.1415\n9.1\n\n\n1\n226-31-3081\nC\nNaypyitaw\nNormal\nFemale\nElectronic accessories\n15.28\n5\n3.8200\n80.2200\n3/8/2019\n10:29\nCash\n76.40\n4.761905\n3.8200\n9.6\n\n\n2\n631-41-3108\nA\nYangon\nNormal\nMale\nHome and lifestyle\n46.33\n7\n16.2155\n340.5255\n3/3/2019\n13:23\nCredit card\n324.31\n4.761905\n16.2155\n7.4\n\n\n\n\n\n\n\n\n\n\n\nVer c√≥digo\nimport duckdb\n\n# Conexi√≥n in-memory y registro del DataFrame existente\ncon = duckdb.connect()\ncon.register('sales', df)\n\n# Verificaci√≥n de la tabla mediante SQL\ncon.execute(\"SELECT * FROM sales LIMIT 3\").fetchdf()\n\n\n\n\n\n\n\n\n\nInvoice ID\nBranch\nCity\nCustomer type\nGender\nProduct line\nUnit price\nQuantity\nTax 5%\nTotal\nDate\nTime\nPayment\nCost of goods sold\nGross margin percentage\nGross income\nCustomer stratification rating\n\n\n\n\n0\n750-67-8428\nA\nYangon\nMember\nFemale\nHealth and beauty\n74.69\n7\n26.1415\n548.9715\n1/5/2019\n13:08\nEwallet\n522.83\n4.761905\n26.1415\n9.1\n\n\n1\n226-31-3081\nC\nNaypyitaw\nNormal\nFemale\nElectronic accessories\n15.28\n5\n3.8200\n80.2200\n3/8/2019\n10:29\nCash\n76.40\n4.761905\n3.8200\n9.6\n\n\n2\n631-41-3108\nA\nYangon\nNormal\nMale\nHome and lifestyle\n46.33\n7\n16.2155\n340.5255\n3/3/2019\n13:23\nCredit card\n324.31\n4.761905\n16.2155\n7.4"
  },
  {
    "objectID": "projects/supermarket_sales_myanmar.html#parte-1-pipeline-de-limpieza-y-estandarizaci√≥n",
    "href": "projects/supermarket_sales_myanmar.html#parte-1-pipeline-de-limpieza-y-estandarizaci√≥n",
    "title": "Inteligencia de Retail: An√°lisis de Ventas en Myanmar",
    "section": "üõ†Ô∏è Parte 1: Pipeline de Limpieza y Estandarizaci√≥n",
    "text": "üõ†Ô∏è Parte 1: Pipeline de Limpieza y Estandarizaci√≥n\nLa base de datos original (supermarket_sales.csv) presentaba inconsistencias t√≠picas de registros transaccionales crudos. Para transformar este archivo en un activo anal√≠tico confiable, ejecutamos un proceso de curaci√≥n estructurado:\n\nValidaci√≥n Temporal: Conversi√≥n de fechas a objetos datetime para habilitar an√°lisis de series de tiempo.\nTratamiento de Integridad: Eliminaci√≥n de registros con valores nulos en columnas cr√≠ticas (unit_price, quantity) y filtrado de errores operativos (cantidades menores o iguales a cero).\nEnriquecimiento (Feature Engineering): Extracci√≥n de dimensiones temporales como el nombre del d√≠a y el mes para identificar patrones estacionales.\n\n\nImplementaci√≥n T√©cnica\nA continuaci√≥n, comparamos c√≥mo abordar este flujo de limpieza utilizando la manipulaci√≥n procedimental de Pandas frente a la potencia declarativa de DuckDB:\n\nPandasDuckDB (SQL)\n\n\n\n\nVer c√≥digo\nimport pandas as pd\nimport numpy as np\n\n# Cargar y estandarizar formatos\n# df = pd.read_csv(\"../data/supermarket_sales.csv\")\ndf['Date'] = pd.to_datetime(df['Date'], errors='coerce')\ndf['Product line'] = df['Product line'].str.title()\n\n# Manejo de valores faltantes y correcci√≥n de errores\ndf = df.dropna(subset=['Unit price', 'Quantity'])\ndf = df[df['Quantity'] &gt; 0]\n\n# Enriquecimiento de columnas\ndf['Day of Week'] = df['Date'].dt.day_name()\ndf['Month'] = df['Date'].dt.month_name()\n\ndf[['Invoice ID', 'Date', 'Day of Week', 'Month']].head(3)\n\n\n\n\n\n\n\n\n\nInvoice ID\nDate\nDay of Week\nMonth\n\n\n\n\n0\n750-67-8428\n2019-01-05\nSaturday\nJanuary\n\n\n1\n226-31-3081\n2019-03-08\nFriday\nMarch\n\n\n2\n631-41-3108\n2019-03-03\nSunday\nMarch\n\n\n\n\n\n\n\n\n\n\n\nVer c√≥digo\nimport duckdb\n\n# Conectamos DuckDB al DataFrame cargado\ncon = duckdb.connect()\ncon.register('raw_data', df)\n\n# Replicamos la l√≥gica de limpieza mediante SQL\nquery = \"\"\"\nSELECT \n    *,\n    dayname(Date) AS Day_of_Week,\n    monthname(Date) AS Month\nFROM raw_data\nWHERE \"Unit price\" IS NOT NULL \n  AND Quantity &gt; 0\nLIMIT 3\n\"\"\"\ncon.execute(query).fetchdf()\n\n\n\n\n\n\n\n\n\nInvoice ID\nBranch\nCity\nCustomer type\nGender\nProduct line\nUnit price\nQuantity\nTax 5%\nTotal\n...\nTime\nPayment\nCost of goods sold\nGross margin percentage\nGross income\nCustomer stratification rating\nDay of Week\nMonth\nDay_of_Week\nMonth_1\n\n\n\n\n0\n750-67-8428\nA\nYangon\nMember\nFemale\nHealth And Beauty\n74.69\n7\n26.1415\n548.9715\n...\n13:08\nEwallet\n522.83\n4.761905\n26.1415\n9.1\nSaturday\nJanuary\nSaturday\nJanuary\n\n\n1\n226-31-3081\nC\nNaypyitaw\nNormal\nFemale\nElectronic Accessories\n15.28\n5\n3.8200\n80.2200\n...\n10:29\nCash\n76.40\n4.761905\n3.8200\n9.6\nFriday\nMarch\nFriday\nMarch\n\n\n2\n631-41-3108\nA\nYangon\nNormal\nMale\nHome And Lifestyle\n46.33\n7\n16.2155\n340.5255\n...\n13:23\nCredit card\n324.31\n4.761905\n16.2155\n7.4\nSunday\nMarch\nSunday\nMarch\n\n\n\n\n3 rows √ó 21 columns\n\n\n\n\n\n\n\n\n\n\n\n\nNoteResultado del Proceso\n\n\n\nEste pipeline garantiza un conjunto de datos consistente, almacenado internamente para las fases de visualizaci√≥n. La estandarizaci√≥n de las categor√≠as (ej. Title Case) asegura que los reportes finales mantengan una est√©tica profesional y uniforme.\n\n\nxxxxxxxxxxxxxxxxxxxxxxxxxx"
  },
  {
    "objectID": "projects/supermarket_sales_myanmar.html#parte-2-an√°lisis-exploratorio-y-visualizaciones",
    "href": "projects/supermarket_sales_myanmar.html#parte-2-an√°lisis-exploratorio-y-visualizaciones",
    "title": "Inteligencia de Retail: An√°lisis de Ventas en Myanmar",
    "section": "Parte 2: An√°lisis Exploratorio y Visualizaciones",
    "text": "Parte 2: An√°lisis Exploratorio y Visualizaciones"
  },
  {
    "objectID": "projects/supermarket_sales_myanmar.html#parte-2.1-estad√≠sticas-descriptivas",
    "href": "projects/supermarket_sales_myanmar.html#parte-2.1-estad√≠sticas-descriptivas",
    "title": "Inteligencia de Retail: An√°lisis de Ventas en Myanmar",
    "section": "üî¢ Parte 2.1: Estad√≠sticas Descriptivas",
    "text": "üî¢ Parte 2.1: Estad√≠sticas Descriptivas\nAntes de profundizar en patrones visuales, es fundamental auditar las m√©tricas de tendencia central y dispersi√≥n. Este paso nos permite identificar el ‚Äúcomportamiento promedio‚Äù de las transacciones y detectar posibles valores at√≠picos que puedan sesgar el an√°lisis.\n\nPerfil Estad√≠stico del Inventario y Ventas\nAnalizamos variables cr√≠ticas como el precio unitario, la cantidad de art√≠culos por ticket y el margen bruto.\n\nPandas (Resumen Transpuesto)DuckDB (Agregaciones Manuales)\n\n\nLa funci√≥n .describe() de Pandas es la forma m√°s r√°pida de obtener un panorama completo, incluyendo cuartiles y desviaci√≥n est√°ndar.\n\n\nVer c√≥digo\n# Seleccionamos las columnas num√©ricas clave\nmetrics = ['Unit price', 'Quantity', 'Tax 5%', 'Total', 'Gross income']\n\n# Generamos el resumen y aplicamos estilo para mejorar la lectura\nstats_summary = df[metrics].describe().T\n\n# Formateo est√©tico para el reporte\nstats_summary.style.background_gradient(cmap='Blues').format(\"{:.2f}\")\n\n\n\n\n\n\n\n¬†\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nUnit price\n1000.00\n55.67\n26.49\n10.08\n32.88\n55.23\n77.94\n99.96\n\n\nQuantity\n1000.00\n5.51\n2.92\n1.00\n3.00\n5.00\n8.00\n10.00\n\n\nTax 5%\n1000.00\n15.38\n11.71\n0.51\n5.92\n12.09\n22.45\n49.65\n\n\nTotal\n1000.00\n322.97\n245.89\n10.68\n124.42\n253.85\n471.35\n1042.65\n\n\nGross income\n1000.00\n15.38\n11.71\n0.51\n5.92\n12.09\n22.45\n49.65\n\n\n\n\n\n\n\nCon SQL, tenemos un control total sobre qu√© m√©tricas calcular, permitiendo una visi√≥n personalizada de la eficiencia operativa.\n\n\nVer c√≥digo\nquery_stats = \"\"\"\nSELECT \n    AVG(\"Unit price\") AS avg_price,\n    MIN(\"Unit price\") AS min_price,\n    MAX(\"Unit price\") AS max_price,\n    AVG(Quantity) AS avg_quantity,\n    SUM(Total) AS total_revenue,\n    AVG(\"gross income\") AS avg_income\nFROM df\n\"\"\"\ncon.execute(query_stats).fetchdf()\n\n\n\n\n\n\n\n\n\navg_price\nmin_price\nmax_price\navg_quantity\ntotal_revenue\navg_income\n\n\n\n\n0\n55.67213\n10.08\n99.96\n5.51\n322966.749\n15.379369\n\n\n\n\n\n\n\n\n\n\n\n\nHallazgos de la Auditor√≠a Num√©rica\nAl observar las tablas anteriores, podemos extraer conclusiones inmediatas sobre la operaci√≥n:\n\nTicket Promedio: La media de las ventas se sit√∫a cerca de los 322.97 MMK, con una desviaci√≥n est√°ndar considerable, lo que indica una alta heterogeneidad en el tama√±o de las compras.\nVolumen de Art√≠culos: En promedio, los clientes adquieren 5.5 unidades por transacci√≥n, con un rango que va desde 1 hasta 10 art√≠culos.\nMargen de Beneficio: El ingreso bruto promedio por ticket es de aproximadamente 15.38 MMK, manteniendo una relaci√≥n constante con el impuesto aplicado.\n\n\n\n\n\n\n\nNoteObservaci√≥n sobre el Margen\n\n\n\nEl gross margin percentage se mantiene constante en el 4.76% para todos los registros. Esto indica una pol√≠tica de precios centralizada donde el margen no var√≠a por categor√≠a de producto, sino que depende estrictamente del volumen de venta."
  },
  {
    "objectID": "projects/supermarket_sales_myanmar.html#parte-2.2-visualizaci√≥n-y-tendencias-de-venta",
    "href": "projects/supermarket_sales_myanmar.html#parte-2.2-visualizaci√≥n-y-tendencias-de-venta",
    "title": "Inteligencia de Retail: An√°lisis de Ventas en Myanmar",
    "section": "üìä Parte 2.2: Visualizaci√≥n y Tendencias de Venta",
    "text": "üìä Parte 2.2: Visualizaci√≥n y Tendencias de Venta\nEn esta etapa, transformamos los datos en activos visuales interactivos. El objetivo es identificar patrones de consumo que las tablas est√°ticas podr√≠an ocultar. Utilizaremos Plotly para la capa de presentaci√≥n por su alta interactividad.\n\n1. Ventas por Linea de Producto\nEste an√°lisis identifica qu√© l√≠neas de producto son los motores de ingresos, permitiendo priorizar esfuerzos de inventario y marketing.\n\nVisualizaci√≥n (Plotly)Alternativa con DuckDB (SQL)\n\n\n\n\nVer c√≥digo\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\n\n# Generamos la variable con Pandas para la gr√°fica\nsales_by_category = df.groupby('Product line')['Total'].sum().sort_values(ascending=False).reset_index()\n\nfig = px.bar(sales_by_category, x='Total', y='Product line', \n             orientation='h',\n             title='Ventas Totales por Categor√≠a de Producto',\n             labels={'Total': 'Ventas Totales (MMK)', 'Product line': 'Categor√≠a'},\n             color='Total', color_continuous_scale='Viridis')\n\nfig.update_layout(showlegend=False, yaxis={'categoryorder':'total ascending'})\nfig.show()\n\n\n\n\n                            \n                                            \n\n\nFigure¬†1\n\n\n\n\n\n\n\n\nVer c√≥digo\n# As√≠ se obtendr√≠a la misma variable usando l√≥gica SQL\nquery_line = \"\"\"\nSELECT \n    \"Product line\", \n    SUM(Total) as total_sales\nFROM df\nGROUP BY 1\nORDER BY total_sales DESC\n\"\"\"\nres_line = con.execute(query_cat).fetchdf()\n\n\n\n\n\nFigura: Ventas Totales por Linea de Producto. Las barras representan el monto total de ventas (en MMK).\n\n\n2. Tendencias por D√≠a de la Semana\nAnalizamos las ventas promedio seg√∫n el d√≠a de la semana para detectar picos de actividad y optimizar la gesti√≥n del personal operativo.\n\nVisualizaci√≥n (Plotly)Alternativa con DuckDB (SQL)\n\n\n\n\nVer c√≥digo\n# Generamos la variable con Pandas\norder = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\nsales_by_day = df.groupby('Day of Week')['Total'].mean().reindex(order).reset_index()\n\nfig = px.line(sales_by_day, x='Day of Week', y='Total', \n              title='Ventas Promedio por D√≠a de la Semana',\n              labels={'Total': 'Ventas Promedio (MMK)', 'Day of Week': 'D√≠a'},\n              markers=True)\n\nfig.update_traces(line_color='teal', line_width=3)\nfig.show()\n\n\n\n\n                            \n                                            \n\n\nFigure¬†2\n\n\n\n\n\n\n\n\nVer c√≥digo\n# L√≥gica SQL para promedios con ordenamiento cronol√≥gico manual\nquery_day = \"\"\"\nSELECT \n    \"Day of Week\", \n    AVG(Total) as avg_sales\nFROM df\nGROUP BY 1\nORDER BY CASE \n    WHEN \"Day of Week\" = 'Monday' THEN 1\n    WHEN \"Day of Week\" = 'Tuesday' THEN 2\n    WHEN \"Day of Week\" = 'Wednesday' THEN 3\n    WHEN \"Day of Week\" = 'Thursday' THEN 4\n    WHEN \"Day of Week\" = 'Friday' THEN 5\n    WHEN \"Day of Week\" = 'Saturday' THEN 6\n    WHEN \"Day of Week\" = 'Sunday' THEN 7\nEND\n\"\"\"\nres_day = con.execute(query_day).fetchdf()\n\n\n\n\n\n\n\n\n\n\n\nNoteHallazgo Estrat√©gico\n\n\n\nEl incremento en las ventas promedio durante el s√°bado y domingo valida una oportunidad para implementar campa√±as de fin de semana y ajustar los turnos de reposici√≥n de stock para evitar quiebres de inventario.\n\n\nFigura: Ventas Promedio por D√≠a de la Semana. La l√≠nea muestra el promedio de ventas (en MMK) para cada d√≠a, con picos en fines de semana.\n\n\n3. Evoluci√≥n de Ventas Diarias\nEl monitoreo de la serie de tiempo es vital para detectar anomal√≠as, tendencias o picos de demanda estacionales. La interactividad de Plotly nos permite navegar por periodos espec√≠ficos y analizar la volatilidad del flujo de caja diario.\n\nVisualizaci√≥n (Plotly)Alternativa con DuckDB (SQL)\n\n\n\n\nVer c√≥digo\n# Agregaci√≥n con Pandas para la serie temporal\nventas_diarias = df.groupby('Date')['Total'].sum().reset_index()\n\n# Crear el gr√°fico de l√≠nea interactivo\nfig_ventas_diarias = px.line(\n    ventas_diarias, \n    x='Date', \n    y='Total', \n    title='Evoluci√≥n de Ventas Diarias',\n    labels={'Total': 'Ventas Totales (MMK)', 'Date': 'Fecha'},\n    template='plotly_white'\n)\n\n# Optimizamos el eje X para asegurar el orden y a√±adir navegaci√≥n\nfig_ventas_diarias.update_xaxes(type='category', rangeslider_visible=True)\nfig_ventas_diarias.show()\n\n\n\n\n                            \n                                            \n\n\nFigure¬†3\n\n\n\n\n\n\n\n\nVer c√≥digo\n# As√≠ se obtendr√≠a la misma serie temporal usando l√≥gica SQL\nquery_series = \"\"\"\nSELECT \n    Date, \n    SUM(Total) as Total\nFROM df\nGROUP BY Date\nORDER BY Date\n\"\"\"\nres_series = con.execute(query_series).fetchdf()\n\n\n\n\n\n\n\n\n\n\n\nNoteAn√°lisis de Continuidad\n\n\n\nLa serie de tiempo revela un comportamiento c√≠clico sin una tendencia de crecimiento lineal evidente en el corto plazo. Esto sugiere que el supermercado opera en un mercado maduro donde las ventas dependen m√°s de factores estacionales (d√≠as de la semana o quincenas) que de una expansi√≥n org√°nica acelerada durante este periodo.\n\n\n\n\n\n4. Desempe√±o por Ciudad y Sucursal\nEste an√°lisis identifica la eficiencia de los nodos geogr√°ficos. Comparamos las tres ciudades principales para entender si el volumen de ventas est√° centralizado o distribuido equitativamente.\n\nVisualizaci√≥n (Plotly)Alternativa con DuckDB (SQL)\n\n\n\n\nVer c√≥digo\n# Agrupaci√≥n con Pandas para comparar sucursales por ciudad\ncity_sales = df.groupby(['City', 'Branch'])['Total'].sum().reset_index()\n\nfig_city = px.bar(\n    city_sales, \n    x='City', \n    y='Total', \n    color='Branch',\n    barmode='group',\n    title='Ventas Totales por Ciudad y Sucursal',\n    labels={'Total': 'Ventas Totales (MMK)', 'City': 'Ciudad'},\n    template='plotly_white',\n    color_discrete_sequence=px.colors.qualitative.Prism\n)\n\nfig_city.show()\n\n\n\n\n                            \n                                            \n\n\nFigure¬†4\n\n\n\n\n\n\n\n\nVer c√≥digo\n# Agregaci√≥n multi-nivel en SQL\nquery_city = \"\"\"\nSELECT \n    City, \n    Branch, \n    SUM(Total) as total_sales\nFROM df\nGROUP BY City, Branch\nORDER BY total_sales DESC\n\"\"\"\nres_city = con.execute(query_city).fetchdf()\n\n\n\n\n\n\n\n\n\n\n\nTipInsight Geogr√°fico\n\n\n\nA pesar de las diferencias demogr√°ficas entre las ciudades, el volumen de ingresos se mantiene notablemente similar entre las sucursales, lo que sugiere una estandarizaci√≥n exitosa del modelo de negocio en las diferentes regiones.\n\n\n\n\n\n5. An√°lisis de M√©todos de Pago y Preferencias\nEl √∫ltimo eslab√≥n de nuestro an√°lisis descriptivo es entender c√≥mo interact√∫an los clientes con el punto de venta. Identificar el m√©todo de pago preferido es crucial para negociar comisiones bancarias y optimizar la experiencia de usuario en caja.\nAnalizamos si existe una brecha digital o de comportamiento entre g√©neros respecto al uso de billeteras electr√≥nicas, efectivo o tarjetas de cr√©dito.\n\nVisualizaci√≥n (Plotly)Alternativa con DuckDB (SQL)\n\n\n\n\nVer c√≥digo\n# Generamos la variable con Pandas\npayment_data = df.groupby(['Payment', 'Gender'])['Total'].sum().reset_index()\n\n# Crear gr√°fico de barras agrupadas\nfig_pay = px.bar(\n    payment_data, \n    x='Payment', \n    y='Total', \n    color='Gender',\n    barmode='group',\n    title='Uso de M√©todos de Pago seg√∫n G√©nero',\n    labels={'Total': 'Ingresos Totales (MMK)', 'Payment': 'M√©todo de Pago'},\n    template='plotly_white',\n    color_discrete_map={'Female': '#EF553B', 'Male': '#636EFA'}\n)\n\nfig_pay.show()\n\n\n\n\n                            \n                                            \n\n\nFigure¬†5\n\n\n\n\n\n\n\n\nVer c√≥digo\n# Consulta para cruzar m√©todos de pago y g√©nero\nquery_pay = \"\"\"\nSELECT \n    Payment, \n    Gender, \n    SUM(Total) as total_revenue\nFROM df\nGROUP BY Payment, Gender\nORDER BY Payment, total_revenue DESC\n\"\"\"\nres_pay = con.execute(query_pay).fetchdf()\n\n\n\n\n\n\n\n\n\n\n\nNoteConclusi√≥n de la Fase Descriptiva\n\n\n\nA diferencia de otros mercados donde predomina el efectivo, aqu√≠ observamos un uso equilibrado de E-wallet. Esto abre la puerta a integrar programas de fidelizaci√≥n digitales que capturen datos de comportamiento en tiempo real.\n\n\n\n\n\nHallazgos Preliminares del An√°lisis Exploratorio\nTras auditar las dimensiones clave del negocio, los datos revelan patrones cr√≠ticos que servir√°n de base para las pruebas de hip√≥tesis posteriores. Estos hallazgos permiten pasar de una descripci√≥n de ‚Äúqu√© pas√≥‚Äù a una estrategia de ‚Äúqu√© optimizar‚Äù.\n\nDominancia de Categor√≠as: Las l√≠neas de Alimentos (Food and Beverages) y Accesorios de Moda (Fashion Accessories) se posicionan como los motores de ingresos. Este comportamiento sugiere una alta rotaci√≥n en productos de consumo b√°sico, los cuales podr√≠an utilizarse como ‚Äúproductos gancho‚Äù para categor√≠as de menor rotaci√≥n.\nCiclo de Ventas Semanal: Existe un incremento estad√≠stico visual en el ticket promedio durante el s√°bado y domingo. Este patr√≥n justifica una planificaci√≥n de personal m√°s robusta para el fin de semana y la ejecuci√≥n de campa√±as de marketing tipo ‚ÄúWeekend Sale‚Äù.\nEficiencia Geogr√°fica: La paridad de ventas entre ciudades indica que la marca tiene una penetraci√≥n de mercado madura y uniforme. No obstante, las sucursales en Naypyitaw muestran una consistencia operativa que podr√≠a servir de modelo para los procesos en Yangon.\nOptimizaci√≥n de Inventario: Se han identificado segmentos con bajo rendimiento en volumen (como la l√≠nea de Health and Beauty en ciertas horas del d√≠a). Esto representa una oportunidad directa para ajustar los niveles de stock y reducir los costos de mantenimiento de inventario.\n\n\n\n\n\n\n\nTipSiguiente Paso: Validaci√≥n Cient√≠fica\n\n\n\nLos hallazgos anteriores son observaciones basadas en datos. En la siguiente secci√≥n, utilizaremos Inferencia Estad√≠stica para determinar si estas diferencias (por ejemplo, el aumento de ventas en fines de semana) son estad√≠sticamente significativas o si son simplemente fruto de la variabilidad natural de los datos."
  },
  {
    "objectID": "projects/supermarket_sales_myanmar.html#parte-3-pr√≥ximos-pasos-en-desarrollo",
    "href": "projects/supermarket_sales_myanmar.html#parte-3-pr√≥ximos-pasos-en-desarrollo",
    "title": "Inteligencia de Retail: An√°lisis de Ventas en Myanmar",
    "section": "Parte 3: Pr√≥ximos Pasos (En Desarrollo)",
    "text": "Parte 3: Pr√≥ximos Pasos (En Desarrollo)\nEn las siguientes fases, planeo: - Implementar un dashboard interactivo en Streamlit para explorar las ventas por categor√≠a, ciudad o tipo de cliente. - Realizar un an√°lisis de correlaci√≥n entre variables como g√©nero, tipo de cliente y monto de compra. - Explorar modelos predictivos para pronosticar ventas futuras."
  },
  {
    "objectID": "projects/supermarket_sales_myanmar.html#conclusiones-preliminares",
    "href": "projects/supermarket_sales_myanmar.html#conclusiones-preliminares",
    "title": "Inteligencia de Retail: An√°lisis de Ventas en Myanmar",
    "section": "Conclusiones Preliminares",
    "text": "Conclusiones Preliminares\nEste an√°lisis inicial revela patrones claros en las ventas del supermercado, destacando categor√≠as y d√≠as clave. Las visualizaciones proporcionan una base s√≥lida para decisiones estrat√©gicas, como optimizar inventarios o planificar promociones. La carpeta data/ asegura que los datos sean accesibles y reutilizables para futuros an√°lisis."
  },
  {
    "objectID": "projects/supermarket_sales_myanmar.html#recursos",
    "href": "projects/supermarket_sales_myanmar.html#recursos",
    "title": "Inteligencia de Retail: An√°lisis de Ventas en Myanmar",
    "section": "Recursos",
    "text": "Recursos\n\nDatos: supermarket_sales.csv en data/ (basado en datasets t√≠picos de ventas, como este ejemplo).\nC√≥digo: Disponible en el repositorio del proyecto [enlace si lo subes]."
  },
  {
    "objectID": "projects/university-dropout.html",
    "href": "projects/university-dropout.html",
    "title": "University Dropout Analysis",
    "section": "",
    "text": "This project examines the factors that contribute to student attrition in Business Administration programs. By analyzing patterns in student data, we can identify at-risk students and develop interventions to improve retention rates.\n\n\n\nThe dataset includes: - Academic performance metrics - Attendance records - Socioeconomic indicators - Survey results on student satisfaction - Course enrollment history\n\n\n\nI used the following analytical techniques: - Logistic regression to predict dropout probability - Cluster analysis to identify student archetypes - Time series analysis to detect critical periods - Correlation analysis to identify key factors\n\n\n\n\nFirst-year performance is highly predictive of dropout risk\nStudents with part-time jobs over 20 hours/week show 45% higher attrition\nAttendance drops significantly 3-4 weeks before formal withdrawal\nPeer study groups reduce dropout rates by 30%\n\n\n\n\n\n\n\n\n\nDropout rates by semester and program\n\n\n\n\n\n\n\nBased on the analysis, I recommend: - Implementing an early warning system based on first-month performance - Creating flexible scheduling options for working students - Expanding peer mentoring programs - Enhancing academic support during critical dropout periods\n\n\n\nThis analysis demonstrates that dropout patterns are predictable and interventions can be targeted to specific risk factors and time periods for maximum effect."
  },
  {
    "objectID": "projects/university-dropout.html#introduction",
    "href": "projects/university-dropout.html#introduction",
    "title": "University Dropout Analysis",
    "section": "",
    "text": "This project examines the factors that contribute to student attrition in Business Administration programs. By analyzing patterns in student data, we can identify at-risk students and develop interventions to improve retention rates."
  },
  {
    "objectID": "projects/university-dropout.html#data-collection",
    "href": "projects/university-dropout.html#data-collection",
    "title": "University Dropout Analysis",
    "section": "",
    "text": "The dataset includes: - Academic performance metrics - Attendance records - Socioeconomic indicators - Survey results on student satisfaction - Course enrollment history"
  },
  {
    "objectID": "projects/university-dropout.html#methodology",
    "href": "projects/university-dropout.html#methodology",
    "title": "University Dropout Analysis",
    "section": "",
    "text": "I used the following analytical techniques: - Logistic regression to predict dropout probability - Cluster analysis to identify student archetypes - Time series analysis to detect critical periods - Correlation analysis to identify key factors"
  },
  {
    "objectID": "projects/university-dropout.html#key-findings",
    "href": "projects/university-dropout.html#key-findings",
    "title": "University Dropout Analysis",
    "section": "",
    "text": "First-year performance is highly predictive of dropout risk\nStudents with part-time jobs over 20 hours/week show 45% higher attrition\nAttendance drops significantly 3-4 weeks before formal withdrawal\nPeer study groups reduce dropout rates by 30%"
  },
  {
    "objectID": "projects/university-dropout.html#visualizations",
    "href": "projects/university-dropout.html#visualizations",
    "title": "University Dropout Analysis",
    "section": "",
    "text": "Dropout rates by semester and program"
  },
  {
    "objectID": "projects/university-dropout.html#recommendations",
    "href": "projects/university-dropout.html#recommendations",
    "title": "University Dropout Analysis",
    "section": "",
    "text": "Based on the analysis, I recommend: - Implementing an early warning system based on first-month performance - Creating flexible scheduling options for working students - Expanding peer mentoring programs - Enhancing academic support during critical dropout periods"
  },
  {
    "objectID": "projects/university-dropout.html#conclusion",
    "href": "projects/university-dropout.html#conclusion",
    "title": "University Dropout Analysis",
    "section": "",
    "text": "This analysis demonstrates that dropout patterns are predictable and interventions can be targeted to specific risk factors and time periods for maximum effect."
  },
  {
    "objectID": "posts/estadistica-descriptiva.html",
    "href": "posts/estadistica-descriptiva.html",
    "title": "Estad√≠stica Descriptiva: La Base del An√°lisis de Datos",
    "section": "",
    "text": "La estad√≠stica descriptiva constituye el primer paso en cualquier proceso de an√°lisis de datos. En este art√≠culo, exploraremos los conceptos fundamentales y c√≥mo aplicarlos efectivamente en proyectos reales.\n\n\nLa estad√≠stica descriptiva comprende m√©todos para organizar, resumir y presentar datos de manera informativa. A diferencia de la estad√≠stica inferencial, que busca hacer predicciones basadas en muestras, la estad√≠stica descriptiva se centra en describir lo que ya existe en nuestros datos.\n\n\n\nLas medidas de tendencia central nos ayudan a identificar los valores ‚Äút√≠picos‚Äù de un conjunto de datos:\n\nMedia: El promedio aritm√©tico de todos los valores.\nMediana: El valor central cuando los datos est√°n ordenados.\nModa: El valor que aparece con mayor frecuencia.\n\nimport numpy as np\nimport pandas as pd\n\n# Ejemplo con un conjunto de datos\ndatos = [23, 45, 12, 67, 34, 23, 56, 23, 78, 45]\n\nmedia = np.mean(datos)\nmediana = np.median(datos)\n# Calculando la moda manualmente\nmoda = max(set(datos), key = datos.count)\n\nprint(f\"Media: {media}\")\nprint(f\"Mediana: {mediana}\")\nprint(f\"Moda: {moda}\")\n\n\n\nLas medidas de dispersi√≥n describen cu√°n extendidos o concentrados est√°n los datos:\n\nRango: La diferencia entre el valor m√°ximo y m√≠nimo.\nDesviaci√≥n est√°ndar: Medida de cu√°nto se alejan t√≠picamente los valores de la media.\nVarianza: El cuadrado de la desviaci√≥n est√°ndar.\nRango intercuart√≠lico (IQR): La diferencia entre el tercer y primer cuartil.\n\n\n\n\nLa representaci√≥n visual es crucial para entender patrones en los datos:\n\n\nLos histogramas muestran la distribuci√≥n de frecuencias de un conjunto de datos continuos:\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 6))\nplt.hist(datos, bins=5, color='skyblue', edgecolor='black')\nplt.title('Histograma de Datos')\nplt.xlabel('Valor')\nplt.ylabel('Frecuencia')\nplt.grid(axis='y', alpha=0.75)\nplt.show()\n\n\n\nLos boxplots son excelentes para visualizar la distribuci√≥n y detectar valores at√≠picos:\nplt.figure(figsize=(8, 6))\nplt.boxplot(datos, vert=False, patch_artist=True)\nplt.title('Diagrama de Caja')\nplt.grid(axis='x', linestyle='--')\nplt.show()\n\n\n\n\nVeamos c√≥mo aplicar estos conceptos a un escenario real de an√°lisis de ventas mensuales:\n# Datos de ventas mensuales (en miles de $)\nventas = {\n    'Enero': 120, 'Febrero': 135, 'Marzo': 142, \n    'Abril': 130, 'Mayo': 125, 'Junio': 145\n}\n\ndf_ventas = pd.DataFrame(list(ventas.items()), columns=['Mes', 'Ventas'])\n\n# Estad√≠sticas descriptivas\nestadisticas = df_ventas['Ventas'].describe()\nprint(estadisticas)\n\n# Visualizaci√≥n\nplt.figure(figsize=(12, 6))\nplt.bar(df_ventas['Mes'], df_ventas['Ventas'], color='green')\nplt.title('Ventas Mensuales')\nplt.xlabel('Mes')\nplt.ylabel('Ventas (miles $)')\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\n\n\nLa estad√≠stica descriptiva proporciona las herramientas fundamentales para explorar y entender cualquier conjunto de datos. Dominar estos conceptos b√°sicos es esencial antes de avanzar a t√©cnicas m√°s complejas de an√°lisis e inferencia.\nEn pr√≥ximos art√≠culos, exploraremos c√≥mo pasar de la descripci√≥n a la inferencia estad√≠stica, permitiendo hacer predicciones y tomar decisiones basadas en datos.\n¬øQu√© t√©cnicas de estad√≠stica descriptiva utilizas m√°s frecuentemente en tus an√°lisis? ¬°Comparte tu experiencia en los comentarios!"
  },
  {
    "objectID": "posts/estadistica-descriptiva.html#qu√©-es-la-estad√≠stica-descriptiva",
    "href": "posts/estadistica-descriptiva.html#qu√©-es-la-estad√≠stica-descriptiva",
    "title": "Estad√≠stica Descriptiva: La Base del An√°lisis de Datos",
    "section": "",
    "text": "La estad√≠stica descriptiva comprende m√©todos para organizar, resumir y presentar datos de manera informativa. A diferencia de la estad√≠stica inferencial, que busca hacer predicciones basadas en muestras, la estad√≠stica descriptiva se centra en describir lo que ya existe en nuestros datos."
  },
  {
    "objectID": "posts/estadistica-descriptiva.html#medidas-de-tendencia-central",
    "href": "posts/estadistica-descriptiva.html#medidas-de-tendencia-central",
    "title": "Estad√≠stica Descriptiva: La Base del An√°lisis de Datos",
    "section": "",
    "text": "Las medidas de tendencia central nos ayudan a identificar los valores ‚Äút√≠picos‚Äù de un conjunto de datos:\n\nMedia: El promedio aritm√©tico de todos los valores.\nMediana: El valor central cuando los datos est√°n ordenados.\nModa: El valor que aparece con mayor frecuencia.\n\nimport numpy as np\nimport pandas as pd\n\n# Ejemplo con un conjunto de datos\ndatos = [23, 45, 12, 67, 34, 23, 56, 23, 78, 45]\n\nmedia = np.mean(datos)\nmediana = np.median(datos)\n# Calculando la moda manualmente\nmoda = max(set(datos), key = datos.count)\n\nprint(f\"Media: {media}\")\nprint(f\"Mediana: {mediana}\")\nprint(f\"Moda: {moda}\")"
  },
  {
    "objectID": "posts/estadistica-descriptiva.html#medidas-de-dispersi√≥n",
    "href": "posts/estadistica-descriptiva.html#medidas-de-dispersi√≥n",
    "title": "Estad√≠stica Descriptiva: La Base del An√°lisis de Datos",
    "section": "",
    "text": "Las medidas de dispersi√≥n describen cu√°n extendidos o concentrados est√°n los datos:\n\nRango: La diferencia entre el valor m√°ximo y m√≠nimo.\nDesviaci√≥n est√°ndar: Medida de cu√°nto se alejan t√≠picamente los valores de la media.\nVarianza: El cuadrado de la desviaci√≥n est√°ndar.\nRango intercuart√≠lico (IQR): La diferencia entre el tercer y primer cuartil."
  },
  {
    "objectID": "posts/estadistica-descriptiva.html#visualizaci√≥n-en-la-estad√≠stica-descriptiva",
    "href": "posts/estadistica-descriptiva.html#visualizaci√≥n-en-la-estad√≠stica-descriptiva",
    "title": "Estad√≠stica Descriptiva: La Base del An√°lisis de Datos",
    "section": "",
    "text": "La representaci√≥n visual es crucial para entender patrones en los datos:\n\n\nLos histogramas muestran la distribuci√≥n de frecuencias de un conjunto de datos continuos:\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 6))\nplt.hist(datos, bins=5, color='skyblue', edgecolor='black')\nplt.title('Histograma de Datos')\nplt.xlabel('Valor')\nplt.ylabel('Frecuencia')\nplt.grid(axis='y', alpha=0.75)\nplt.show()\n\n\n\nLos boxplots son excelentes para visualizar la distribuci√≥n y detectar valores at√≠picos:\nplt.figure(figsize=(8, 6))\nplt.boxplot(datos, vert=False, patch_artist=True)\nplt.title('Diagrama de Caja')\nplt.grid(axis='x', linestyle='--')\nplt.show()"
  },
  {
    "objectID": "posts/estadistica-descriptiva.html#aplicaci√≥n-pr√°ctica-an√°lisis-de-ventas",
    "href": "posts/estadistica-descriptiva.html#aplicaci√≥n-pr√°ctica-an√°lisis-de-ventas",
    "title": "Estad√≠stica Descriptiva: La Base del An√°lisis de Datos",
    "section": "",
    "text": "Veamos c√≥mo aplicar estos conceptos a un escenario real de an√°lisis de ventas mensuales:\n# Datos de ventas mensuales (en miles de $)\nventas = {\n    'Enero': 120, 'Febrero': 135, 'Marzo': 142, \n    'Abril': 130, 'Mayo': 125, 'Junio': 145\n}\n\ndf_ventas = pd.DataFrame(list(ventas.items()), columns=['Mes', 'Ventas'])\n\n# Estad√≠sticas descriptivas\nestadisticas = df_ventas['Ventas'].describe()\nprint(estadisticas)\n\n# Visualizaci√≥n\nplt.figure(figsize=(12, 6))\nplt.bar(df_ventas['Mes'], df_ventas['Ventas'], color='green')\nplt.title('Ventas Mensuales')\nplt.xlabel('Mes')\nplt.ylabel('Ventas (miles $)')\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/estadistica-descriptiva.html#conclusi√≥n",
    "href": "posts/estadistica-descriptiva.html#conclusi√≥n",
    "title": "Estad√≠stica Descriptiva: La Base del An√°lisis de Datos",
    "section": "",
    "text": "La estad√≠stica descriptiva proporciona las herramientas fundamentales para explorar y entender cualquier conjunto de datos. Dominar estos conceptos b√°sicos es esencial antes de avanzar a t√©cnicas m√°s complejas de an√°lisis e inferencia.\nEn pr√≥ximos art√≠culos, exploraremos c√≥mo pasar de la descripci√≥n a la inferencia estad√≠stica, permitiendo hacer predicciones y tomar decisiones basadas en datos.\n¬øQu√© t√©cnicas de estad√≠stica descriptiva utilizas m√°s frecuentemente en tus an√°lisis? ¬°Comparte tu experiencia en los comentarios!"
  },
  {
    "objectID": "posts/estadistica-inferencial.html",
    "href": "posts/estadistica-inferencial.html",
    "title": "Estad√≠stica Inferencial: De la Muestra a la Poblaci√≥n",
    "section": "",
    "text": "La estad√≠stica inferencial nos permite generalizar conclusiones de una muestra a una poblaci√≥n m√°s amplia. Este art√≠culo explora los conceptos fundamentales y c√≥mo aplicarlos correctamente en proyectos de an√°lisis de datos.\n\n\nLa estad√≠stica inferencial se basa en la teor√≠a de la probabilidad para hacer predicciones o inferencias sobre una poblaci√≥n a partir de una muestra. Los conceptos clave incluyen:\n\nPoblaciones y muestras: La poblaci√≥n es el conjunto completo de elementos de inter√©s, mientras que la muestra es un subconjunto representativo.\nPar√°metros y estad√≠sticos: Los par√°metros describen caracter√≠sticas de la poblaci√≥n (generalmente desconocidos), mientras que los estad√≠sticos describen caracter√≠sticas de la muestra (observables).\nDistribuciones de muestreo: Describen c√≥mo var√≠an los estad√≠sticos de muestra a muestra.\n\n\n\n\nLos intervalos de confianza proporcionan un rango de valores donde es probable que se encuentre el par√°metro poblacional con un nivel de confianza espec√≠fico.\nimport numpy as np\nfrom scipy import stats\n\n# Ejemplo: Calcular intervalo de confianza para la media\nmuestra = np.random.normal(loc=50, scale=5, size=100)\nmedia_muestral = np.mean(muestra)\nerror_estandar = stats.sem(muestra)\nintervalo = stats.t.interval(0.95, len(muestra)-1, loc=media_muestral, scale=error_estandar)\n\nprint(f\"Media muestral: {media_muestral:.2f}\")\nprint(f\"Intervalo de confianza del 95%: ({intervalo[0]:.2f}, {intervalo[1]:.2f})\")\n\n\n\nLas pruebas de hip√≥tesis nos permiten tomar decisiones sobre afirmaciones poblacionales basadas en evidencia muestral.\n\n\n\nFormular las hip√≥tesis nula (H‚ÇÄ) y alternativa (H‚ÇÅ)\nSeleccionar el nivel de significancia (Œ±)\nCalcular el estad√≠stico de prueba\nDeterminar el p-valor o valor cr√≠tico\nTomar una decisi√≥n y formular la conclusi√≥n\n\n\n\n\n# Ejemplo: Probar si la media poblacional es igual a 48\nhipotesis_valor = 48\nt_stat, p_valor = stats.ttest_1samp(muestra, hipotesis_valor)\n\nprint(f\"Estad√≠stico t: {t_stat:.3f}\")\nprint(f\"Valor p: {p_valor:.4f}\")\nprint(f\"Conclusi√≥n: {'Rechazamos' if p_valor &lt; 0.05 else 'No rechazamos'} la hip√≥tesis nula con Œ±=0.05\")\n\n\n\n\nANOVA permite comparar medias entre tres o m√°s grupos.\n# Datos simulados para tres grupos\ngrupo_a = np.random.normal(loc=50, scale=5, size=30)\ngrupo_b = np.random.normal(loc=52, scale=4, size=30)\ngrupo_c = np.random.normal(loc=48, scale=6, size=30)\n\n# Realizar ANOVA\nf_stat, p_valor = stats.f_oneway(grupo_a, grupo_b, grupo_c)\nprint(f\"Estad√≠stico F: {f_stat:.3f}\")\nprint(f\"Valor p: {p_valor:.4f}\")\n\n\n\nLa regresi√≥n lineal es una t√©cnica potente para modelar relaciones entre variables.\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\n# Datos simulados\nx = np.random.uniform(0, 10, 50)\ny = 2*x + 1 + np.random.normal(0, 2, 50)  # y = 2x + 1 + ruido\n\n# Ajustar modelo de regresi√≥n\npendiente, interseccion, r_valor, p_valor, error_estandar = stats.linregress(x, y)\n\n# Gr√°fico de dispersi√≥n con l√≠nea de regresi√≥n\nplt.figure(figsize=(10, 6))\nplt.scatter(x, y, alpha=0.7)\nplt.plot(x, pendiente*x + interseccion, 'r', label=f'y = {pendiente:.2f}x + {interseccion:.2f}')\nplt.title('Regresi√≥n Lineal')\nplt.xlabel('Variable Independiente (X)')\nplt.ylabel('Variable Dependiente (Y)')\nplt.legend()\nplt.grid(alpha=0.3)\nplt.show()\n\nprint(f\"Modelo: y = {pendiente:.3f}x + {interseccion:.3f}\")\nprint(f\"R¬≤: {r_valor**2:.3f}\")\n\n\n\nEs importante evitar estos errores frecuentes:\n\nSesgo de selecci√≥n: Cuando la muestra no es representativa de la poblaci√≥n\nConfundir correlaci√≥n con causalidad: La correlaci√≥n no implica necesariamente una relaci√≥n causal\nP-hacking: Manipular datos o an√°lisis para obtener resultados significativos\nIgnorar los supuestos: No verificar las condiciones necesarias para aplicar una prueba estad√≠stica\nSobreinterpretar resultados: Extraer conclusiones m√°s all√° de lo que los datos realmente permiten\n\n\n\n\nLa estad√≠stica inferencial proporciona herramientas poderosas para extraer conclusiones significativas de nuestros datos. Sin embargo, requiere un enfoque cuidadoso y una comprensi√≥n clara de los supuestos subyacentes.\nEn el pr√≥ximo art√≠culo, exploraremos t√©cnicas avanzadas de muestreo y su impacto en la precisi√≥n de nuestras inferencias.\n¬øHas enfrentado desaf√≠os al aplicar m√©todos de estad√≠stica inferencial en tus proyectos? Comparte tus experiencias en los comentarios."
  },
  {
    "objectID": "posts/estadistica-inferencial.html#fundamentos-de-la-inferencia-estad√≠stica",
    "href": "posts/estadistica-inferencial.html#fundamentos-de-la-inferencia-estad√≠stica",
    "title": "Estad√≠stica Inferencial: De la Muestra a la Poblaci√≥n",
    "section": "",
    "text": "La estad√≠stica inferencial se basa en la teor√≠a de la probabilidad para hacer predicciones o inferencias sobre una poblaci√≥n a partir de una muestra. Los conceptos clave incluyen:\n\nPoblaciones y muestras: La poblaci√≥n es el conjunto completo de elementos de inter√©s, mientras que la muestra es un subconjunto representativo.\nPar√°metros y estad√≠sticos: Los par√°metros describen caracter√≠sticas de la poblaci√≥n (generalmente desconocidos), mientras que los estad√≠sticos describen caracter√≠sticas de la muestra (observables).\nDistribuciones de muestreo: Describen c√≥mo var√≠an los estad√≠sticos de muestra a muestra."
  },
  {
    "objectID": "posts/estadistica-inferencial.html#intervalos-de-confianza",
    "href": "posts/estadistica-inferencial.html#intervalos-de-confianza",
    "title": "Estad√≠stica Inferencial: De la Muestra a la Poblaci√≥n",
    "section": "",
    "text": "Los intervalos de confianza proporcionan un rango de valores donde es probable que se encuentre el par√°metro poblacional con un nivel de confianza espec√≠fico.\nimport numpy as np\nfrom scipy import stats\n\n# Ejemplo: Calcular intervalo de confianza para la media\nmuestra = np.random.normal(loc=50, scale=5, size=100)\nmedia_muestral = np.mean(muestra)\nerror_estandar = stats.sem(muestra)\nintervalo = stats.t.interval(0.95, len(muestra)-1, loc=media_muestral, scale=error_estandar)\n\nprint(f\"Media muestral: {media_muestral:.2f}\")\nprint(f\"Intervalo de confianza del 95%: ({intervalo[0]:.2f}, {intervalo[1]:.2f})\")"
  },
  {
    "objectID": "posts/estadistica-inferencial.html#pruebas-de-hip√≥tesis",
    "href": "posts/estadistica-inferencial.html#pruebas-de-hip√≥tesis",
    "title": "Estad√≠stica Inferencial: De la Muestra a la Poblaci√≥n",
    "section": "",
    "text": "Las pruebas de hip√≥tesis nos permiten tomar decisiones sobre afirmaciones poblacionales basadas en evidencia muestral.\n\n\n\nFormular las hip√≥tesis nula (H‚ÇÄ) y alternativa (H‚ÇÅ)\nSeleccionar el nivel de significancia (Œ±)\nCalcular el estad√≠stico de prueba\nDeterminar el p-valor o valor cr√≠tico\nTomar una decisi√≥n y formular la conclusi√≥n\n\n\n\n\n# Ejemplo: Probar si la media poblacional es igual a 48\nhipotesis_valor = 48\nt_stat, p_valor = stats.ttest_1samp(muestra, hipotesis_valor)\n\nprint(f\"Estad√≠stico t: {t_stat:.3f}\")\nprint(f\"Valor p: {p_valor:.4f}\")\nprint(f\"Conclusi√≥n: {'Rechazamos' if p_valor &lt; 0.05 else 'No rechazamos'} la hip√≥tesis nula con Œ±=0.05\")"
  },
  {
    "objectID": "posts/estadistica-inferencial.html#anova-an√°lisis-de-varianza",
    "href": "posts/estadistica-inferencial.html#anova-an√°lisis-de-varianza",
    "title": "Estad√≠stica Inferencial: De la Muestra a la Poblaci√≥n",
    "section": "",
    "text": "ANOVA permite comparar medias entre tres o m√°s grupos.\n# Datos simulados para tres grupos\ngrupo_a = np.random.normal(loc=50, scale=5, size=30)\ngrupo_b = np.random.normal(loc=52, scale=4, size=30)\ngrupo_c = np.random.normal(loc=48, scale=6, size=30)\n\n# Realizar ANOVA\nf_stat, p_valor = stats.f_oneway(grupo_a, grupo_b, grupo_c)\nprint(f\"Estad√≠stico F: {f_stat:.3f}\")\nprint(f\"Valor p: {p_valor:.4f}\")"
  },
  {
    "objectID": "posts/estadistica-inferencial.html#regresi√≥n-lineal",
    "href": "posts/estadistica-inferencial.html#regresi√≥n-lineal",
    "title": "Estad√≠stica Inferencial: De la Muestra a la Poblaci√≥n",
    "section": "",
    "text": "La regresi√≥n lineal es una t√©cnica potente para modelar relaciones entre variables.\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\n# Datos simulados\nx = np.random.uniform(0, 10, 50)\ny = 2*x + 1 + np.random.normal(0, 2, 50)  # y = 2x + 1 + ruido\n\n# Ajustar modelo de regresi√≥n\npendiente, interseccion, r_valor, p_valor, error_estandar = stats.linregress(x, y)\n\n# Gr√°fico de dispersi√≥n con l√≠nea de regresi√≥n\nplt.figure(figsize=(10, 6))\nplt.scatter(x, y, alpha=0.7)\nplt.plot(x, pendiente*x + interseccion, 'r', label=f'y = {pendiente:.2f}x + {interseccion:.2f}')\nplt.title('Regresi√≥n Lineal')\nplt.xlabel('Variable Independiente (X)')\nplt.ylabel('Variable Dependiente (Y)')\nplt.legend()\nplt.grid(alpha=0.3)\nplt.show()\n\nprint(f\"Modelo: y = {pendiente:.3f}x + {interseccion:.3f}\")\nprint(f\"R¬≤: {r_valor**2:.3f}\")"
  },
  {
    "objectID": "posts/estadistica-inferencial.html#errores-comunes-en-la-inferencia-estad√≠stica",
    "href": "posts/estadistica-inferencial.html#errores-comunes-en-la-inferencia-estad√≠stica",
    "title": "Estad√≠stica Inferencial: De la Muestra a la Poblaci√≥n",
    "section": "",
    "text": "Es importante evitar estos errores frecuentes:\n\nSesgo de selecci√≥n: Cuando la muestra no es representativa de la poblaci√≥n\nConfundir correlaci√≥n con causalidad: La correlaci√≥n no implica necesariamente una relaci√≥n causal\nP-hacking: Manipular datos o an√°lisis para obtener resultados significativos\nIgnorar los supuestos: No verificar las condiciones necesarias para aplicar una prueba estad√≠stica\nSobreinterpretar resultados: Extraer conclusiones m√°s all√° de lo que los datos realmente permiten"
  },
  {
    "objectID": "posts/estadistica-inferencial.html#conclusi√≥n",
    "href": "posts/estadistica-inferencial.html#conclusi√≥n",
    "title": "Estad√≠stica Inferencial: De la Muestra a la Poblaci√≥n",
    "section": "",
    "text": "La estad√≠stica inferencial proporciona herramientas poderosas para extraer conclusiones significativas de nuestros datos. Sin embargo, requiere un enfoque cuidadoso y una comprensi√≥n clara de los supuestos subyacentes.\nEn el pr√≥ximo art√≠culo, exploraremos t√©cnicas avanzadas de muestreo y su impacto en la precisi√≥n de nuestras inferencias.\n¬øHas enfrentado desaf√≠os al aplicar m√©todos de estad√≠stica inferencial en tus proyectos? Comparte tus experiencias en los comentarios."
  }
]
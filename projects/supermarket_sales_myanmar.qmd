---
title: "Inteligencia de Retail: An√°lisis de Ventas en Myanmar"
subtitle: "Explorando patrones de consumo y rendimiento comercial mediante Python"
author: "LC Pallares"
date: "2025-04-23"
date-modified:  "2026-01-06"
categories: [Python, EDA, Plotly, Retail]
image: "https://placehold.co/800x400" # C√°mbiala por un screenshot de tu mejor gr√°fico
description: "Un an√°lisis profundo sobre el comportamiento del consumidor en tres sucursales de Myanmar, enfocado en la optimizaci√≥n de inventarios y marketing."
format:
  html:
    toc: true
    toc-depth: 3
    code-fold: true
    code-tools: true
    code-summary: "Ver c√≥digo"
    df-print: kable # Hace que las tablas de pandas se vean geniales
    theme: cosmo
---

::: {.callout-note}
## Objetivo del An√°lisis
Identificar los motores de ingresos y el comportamiento de compra para proponer estrategias basadas en datos que mejoren la rentabilidad de la cadena de supermercados.
:::


## üìù Resumen del Proyecto

Este an√°lisis profundiza en las operaciones de una cadena de supermercados en **Myanmar**, utilizando un conjunto de datos que detalla transacciones hist√≥ricas en tres ciudades principales. El objetivo es transformar datos crudos en **inteligencia de negocios** para identificar motores de ingresos y entender las din√°micas del consumidor local.

::: {.callout-note}
## El Dataset
El conjunto de datos abarca variables clave como categor√≠as de productos, demograf√≠a del cliente, m√©todos de pago y m√©tricas financieras (margen bruto e impuestos), permitiendo una visi√≥n 360¬∞ de la din√°mica del retail.
:::

### üéØ Objetivos Principales
* **Optimizaci√≥n Comercial:** Identificar las l√≠neas de productos con mayor rendimiento.
* **Inteligencia del Cliente:** Analizar patrones de compra seg√∫n demograf√≠a y tipo de membres√≠a.
* **An√°lisis Temporal:** Detectar picos de demanda para mejorar la planificaci√≥n operativa.


## üõ†Ô∏è Metodolog√≠a

Para este an√°lisis, implementamos un pipeline de datos moderno que prioriza la velocidad y el rigor estad√≠stico, estructurado en tres niveles:


### 1. Ingesta y Limpieza (Pandas)
* **Normalizaci√≥n:** Renombramos el esquema a *snake_case* y ajustamos tipos de datos (fechas y categor√≠as) para asegurar la integridad de los c√°lculos.
* **Curaci√≥n:** Tratamiento de nulos y validaci√≥n de rangos en precios y cantidades.

### 2. Motor de Consultas (DuckDB)
* **Eficiencia:** Utilizamos **DuckDB** como motor anal√≠tico *in-process*. Esto nos permite ejecutar consultas SQL complejas (como el *Market Basket Analysis*) directamente sobre los DataFrames de Pandas, logrando una ejecuci√≥n mucho m√°s r√°pida que los m√©todos tradicionales.

### 3. Validaci√≥n y Visualizaci√≥n (Scipy & Plotly)
* **Rigor:** Aplicamos pruebas de hip√≥tesis (**T-Tests**) para confirmar que los hallazgos no son producto del azar.
* **Interactividad:** Traducimos los resultados en gr√°ficos din√°micos que permiten explorar dimensiones de tiempo, ciudad y producto.

::: {.callout-tip}
## El Diferenciador T√©cnico
La combinaci√≥n de **Pandas + DuckDB** permite que este flujo de trabajo sea escalable: la flexibilidad de Python con la potencia de un motor SQL dise√±ado para anal√≠tica masiva.
:::

::: {.panel-tabset}

#### Pandas
```{python}
#| label: prep-pandas
import pandas as pd

# Carga y normalizaci√≥n de columnas a snake_case
df = pd.read_csv("../data/supermarket_sales.csv")
# df.columns = [c.lower().replace(' ', '_') for c in df.columns]

# Conversi√≥n de tipos
# df['date'] = pd.to_datetime(df['date'])
df.head(3)
```

#### DuckDB (SQL)
```{python}
#| label: prep-duckdb
import duckdb

# Conexi√≥n in-memory y registro del DataFrame existente
con = duckdb.connect()
con.register('sales', df)

# Verificaci√≥n de la tabla mediante SQL
con.execute("SELECT * FROM sales LIMIT 3").fetchdf()
```

:::



## üõ†Ô∏è Parte 1: Pipeline de Limpieza y Estandarizaci√≥n

La base de datos original (`supermarket_sales.csv`) presentaba inconsistencias t√≠picas de registros transaccionales crudos. Para transformar este archivo en un activo anal√≠tico confiable, ejecutamos un proceso de curaci√≥n estructurado:

1.  **Validaci√≥n Temporal:** Conversi√≥n de fechas a objetos `datetime` para habilitar an√°lisis de series de tiempo.
2.  **Tratamiento de Integridad:** Eliminaci√≥n de registros con valores nulos en columnas cr√≠ticas (`unit_price`, `quantity`) y filtrado de errores operativos (cantidades menores o iguales a cero).
3.  **Enriquecimiento (Feature Engineering):** Extracci√≥n de dimensiones temporales como el nombre del d√≠a y el mes para identificar patrones estacionales.



### Implementaci√≥n T√©cnica

A continuaci√≥n, comparamos c√≥mo abordar este flujo de limpieza utilizando la manipulaci√≥n procedimental de **Pandas** frente a la potencia declarativa de **DuckDB**:

::: {.panel-tabset}

#### Pandas
```{python}
#| label: cleaning-pandas
import pandas as pd
import numpy as np

# Cargar y estandarizar formatos
# df = pd.read_csv("../data/supermarket_sales.csv")
df['Date'] = pd.to_datetime(df['Date'], errors='coerce')
df['Product line'] = df['Product line'].str.title()

# Manejo de valores faltantes y correcci√≥n de errores
df = df.dropna(subset=['Unit price', 'Quantity'])
df = df[df['Quantity'] > 0]

# Enriquecimiento de columnas
df['Day of Week'] = df['Date'].dt.day_name()
df['Month'] = df['Date'].dt.month_name()

df[['Invoice ID', 'Date', 'Day of Week', 'Month']].head(3)
```

#### DuckDB (SQL)
```{python}
#| label: cleaning-duckdb
import duckdb

# Conectamos DuckDB al DataFrame cargado
con = duckdb.connect()
con.register('raw_data', df)

# Replicamos la l√≥gica de limpieza mediante SQL
query = """
SELECT 
    *,
    dayname(Date) AS Day_of_Week,
    monthname(Date) AS Month
FROM raw_data
WHERE "Unit price" IS NOT NULL 
  AND Quantity > 0
LIMIT 3
"""
con.execute(query).fetchdf()
```

:::

::: {.callout-note}
## Resultado del Proceso
Este pipeline garantiza un conjunto de datos consistente, almacenado internamente para las fases de visualizaci√≥n. La estandarizaci√≥n de las categor√≠as (ej. *Title Case*) asegura que los reportes finales mantengan una est√©tica profesional y uniforme.
:::








xxxxxxxxxxxxxxxxxxxxxxxxxx


## Parte 2: An√°lisis Exploratorio y Visualizaciones

## üî¢ Parte 2.1: Estad√≠sticas Descriptivas

Antes de profundizar en patrones visuales, es fundamental auditar las m√©tricas de tendencia central y dispersi√≥n. Este paso nos permite identificar el "comportamiento promedio" de las transacciones y detectar posibles valores at√≠picos que puedan sesgar el an√°lisis.

### Perfil Estad√≠stico del Inventario y Ventas

Analizamos variables cr√≠ticas como el precio unitario, la cantidad de art√≠culos por ticket y el margen bruto.

::: {.panel-tabset}

#### Pandas (Resumen Transpuesto)
La funci√≥n `.describe()` de Pandas es la forma m√°s r√°pida de obtener un panorama completo, incluyendo cuartiles y desviaci√≥n est√°ndar.

```{python}
#| label: stats-pandas
# Seleccionamos las columnas num√©ricas clave
metrics = ['Unit price', 'Quantity', 'Tax 5%', 'Total', 'Gross income']

# Generamos el resumen y aplicamos estilo para mejorar la lectura
stats_summary = df[metrics].describe().T

# Formateo est√©tico para el reporte
stats_summary.style.background_gradient(cmap='Blues').format("{:.2f}")
```

#### DuckDB (Agregaciones Manuales)
Con SQL, tenemos un control total sobre qu√© m√©tricas calcular, permitiendo una visi√≥n personalizada de la eficiencia operativa.

```{python}
#| label: stats-duckdb
query_stats = """
SELECT 
    AVG("Unit price") AS avg_price,
    MIN("Unit price") AS min_price,
    MAX("Unit price") AS max_price,
    AVG(Quantity) AS avg_quantity,
    SUM(Total) AS total_revenue,
    AVG("gross income") AS avg_income
FROM df
"""
con.execute(query_stats).fetchdf()
```

:::



### Hallazgos de la Auditor√≠a Num√©rica

Al observar las tablas anteriores, podemos extraer conclusiones inmediatas sobre la operaci√≥n:

* **Ticket Promedio:** La media de las ventas se sit√∫a cerca de los **322.97 MMK**, con una desviaci√≥n est√°ndar considerable, lo que indica una alta heterogeneidad en el tama√±o de las compras.
* **Volumen de Art√≠culos:** En promedio, los clientes adquieren **5.5 unidades** por transacci√≥n, con un rango que va desde 1 hasta 10 art√≠culos.
* **Margen de Beneficio:** El ingreso bruto promedio por ticket es de aproximadamente **15.38 MMK**, manteniendo una relaci√≥n constante con el impuesto aplicado.

::: {.callout-note}
## Observaci√≥n sobre el Margen
El `gross margin percentage` se mantiene constante en el **4.76%** para todos los registros. Esto indica una pol√≠tica de precios centralizada donde el margen no var√≠a por categor√≠a de producto, sino que depende estrictamente del volumen de venta.
:::















## üìä Parte 2.2: Visualizaci√≥n y Tendencias de Venta

En esta etapa, transformamos los datos en activos visuales interactivos. El objetivo es identificar patrones de consumo que las tablas est√°ticas podr√≠an ocultar. Utilizaremos **Plotly** para la capa de presentaci√≥n por su alta interactividad.

### 1. Ventas por Linea de Producto

Este an√°lisis identifica qu√© l√≠neas de producto son los motores de ingresos, permitiendo priorizar esfuerzos de inventario y marketing.

::: {.panel-tabset}

#### Visualizaci√≥n (Plotly)
```{python}
#| label: fig-sales-line
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go

# Generamos la variable con Pandas para la gr√°fica
sales_by_category = df.groupby('Product line')['Total'].sum().sort_values(ascending=False).reset_index()

fig = px.bar(sales_by_category, x='Total', y='Product line', 
             orientation='h',
             title='Ventas Totales por Categor√≠a de Producto',
             labels={'Total': 'Ventas Totales (MMK)', 'Product line': 'Categor√≠a'},
             color='Total', color_continuous_scale='Viridis')

fig.update_layout(showlegend=False, yaxis={'categoryorder':'total ascending'})
fig.show()
```

#### Alternativa con DuckDB (SQL)
```{python}
#| echo: true
#| eval: false
# As√≠ se obtendr√≠a la misma variable usando l√≥gica SQL
query_line = """
SELECT 
    "Product line", 
    SUM(Total) as total_sales
FROM df
GROUP BY 1
ORDER BY total_sales DESC
"""
res_line = con.execute(query_cat).fetchdf()
```

:::

*Figura: Ventas Totales por Linea de Producto. Las barras representan el monto total de ventas (en MMK).*



### 2. Tendencias por D√≠a de la Semana

Analizamos las ventas promedio seg√∫n el d√≠a de la semana para detectar picos de actividad y optimizar la gesti√≥n del personal operativo.

::: {.panel-tabset}

#### Visualizaci√≥n (Plotly)
```{python}
#| label: fig-sales-day

# Generamos la variable con Pandas
order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
sales_by_day = df.groupby('Day of Week')['Total'].mean().reindex(order).reset_index()

fig = px.line(sales_by_day, x='Day of Week', y='Total', 
              title='Ventas Promedio por D√≠a de la Semana',
              labels={'Total': 'Ventas Promedio (MMK)', 'Day of Week': 'D√≠a'},
              markers=True)

fig.update_traces(line_color='teal', line_width=3)
fig.show()
```

#### Alternativa con DuckDB (SQL)
```{python}
#| echo: true
#| eval: false
# L√≥gica SQL para promedios con ordenamiento cronol√≥gico manual
query_day = """
SELECT 
    "Day of Week", 
    AVG(Total) as avg_sales
FROM df
GROUP BY 1
ORDER BY CASE 
    WHEN "Day of Week" = 'Monday' THEN 1
    WHEN "Day of Week" = 'Tuesday' THEN 2
    WHEN "Day of Week" = 'Wednesday' THEN 3
    WHEN "Day of Week" = 'Thursday' THEN 4
    WHEN "Day of Week" = 'Friday' THEN 5
    WHEN "Day of Week" = 'Saturday' THEN 6
    WHEN "Day of Week" = 'Sunday' THEN 7
END
"""
res_day = con.execute(query_day).fetchdf()
```

:::

::: {.callout-note}
## Hallazgo Estrat√©gico
El incremento en las ventas promedio durante el **s√°bado** y **domingo** valida una oportunidad para implementar campa√±as de fin de semana y ajustar los turnos de reposici√≥n de stock para evitar quiebres de inventario.
:::
 
*Figura: Ventas Promedio por D√≠a de la Semana. La l√≠nea muestra el promedio de ventas (en MMK) para cada d√≠a, con picos en fines de semana.*


### 3. Evoluci√≥n de Ventas Diarias

El monitoreo de la serie de tiempo es vital para detectar anomal√≠as, tendencias o picos de demanda estacionales. La interactividad de Plotly nos permite navegar por periodos espec√≠ficos y analizar la volatilidad del flujo de caja diario.



::: {.panel-tabset}

#### Visualizaci√≥n (Plotly)
```{python}
#| label: fig-ventas-diarias

# Agregaci√≥n con Pandas para la serie temporal
ventas_diarias = df.groupby('Date')['Total'].sum().reset_index()

# Crear el gr√°fico de l√≠nea interactivo
fig_ventas_diarias = px.line(
    ventas_diarias, 
    x='Date', 
    y='Total', 
    title='Evoluci√≥n de Ventas Diarias',
    labels={'Total': 'Ventas Totales (MMK)', 'Date': 'Fecha'},
    template='plotly_white'
)

# Optimizamos el eje X para asegurar el orden y a√±adir navegaci√≥n
fig_ventas_diarias.update_xaxes(type='category', rangeslider_visible=True)
fig_ventas_diarias.show()
```

#### Alternativa con DuckDB (SQL)
```{python}
#| echo: true
#| eval: false

# As√≠ se obtendr√≠a la misma serie temporal usando l√≥gica SQL
query_series = """
SELECT 
    Date, 
    SUM(Total) as Total
FROM df
GROUP BY Date
ORDER BY Date
"""
res_series = con.execute(query_series).fetchdf()
```

:::

::: {.callout-note}
## An√°lisis de Continuidad
La serie de tiempo revela un comportamiento c√≠clico sin una tendencia de crecimiento lineal evidente en el corto plazo. Esto sugiere que el supermercado opera en un mercado maduro donde las ventas dependen m√°s de factores estacionales (d√≠as de la semana o quincenas) que de una expansi√≥n org√°nica acelerada durante este periodo.
:::


---

### 4. Desempe√±o por Ciudad y Sucursal

Este an√°lisis identifica la eficiencia de los nodos geogr√°ficos. Comparamos las tres ciudades principales para entender si el volumen de ventas est√° centralizado o distribuido equitativamente.



::: {.panel-tabset}

#### Visualizaci√≥n (Plotly)
```{python}
#| label: fig-city-branch

# Agrupaci√≥n con Pandas para comparar sucursales por ciudad
city_sales = df.groupby(['City', 'Branch'])['Total'].sum().reset_index()

fig_city = px.bar(
    city_sales, 
    x='City', 
    y='Total', 
    color='Branch',
    barmode='group',
    title='Ventas Totales por Ciudad y Sucursal',
    labels={'Total': 'Ventas Totales (MMK)', 'City': 'Ciudad'},
    template='plotly_white',
    color_discrete_sequence=px.colors.qualitative.Prism
)

fig_city.show()
```

#### Alternativa con DuckDB (SQL)
```{python}
#| echo: true
#| eval: false

# Agregaci√≥n multi-nivel en SQL
query_city = """
SELECT 
    City, 
    Branch, 
    SUM(Total) as total_sales
FROM df
GROUP BY City, Branch
ORDER BY total_sales DESC
"""
res_city = con.execute(query_city).fetchdf()
```

:::

::: {.callout-tip}
## Insight Geogr√°fico
A pesar de las diferencias demogr√°ficas entre las ciudades, el volumen de ingresos se mantiene notablemente similar entre las sucursales, lo que sugiere una estandarizaci√≥n exitosa del modelo de negocio en las diferentes regiones.
:::

---


### 5. An√°lisis de M√©todos de Pago y Preferencias

El √∫ltimo eslab√≥n de nuestro an√°lisis descriptivo es entender c√≥mo interact√∫an los clientes con el punto de venta. Identificar el m√©todo de pago preferido es crucial para negociar comisiones bancarias y optimizar la experiencia de usuario en caja.


Analizamos si existe una brecha digital o de comportamiento entre g√©neros respecto al uso de billeteras electr√≥nicas, efectivo o tarjetas de cr√©dito.

::: {.panel-tabset}

#### Visualizaci√≥n (Plotly)
```{python}
#| label: fig-payment-gender

# Generamos la variable con Pandas
payment_data = df.groupby(['Payment', 'Gender'])['Total'].sum().reset_index()

# Crear gr√°fico de barras agrupadas
fig_pay = px.bar(
    payment_data, 
    x='Payment', 
    y='Total', 
    color='Gender',
    barmode='group',
    title='Uso de M√©todos de Pago seg√∫n G√©nero',
    labels={'Total': 'Ingresos Totales (MMK)', 'Payment': 'M√©todo de Pago'},
    template='plotly_white',
    color_discrete_map={'Female': '#EF553B', 'Male': '#636EFA'}
)

fig_pay.show()
```

#### Alternativa con DuckDB (SQL)
```{python}
#| echo: true
#| eval: false

# Consulta para cruzar m√©todos de pago y g√©nero
query_pay = """
SELECT 
    Payment, 
    Gender, 
    SUM(Total) as total_revenue
FROM df
GROUP BY Payment, Gender
ORDER BY Payment, total_revenue DESC
"""
res_pay = con.execute(query_pay).fetchdf()
```

:::

::: {.callout-note}
## Conclusi√≥n de la Fase Descriptiva
A diferencia de otros mercados donde predomina el efectivo, aqu√≠ observamos un uso equilibrado de **E-wallet**. Esto abre la puerta a integrar programas de fidelizaci√≥n digitales que capturen datos de comportamiento en tiempo real.
:::

---












### Hallazgos Preliminares del An√°lisis Exploratorio

Tras auditar las dimensiones clave del negocio, los datos revelan patrones cr√≠ticos que servir√°n de base para las pruebas de hip√≥tesis posteriores. Estos hallazgos permiten pasar de una descripci√≥n de "qu√© pas√≥" a una estrategia de "qu√© optimizar".



* **Dominancia de Categor√≠as:** Las l√≠neas de **Alimentos (Food and Beverages)** y **Accesorios de Moda (Fashion Accessories)** se posicionan como los motores de ingresos. Este comportamiento sugiere una alta rotaci√≥n en productos de consumo b√°sico, los cuales podr√≠an utilizarse como "productos gancho" para categor√≠as de menor rotaci√≥n.
* **Ciclo de Ventas Semanal:** Existe un incremento estad√≠stico visual en el ticket promedio durante el **s√°bado y domingo**. Este patr√≥n justifica una planificaci√≥n de personal m√°s robusta para el fin de semana y la ejecuci√≥n de campa√±as de marketing tipo "Weekend Sale".
* **Eficiencia Geogr√°fica:** La paridad de ventas entre ciudades indica que la marca tiene una penetraci√≥n de mercado madura y uniforme. No obstante, las sucursales en **Naypyitaw** muestran una consistencia operativa que podr√≠a servir de modelo para los procesos en **Yangon**.
* **Optimizaci√≥n de Inventario:** Se han identificado segmentos con bajo rendimiento en volumen (como la l√≠nea de *Health and Beauty* en ciertas horas del d√≠a). Esto representa una oportunidad directa para ajustar los niveles de stock y reducir los costos de mantenimiento de inventario.

::: {.callout-tip}
## Siguiente Paso: Validaci√≥n Cient√≠fica
Los hallazgos anteriores son observaciones basadas en datos. En la siguiente secci√≥n, utilizaremos **Inferencia Estad√≠stica** para determinar si estas diferencias (por ejemplo, el aumento de ventas en fines de semana) son estad√≠sticamente significativas o si son simplemente fruto de la variabilidad natural de los datos.
:::

---





## Parte 3: Pr√≥ximos Pasos (En Desarrollo)

En las siguientes fases, planeo:
- Implementar un dashboard interactivo en Streamlit para explorar las ventas por categor√≠a, ciudad o tipo de cliente.
- Realizar un an√°lisis de correlaci√≥n entre variables como g√©nero, tipo de cliente y monto de compra.
- Explorar modelos predictivos para pronosticar ventas futuras.

## Conclusiones Preliminares

Este an√°lisis inicial revela patrones claros en las ventas del supermercado, destacando categor√≠as y d√≠as clave. Las visualizaciones proporcionan una base s√≥lida para decisiones estrat√©gicas, como optimizar inventarios o planificar promociones. La carpeta `data/` asegura que los datos sean accesibles y reutilizables para futuros an√°lisis.

## Recursos
- **Datos**: `supermarket_sales.csv` en `data/` (basado en datasets t√≠picos de ventas, como [este ejemplo](https://www.kaggle.com/datasets/aungpyiaye/supermarket-sales)).
- **C√≥digo**: Disponible en el repositorio del proyecto [enlace si lo subes].